{
    "docs": [
        {
            "location": "/", 
            "text": "Tr\u00e6f\u026ak is a modern HTTP reverse proxy and load balancer made to deploy microservices with ease.\nIt supports several backends (\nDocker\n, \nSwarm\n, \nMesos/Marathon\n, \nConsul\n, \nEtcd\n, \nZookeeper\n, \nBoltDB\n, \nAmazon ECS\n, Rest API, file...) to manage its configuration automatically and dynamically.\n\n\nOverview\n\n\nImagine that you have deployed a bunch of microservices on your infrastructure. You probably used a service registry (like etcd or consul) and/or an orchestrator (swarm, Mesos/Marathon) to manage all these services.\nIf you want your users to access some of your microservices from the Internet, you will have to use a reverse proxy and configure it using virtual hosts or prefix paths:\n\n\n\n\ndomain \napi.domain.com\n will point the microservice \napi\n in your private network\n\n\npath \ndomain.com/web\n will point the microservice \nweb\n in your private network\n\n\ndomain \nbackoffice.domain.com\n will point the microservices \nbackoffice\n in your private network, load-balancing between your multiple instances\n\n\n\n\nBut a microservices architecture is dynamic... Services are added, removed, killed or upgraded often, eventually several times a day.\n\n\nTraditional reverse-proxies are not natively dynamic. You can't change their configuration and hot-reload easily.\n\n\nHere enters Tr\u00e6f\u026ak.\n\n\n\n\nTr\u00e6f\u026ak can listen to your service registry/orchestrator API, and knows each time a microservice is added, removed, killed or upgraded, and can generate its configuration automatically.\nRoutes to your services will be created instantly.\n\n\nRun it and forget it!\n\n\nQuickstart\n\n\nYou can have a quick look at Tr\u00e6f\u026ak in this \nKatacoda tutorial\n that shows how to load balance requests between multiple Docker containers.\n\n\nHere is a talk given by \nEd Robinson\n at the \nContainerCamp UK\n conference.\nYou will learn fundamental Tr\u00e6f\u026ak features and see some demos with Kubernetes.\n\n\n\n\nHere is a talk (in French) given by \nEmile Vauge\n at the \nDevoxx France 2016\n conference. \nYou will learn fundamental Tr\u00e6f\u026ak features and see some demos with Docker, Mesos/Marathon and Let's Encrypt. \n\n\n\n\nGet it\n\n\nBinary\n\n\nYou can grab the latest binary from the \nreleases\n page and just run it with the \nsample configuration file\n:\n\n\n./traefik -c traefik.toml\n\n\n\n\nDocker\n\n\nUsing the tiny Docker image:\n\n\ndocker run -d -p 8080:8080 -p 80:80 -v $PWD/traefik.toml:/etc/traefik/traefik.toml traefik\n\n\n\n\nTest it\n\n\nYou can test Tr\u00e6f\u026ak easily using \nDocker compose\n, with this \ndocker-compose.yml\n file in a folder named \ntraefik\n:\n\n\nversion: '2'\n\nservices:\n  proxy:\n    image: traefik\n    command: --web --docker --docker.domain=docker.localhost --logLevel=DEBUG\n    networks:\n      - webgateway\n    ports:\n      - \n80:80\n\n      - \n8080:8080\n\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /dev/null:/traefik.toml\n\nnetworks:\n  webgateway:\n    driver: bridge\n\n\n\n\nStart it from within the \ntraefik\n folder:\n\n\ndocker-compose up -d\n\n\n\nIn a browser you may open \nhttp://localhost:8080\n to access Tr\u00e6f\u026ak's dashboard and observe the following magic.\n\n\nNow, create a folder named \ntest\n and create a \ndocker-compose.yml\n in it with this content:\n\n\nversion: '2'\n\nservices:\n  whoami:\n    image: emilevauge/whoami\n    networks:\n      - web\n    labels:\n      - \ntraefik.backend=whoami\n\n      - \ntraefik.frontend.rule=Host:whoami.docker.localhost\n\n\nnetworks:\n  web:\n    external:\n      name: traefik_webgateway\n\n\n\n\nThen, start and scale it in the \ntest\n folder:\n\n\ndocker-compose up -d\ndocker-compose scale whoami=2\n\n\n\n\nFinally, test load-balancing between the two services \ntest_whoami_1\n and \ntest_whoami_2\n:\n\n\n$ curl -H Host:whoami.docker.localhost http://127.0.0.1\nHostname: ef194d07634a\nIP: 127.0.0.1\nIP: ::1\nIP: 172.17.0.4\nIP: fe80::42:acff:fe11:4\nGET / HTTP/1.1\nHost: 172.17.0.4:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 172.17.0.1\nX-Forwarded-Host: 172.17.0.4:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: dbb60406010d\n\n$ curl -H Host:whoami.docker.localhost http://127.0.0.1\nHostname: 6c3c5df0c79a\nIP: 127.0.0.1\nIP: ::1\nIP: 172.17.0.3\nIP: fe80::42:acff:fe11:3\nGET / HTTP/1.1\nHost: 172.17.0.3:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 172.17.0.1\nX-Forwarded-Host: 172.17.0.3:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: dbb60406010d", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#overview", 
            "text": "Imagine that you have deployed a bunch of microservices on your infrastructure. You probably used a service registry (like etcd or consul) and/or an orchestrator (swarm, Mesos/Marathon) to manage all these services.\nIf you want your users to access some of your microservices from the Internet, you will have to use a reverse proxy and configure it using virtual hosts or prefix paths:   domain  api.domain.com  will point the microservice  api  in your private network  path  domain.com/web  will point the microservice  web  in your private network  domain  backoffice.domain.com  will point the microservices  backoffice  in your private network, load-balancing between your multiple instances   But a microservices architecture is dynamic... Services are added, removed, killed or upgraded often, eventually several times a day.  Traditional reverse-proxies are not natively dynamic. You can't change their configuration and hot-reload easily.  Here enters Tr\u00e6f\u026ak.   Tr\u00e6f\u026ak can listen to your service registry/orchestrator API, and knows each time a microservice is added, removed, killed or upgraded, and can generate its configuration automatically.\nRoutes to your services will be created instantly.  Run it and forget it!", 
            "title": "Overview"
        }, 
        {
            "location": "/#quickstart", 
            "text": "You can have a quick look at Tr\u00e6f\u026ak in this  Katacoda tutorial  that shows how to load balance requests between multiple Docker containers.  Here is a talk given by  Ed Robinson  at the  ContainerCamp UK  conference.\nYou will learn fundamental Tr\u00e6f\u026ak features and see some demos with Kubernetes.   Here is a talk (in French) given by  Emile Vauge  at the  Devoxx France 2016  conference. \nYou will learn fundamental Tr\u00e6f\u026ak features and see some demos with Docker, Mesos/Marathon and Let's Encrypt.", 
            "title": "Quickstart"
        }, 
        {
            "location": "/#get-it", 
            "text": "", 
            "title": "Get it"
        }, 
        {
            "location": "/#binary", 
            "text": "You can grab the latest binary from the  releases  page and just run it with the  sample configuration file :  ./traefik -c traefik.toml", 
            "title": "Binary"
        }, 
        {
            "location": "/#docker", 
            "text": "Using the tiny Docker image:  docker run -d -p 8080:8080 -p 80:80 -v $PWD/traefik.toml:/etc/traefik/traefik.toml traefik", 
            "title": "Docker"
        }, 
        {
            "location": "/#test-it", 
            "text": "You can test Tr\u00e6f\u026ak easily using  Docker compose , with this  docker-compose.yml  file in a folder named  traefik :  version: '2'\n\nservices:\n  proxy:\n    image: traefik\n    command: --web --docker --docker.domain=docker.localhost --logLevel=DEBUG\n    networks:\n      - webgateway\n    ports:\n      -  80:80 \n      -  8080:8080 \n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /dev/null:/traefik.toml\n\nnetworks:\n  webgateway:\n    driver: bridge  Start it from within the  traefik  folder:  docker-compose up -d  In a browser you may open  http://localhost:8080  to access Tr\u00e6f\u026ak's dashboard and observe the following magic.  Now, create a folder named  test  and create a  docker-compose.yml  in it with this content:  version: '2'\n\nservices:\n  whoami:\n    image: emilevauge/whoami\n    networks:\n      - web\n    labels:\n      -  traefik.backend=whoami \n      -  traefik.frontend.rule=Host:whoami.docker.localhost \n\nnetworks:\n  web:\n    external:\n      name: traefik_webgateway  Then, start and scale it in the  test  folder:  docker-compose up -d\ndocker-compose scale whoami=2  Finally, test load-balancing between the two services  test_whoami_1  and  test_whoami_2 :  $ curl -H Host:whoami.docker.localhost http://127.0.0.1\nHostname: ef194d07634a\nIP: 127.0.0.1\nIP: ::1\nIP: 172.17.0.4\nIP: fe80::42:acff:fe11:4\nGET / HTTP/1.1\nHost: 172.17.0.4:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 172.17.0.1\nX-Forwarded-Host: 172.17.0.4:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: dbb60406010d\n\n$ curl -H Host:whoami.docker.localhost http://127.0.0.1\nHostname: 6c3c5df0c79a\nIP: 127.0.0.1\nIP: ::1\nIP: 172.17.0.3\nIP: fe80::42:acff:fe11:3\nGET / HTTP/1.1\nHost: 172.17.0.3:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 172.17.0.1\nX-Forwarded-Host: 172.17.0.3:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: dbb60406010d", 
            "title": "Test it"
        }, 
        {
            "location": "/basics/", 
            "text": "Concepts\n\n\nLet's take our example from the \noverview\n again:\n\n\n\n\nImagine that you have deployed a bunch of microservices on your infrastructure. You probably used a service registry (like etcd or consul) and/or an orchestrator (swarm, Mesos/Marathon) to manage all these services.\nIf you want your users to access some of your microservices from the Internet, you will have to use a reverse proxy and configure it using virtual hosts or prefix paths:\n\n\n\n\ndomain \napi.domain.com\n will point the microservice \napi\n in your private network\n\n\npath \ndomain.com/web\n will point the microservice \nweb\n in your private network\n\n\ndomain \nbackoffice.domain.com\n will point the microservices \nbackoffice\n in your private network, load-balancing between your multiple instances\n\n\n\n\n\n\n\n\nLet's zoom on Tr\u00e6f\u026ak and have an overview of its internal architecture:\n\n\n\n\n\n\nIncoming requests end on \nentrypoints\n, as the name suggests, they are the network entry points into Tr\u00e6f\u026ak (listening port, SSL, traffic redirection...).\n\n\nTraffic is then forwarded to a matching \nfrontend\n. A frontend defines routes from \nentrypoints\n to \nbackends\n.\nRoutes are created using requests fields (\nHost\n, \nPath\n, \nHeaders\n...) and can match or not a request.\n\n\nThe \nfrontend\n will then send the request to a \nbackend\n. A backend can be composed by one or more \nservers\n, and by a load-balancing strategy.\n\n\nFinally, the \nserver\n will forward the request to the corresponding microservice in the private network.\n\n\n\n\nEntrypoints\n\n\nEntrypoints are the network entry points into Tr\u00e6f\u026ak.\nThey can be defined using:\n\n\n\n\na port (80, 443...)\n\n\nSSL (Certificates, Keys, authentication with a client certificate signed by a trusted CA...)\n\n\nredirection to another entrypoint (redirect \nHTTP\n to \nHTTPS\n)\n\n\n\n\nHere is an example of entrypoints definition:\n\n\n[entryPoints]\n  [entryPoints.http]\n  address = \n:80\n\n    [entryPoints.http.redirect]\n    entryPoint = \nhttps\n\n  [entryPoints.https]\n  address = \n:443\n\n    [entryPoints.https.tls]\n      [[entryPoints.https.tls.certificates]]\n      certFile = \ntests/traefik.crt\n\n      keyFile = \ntests/traefik.key\n\n\n\n\n\n\n\nTwo entrypoints are defined \nhttp\n and \nhttps\n.\n\n\nhttp\n listens on port \n80\n and \nhttps\n on port \n443\n.\n\n\nWe enable SSL on \nhttps\n by giving a certificate and a key.\n\n\nWe also redirect all the traffic from entrypoint \nhttp\n to \nhttps\n.\n\n\n\n\nAnd here is another example with client certificate authentication:\n\n\n[entryPoints]\n  [entryPoints.https]\n  address = \n:443\n\n  [entryPoints.https.tls]\n  clientCAFiles = [\ntests/clientca1.crt\n, \ntests/clientca2.crt\n]\n    [[entryPoints.https.tls.certificates]]\n    certFile = \ntests/traefik.crt\n\n    keyFile = \ntests/traefik.key\n\n\n\n\n\n\n\nWe enable SSL on \nhttps\n by giving a certificate and a key.\n\n\nOne or several files containing Certificate Authorities in PEM format are added.\n\n\nIt is possible to have multiple CA:s in the same file or keep them in separate files.\n\n\n\n\nFrontends\n\n\nA frontend is a set of rules that forwards the incoming traffic from an entrypoint to a backend.\nFrontends can be defined using the following rules:\n\n\n\n\nHeaders: Content-Type, application/json\n: Headers adds a matcher for request header values. It accepts a sequence of key/value pairs to be matched.\n\n\nHeadersRegexp: Content-Type, application/(text|json)\n: Regular expressions can be used with headers as well. It accepts a sequence of key/value pairs, where the value has regex support.\n\n\nHost: traefik.io, www.traefik.io\n: Match request host with given host list.\n\n\nHostRegexp: traefik.io, {subdomain:[a-z]+}.traefik.io\n: Adds a matcher for the URL hosts. It accepts templates with zero or more URL variables enclosed by \n{}\n. Variables can define an optional regexp pattern to be matched.\n\n\nMethod: GET, POST, PUT\n: Method adds a matcher for HTTP methods. It accepts a sequence of one or more methods to be matched.\n\n\nPath: /products/, /articles/{category}/{id:[0-9]+}\n: Path adds a matcher for the URL paths. It accepts templates with zero or more URL variables enclosed by \n{}\n.\n\n\nPathStrip\n: Same as \nPath\n but strip the given prefix from the request URL's Path.\n\n\nPathPrefix\n: PathPrefix adds a matcher for the URL path prefixes. This matches if the given template is a prefix of the full URL path.\n\n\nPathPrefixStrip\n: Same as \nPathPrefix\n but strip the given prefix from the request URL's Path.\n\n\nAddPrefix\n : Add prefix from the request URL's Path.\n\n\n\n\nYou can use multlple values for a rule by separating them with \n,\n.\nYou can use multiple rules by separating them by \n;\n.\n\n\nYou can optionally enable \npassHostHeader\n to forward client \nHost\n header to the backend.\n\n\nHere is an example of frontends definition:\n\n\n[frontends]\n  [frontends.frontend1]\n  backend = \nbackend2\n\n    [frontends.frontend1.routes.test_1]\n    rule = \nHost:test.localhost,test2.localhost\n\n  [frontends.frontend2]\n  backend = \nbackend1\n\n  passHostHeader = true\n  priority = 10\n  entrypoints = [\nhttps\n] # overrides defaultEntryPoints\n    [frontends.frontend2.routes.test_1]\n    rule = \nHostRegexp:localhost,{subdomain:[a-z]+}.localhost\n\n  [frontends.frontend3]\n  backend = \nbackend2\n\n    [frontends.frontend3.routes.test_1]\n    rule = \nHost:test3.localhost;Path:/test\n\n\n\n\n\n\n\nThree frontends are defined: \nfrontend1\n, \nfrontend2\n and \nfrontend3\n\n\nfrontend1\n will forward the traffic to the \nbackend2\n if the rule \nHost:test.localhost,test2.localhost\n is matched\n\n\nfrontend2\n will forward the traffic to the \nbackend1\n if the rule \nHost:localhost,{subdomain:[a-z]+}.localhost\n is matched (forwarding client \nHost\n header to the backend)\n\n\nfrontend3\n will forward the traffic to the \nbackend2\n if the rules \nHost:test3.localhost\n \nAND\n \nPath:/test\n are matched\n\n\n\n\nCombining multiple rules\n\n\nAs seen in the previous example, you can combine multiple rules.\nIn TOML file, you can use multiple routes:\n\n\n  [frontends.frontend3]\n  backend = \nbackend2\n\n    [frontends.frontend3.routes.test_1]\n    rule = \nHost:test3.localhost\n\n    [frontends.frontend3.routes.test_2]\n    rule = \nPath:/test\n\n\n\n\n\nHere \nfrontend3\n will forward the traffic to the \nbackend2\n if the rules \nHost:test3.localhost\n \nAND\n \nPath:/test\n are matched.\nYou can also use the notation using a \n;\n separator, same result:\n\n\n  [frontends.frontend3]\n  backend = \nbackend2\n\n    [frontends.frontend3.routes.test_1]\n    rule = \nHost:test3.localhost;Path:/test\n\n\n\n\n\nFinally, you can create a rule to bind multiple domains or Path to a frontend, using the \n,\n separator:\n\n\n [frontends.frontend2]\n    [frontends.frontend2.routes.test_1]\n    rule = \nHost:test1.localhost,test2.localhost\n\n  [frontends.frontend3]\n  backend = \nbackend2\n\n    [frontends.frontend3.routes.test_1]\n    rule = \nPath:/test1,/test2\n\n\n\n\n\nPriorities\n\n\nBy default, routes will be sorted (in descending order) using rules length (to avoid path overlap):\n\nPathPrefix:/12345\n will be matched before \nPathPrefix:/1234\n that will be matched before \nPathPrefix:/1\n.\n\n\nYou can customize priority by frontend:\n\n\n  [frontends]\n    [frontends.frontend1]\n    backend = \nbackend1\n\n    priority = 10\n    passHostHeader = true\n      [frontends.frontend1.routes.test_1]\n      rule = \nPathPrefix:/to\n\n    [frontends.frontend2]\n    priority = 5\n    backend = \nbackend2\n\n    passHostHeader = true\n      [frontends.frontend2.routes.test_1]\n      rule = \nPathPrefix:/toto\n\n\n\n\n\nHere, \nfrontend1\n will be matched before \nfrontend2\n (\n10 \n 5\n).\n\n\nBackends\n\n\nA backend is responsible to load-balance the traffic coming from one or more frontends to a set of http servers.\nVarious methods of load-balancing are supported:\n\n\n\n\nwrr\n: Weighted Round Robin\n\n\ndrr\n: Dynamic Round Robin: increases weights on servers that perform better than others. It also rolls back to original weights if the servers have changed.\n\n\n\n\nA circuit breaker can also be applied to a backend, preventing high loads on failing servers.\nInitial state is Standby. CB observes the statistics and does not modify the request.\nIn case the condition matches, CB enters Tripped state, where it responds with predefined code or redirects to another frontend.\nOnce Tripped timer expires, CB enters Recovering state and resets all stats.\nIn case the condition does not match and recovery timer expires, CB enters Standby state.\n\n\nIt can be configured using:\n\n\n\n\nMethods: \nLatencyAtQuantileMS\n, \nNetworkErrorRatio\n, \nResponseCodeRatio\n\n\nOperators:  \nAND\n, \nOR\n, \nEQ\n, \nNEQ\n, \nLT\n, \nLE\n, \nGT\n, \nGE\n\n\n\n\nFor example:\n\n\n\n\nNetworkErrorRatio() \n 0.5\n: watch error ratio over 10 second sliding window for a frontend\n\n\nLatencyAtQuantileMS(50.0) \n 50\n:  watch latency at quantile in milliseconds.\n\n\nResponseCodeRatio(500, 600, 0, 600) \n 0.5\n: ratio of response codes in range [500-600) to  [0-600)\n\n\n\n\nTo proactively prevent backends from being overwhelmed with high load, a maximum connection limit can\nalso be applied to each backend.\n\n\nMaximum connections can be configured by specifying an integer value for \nmaxconn.amount\n and\n\nmaxconn.extractorfunc\n which is a strategy used to determine how to categorize requests in order to\nevaluate the maximum connections.\n\n\nFor example:\n\n\n[backends]\n  [backends.backend1]\n    [backends.backend1.maxconn]\n       amount = 10\n       extractorfunc = \nrequest.host\n\n\n\n\n\n\n\nbackend1\n will return \nHTTP code 429 Too Many Requests\n if there are already 10 requests in progress for the same Host header.\n\n\nAnother possible value for \nextractorfunc\n is \nclient.ip\n which will categorize requests based on client source ip.\n\n\nLastly \nextractorfunc\n can take the value of \nrequest.header.ANY_HEADER\n which will categorize requests based on \nANY_HEADER\n that you provide.\n\n\n\n\nSticky sessions are supported with both load balancers. When sticky sessions are enabled, a cookie called \n_TRAEFIK_BACKEND\n is set on the initial\nrequest. On subsequent requests, the client will be directed to the backend stored in the cookie if it is still healthy. If not, a new backend\nwill be assigned.\n\n\nFor example:\n\n\n[backends]\n  [backends.backend1]\n    [backends.backend1.loadbalancer]\n      sticky = true\n\n\n\n\nA health check can be configured in order to remove a backend from LB rotation\nas long as it keeps returning HTTP status codes other than 200 OK to HTTP GET\nrequests periodically carried out by Traefik. The check is defined by a path\nappended to the backend URL and an interval (given in a format understood by \ntime.ParseDuration\n) specifying how\noften the health check should be executed (the default being 30 seconds). Each\nbackend must respond to the health check within 5 seconds.\n\n\nA recovering backend returning 200 OK responses again is being returned to the\nLB rotation pool.\n\n\nFor example:\n\n\n[backends]\n  [backends.backend1]\n    [backends.backend1.healthcheck]\n      path = \n/health\n\n      interval = \n10s\n\n\n\n\n\nServers\n\n\nServers are simply defined using a \nURL\n. You can also apply a custom \nweight\n to each server (this will be used by load-balancing).\n\n\nHere is an example of backends and servers definition:\n\n\n[backends]\n  [backends.backend1]\n    [backends.backend1.circuitbreaker]\n      expression = \nNetworkErrorRatio() \n 0.5\n\n    [backends.backend1.servers.server1]\n    url = \nhttp://172.17.0.2:80\n\n    weight = 10\n    [backends.backend1.servers.server2]\n    url = \nhttp://172.17.0.3:80\n\n    weight = 1\n  [backends.backend2]\n    [backends.backend2.LoadBalancer]\n      method = \ndrr\n\n    [backends.backend2.servers.server1]\n    url = \nhttp://172.17.0.4:80\n\n    weight = 1\n    [backends.backend2.servers.server2]\n    url = \nhttp://172.17.0.5:80\n\n    weight = 2\n\n\n\n\n\n\nTwo backends are defined: \nbackend1\n and \nbackend2\n\n\nbackend1\n will forward the traffic to two servers: \nhttp://172.17.0.2:80\"\n with weight \n10\n and \nhttp://172.17.0.3:80\n with weight \n1\n using default \nwrr\n load-balancing strategy.\n\n\nbackend2\n will forward the traffic to two servers: \nhttp://172.17.0.4:80\"\n with weight \n1\n and \nhttp://172.17.0.5:80\n with weight \n2\n using \ndrr\n load-balancing strategy.\n\n\na circuit breaker is added on \nbackend1\n using the expression \nNetworkErrorRatio() \n 0.5\n: watch error ratio over 10 second sliding window\n\n\n\n\nConfiguration\n\n\nTr\u00e6f\u026ak's configuration has two parts: \n\n\n\n\nThe \nstatic Tr\u00e6f\u026ak configuration\n which is loaded only at the beginning. \n\n\nThe \ndynamic Tr\u00e6f\u026ak configuration\n which can be hot-reloaded (no need to restart the process).\n\n\n\n\nStatic Tr\u00e6f\u026ak configuration\n\n\nThe static configuration is the global configuration which is setting up connections to configuration backends and entrypoints. \n\n\nTr\u00e6f\u026ak can be configured using many configuration sources with the following precedence order. \nEach item takes precedence over the item below it:\n\n\n\n\nKey-value Store\n\n\nArguments\n\n\nConfiguration file\n\n\nDefault\n\n\n\n\nIt means that arguments override configuration file, and Key-value Store overrides arguments.\n\n\nConfiguration file\n\n\nBy default, Tr\u00e6f\u026ak will try to find a \ntraefik.toml\n in the following places:\n\n\n\n\n/etc/traefik/\n\n\n$HOME/.traefik/\n\n\n.\n \nthe working directory\n\n\n\n\nYou can override this by setting a \nconfigFile\n argument:\n\n\n$ traefik --configFile=foo/bar/myconfigfile.toml\n\n\n\n\nPlease refer to the \nglobal configuration\n section to get documentation on it.\n\n\nArguments\n\n\nEach argument (and command) is described in the help section:\n\n\n$ traefik --help\n\n\n\n\nNote that all default values will be displayed as well.\n\n\nKey-value stores\n\n\nTr\u00e6f\u026ak supports several Key-value stores:\n\n\n\n\nConsul\n\n\netcd\n\n\nZooKeeper\n \n\n\nboltdb\n\n\n\n\nPlease refer to the \nUser Guide Key-value store configuration\n section to get documentation on it.\n\n\nDynamic Tr\u00e6f\u026ak configuration\n\n\nThe dynamic configuration concerns : \n\n\n\n\nFrontends\n\n\nBackends\n \n\n\nServers\n \n\n\n\n\nTr\u00e6f\u026ak can hot-reload those rules which could be provided by \nmultiple configuration backends\n.\n\n\nWe only need to enable \nwatch\n option to make Tr\u00e6f\u026ak watch configuration backend changes and generate its configuration automatically.\nRoutes to services will be created and updated instantly at any changes.\n\n\nPlease refer to the \nconfiguration backends\n section to get documentation on it.\n\n\nCommands\n\n\nUsage: \ntraefik\u00a0[command] [--flag=flag_argument]\n\n\nList of Tr\u00e6f\u026ak available\u00a0commands with description :\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n\nversion\n : Print\u00a0version\u00a0\n\n\nstoreconfig\n : Store the static traefik configuration into a Key-value stores.\u00a0Please refer to the \nStore Tr\u00e6f\u026ak configuration\n section to get documentation on it.\n\n\n\n\nEach command may have related flags. \nAll those related flags will be displayed with :\n\n\n$ traefik\u00a0[command]\u00a0--help\n\n\n\n\nNote that each command is described at the beginning of the help section:\n\n\n$ traefik --help", 
            "title": "Basics"
        }, 
        {
            "location": "/basics/#concepts", 
            "text": "Let's take our example from the  overview  again:   Imagine that you have deployed a bunch of microservices on your infrastructure. You probably used a service registry (like etcd or consul) and/or an orchestrator (swarm, Mesos/Marathon) to manage all these services.\nIf you want your users to access some of your microservices from the Internet, you will have to use a reverse proxy and configure it using virtual hosts or prefix paths:   domain  api.domain.com  will point the microservice  api  in your private network  path  domain.com/web  will point the microservice  web  in your private network  domain  backoffice.domain.com  will point the microservices  backoffice  in your private network, load-balancing between your multiple instances     Let's zoom on Tr\u00e6f\u026ak and have an overview of its internal architecture:    Incoming requests end on  entrypoints , as the name suggests, they are the network entry points into Tr\u00e6f\u026ak (listening port, SSL, traffic redirection...).  Traffic is then forwarded to a matching  frontend . A frontend defines routes from  entrypoints  to  backends .\nRoutes are created using requests fields ( Host ,  Path ,  Headers ...) and can match or not a request.  The  frontend  will then send the request to a  backend . A backend can be composed by one or more  servers , and by a load-balancing strategy.  Finally, the  server  will forward the request to the corresponding microservice in the private network.", 
            "title": "Concepts"
        }, 
        {
            "location": "/basics/#entrypoints", 
            "text": "Entrypoints are the network entry points into Tr\u00e6f\u026ak.\nThey can be defined using:   a port (80, 443...)  SSL (Certificates, Keys, authentication with a client certificate signed by a trusted CA...)  redirection to another entrypoint (redirect  HTTP  to  HTTPS )   Here is an example of entrypoints definition:  [entryPoints]\n  [entryPoints.http]\n  address =  :80 \n    [entryPoints.http.redirect]\n    entryPoint =  https \n  [entryPoints.https]\n  address =  :443 \n    [entryPoints.https.tls]\n      [[entryPoints.https.tls.certificates]]\n      certFile =  tests/traefik.crt \n      keyFile =  tests/traefik.key    Two entrypoints are defined  http  and  https .  http  listens on port  80  and  https  on port  443 .  We enable SSL on  https  by giving a certificate and a key.  We also redirect all the traffic from entrypoint  http  to  https .   And here is another example with client certificate authentication:  [entryPoints]\n  [entryPoints.https]\n  address =  :443 \n  [entryPoints.https.tls]\n  clientCAFiles = [ tests/clientca1.crt ,  tests/clientca2.crt ]\n    [[entryPoints.https.tls.certificates]]\n    certFile =  tests/traefik.crt \n    keyFile =  tests/traefik.key    We enable SSL on  https  by giving a certificate and a key.  One or several files containing Certificate Authorities in PEM format are added.  It is possible to have multiple CA:s in the same file or keep them in separate files.", 
            "title": "Entrypoints"
        }, 
        {
            "location": "/basics/#frontends", 
            "text": "A frontend is a set of rules that forwards the incoming traffic from an entrypoint to a backend.\nFrontends can be defined using the following rules:   Headers: Content-Type, application/json : Headers adds a matcher for request header values. It accepts a sequence of key/value pairs to be matched.  HeadersRegexp: Content-Type, application/(text|json) : Regular expressions can be used with headers as well. It accepts a sequence of key/value pairs, where the value has regex support.  Host: traefik.io, www.traefik.io : Match request host with given host list.  HostRegexp: traefik.io, {subdomain:[a-z]+}.traefik.io : Adds a matcher for the URL hosts. It accepts templates with zero or more URL variables enclosed by  {} . Variables can define an optional regexp pattern to be matched.  Method: GET, POST, PUT : Method adds a matcher for HTTP methods. It accepts a sequence of one or more methods to be matched.  Path: /products/, /articles/{category}/{id:[0-9]+} : Path adds a matcher for the URL paths. It accepts templates with zero or more URL variables enclosed by  {} .  PathStrip : Same as  Path  but strip the given prefix from the request URL's Path.  PathPrefix : PathPrefix adds a matcher for the URL path prefixes. This matches if the given template is a prefix of the full URL path.  PathPrefixStrip : Same as  PathPrefix  but strip the given prefix from the request URL's Path.  AddPrefix  : Add prefix from the request URL's Path.   You can use multlple values for a rule by separating them with  , .\nYou can use multiple rules by separating them by  ; .  You can optionally enable  passHostHeader  to forward client  Host  header to the backend.  Here is an example of frontends definition:  [frontends]\n  [frontends.frontend1]\n  backend =  backend2 \n    [frontends.frontend1.routes.test_1]\n    rule =  Host:test.localhost,test2.localhost \n  [frontends.frontend2]\n  backend =  backend1 \n  passHostHeader = true\n  priority = 10\n  entrypoints = [ https ] # overrides defaultEntryPoints\n    [frontends.frontend2.routes.test_1]\n    rule =  HostRegexp:localhost,{subdomain:[a-z]+}.localhost \n  [frontends.frontend3]\n  backend =  backend2 \n    [frontends.frontend3.routes.test_1]\n    rule =  Host:test3.localhost;Path:/test    Three frontends are defined:  frontend1 ,  frontend2  and  frontend3  frontend1  will forward the traffic to the  backend2  if the rule  Host:test.localhost,test2.localhost  is matched  frontend2  will forward the traffic to the  backend1  if the rule  Host:localhost,{subdomain:[a-z]+}.localhost  is matched (forwarding client  Host  header to the backend)  frontend3  will forward the traffic to the  backend2  if the rules  Host:test3.localhost   AND   Path:/test  are matched", 
            "title": "Frontends"
        }, 
        {
            "location": "/basics/#combining-multiple-rules", 
            "text": "As seen in the previous example, you can combine multiple rules.\nIn TOML file, you can use multiple routes:    [frontends.frontend3]\n  backend =  backend2 \n    [frontends.frontend3.routes.test_1]\n    rule =  Host:test3.localhost \n    [frontends.frontend3.routes.test_2]\n    rule =  Path:/test   Here  frontend3  will forward the traffic to the  backend2  if the rules  Host:test3.localhost   AND   Path:/test  are matched.\nYou can also use the notation using a  ;  separator, same result:    [frontends.frontend3]\n  backend =  backend2 \n    [frontends.frontend3.routes.test_1]\n    rule =  Host:test3.localhost;Path:/test   Finally, you can create a rule to bind multiple domains or Path to a frontend, using the  ,  separator:   [frontends.frontend2]\n    [frontends.frontend2.routes.test_1]\n    rule =  Host:test1.localhost,test2.localhost \n  [frontends.frontend3]\n  backend =  backend2 \n    [frontends.frontend3.routes.test_1]\n    rule =  Path:/test1,/test2", 
            "title": "Combining multiple rules"
        }, 
        {
            "location": "/basics/#priorities", 
            "text": "By default, routes will be sorted (in descending order) using rules length (to avoid path overlap): PathPrefix:/12345  will be matched before  PathPrefix:/1234  that will be matched before  PathPrefix:/1 .  You can customize priority by frontend:    [frontends]\n    [frontends.frontend1]\n    backend =  backend1 \n    priority = 10\n    passHostHeader = true\n      [frontends.frontend1.routes.test_1]\n      rule =  PathPrefix:/to \n    [frontends.frontend2]\n    priority = 5\n    backend =  backend2 \n    passHostHeader = true\n      [frontends.frontend2.routes.test_1]\n      rule =  PathPrefix:/toto   Here,  frontend1  will be matched before  frontend2  ( 10   5 ).", 
            "title": "Priorities"
        }, 
        {
            "location": "/basics/#backends", 
            "text": "A backend is responsible to load-balance the traffic coming from one or more frontends to a set of http servers.\nVarious methods of load-balancing are supported:   wrr : Weighted Round Robin  drr : Dynamic Round Robin: increases weights on servers that perform better than others. It also rolls back to original weights if the servers have changed.   A circuit breaker can also be applied to a backend, preventing high loads on failing servers.\nInitial state is Standby. CB observes the statistics and does not modify the request.\nIn case the condition matches, CB enters Tripped state, where it responds with predefined code or redirects to another frontend.\nOnce Tripped timer expires, CB enters Recovering state and resets all stats.\nIn case the condition does not match and recovery timer expires, CB enters Standby state.  It can be configured using:   Methods:  LatencyAtQuantileMS ,  NetworkErrorRatio ,  ResponseCodeRatio  Operators:   AND ,  OR ,  EQ ,  NEQ ,  LT ,  LE ,  GT ,  GE   For example:   NetworkErrorRatio()   0.5 : watch error ratio over 10 second sliding window for a frontend  LatencyAtQuantileMS(50.0)   50 :  watch latency at quantile in milliseconds.  ResponseCodeRatio(500, 600, 0, 600)   0.5 : ratio of response codes in range [500-600) to  [0-600)   To proactively prevent backends from being overwhelmed with high load, a maximum connection limit can\nalso be applied to each backend.  Maximum connections can be configured by specifying an integer value for  maxconn.amount  and maxconn.extractorfunc  which is a strategy used to determine how to categorize requests in order to\nevaluate the maximum connections.  For example:  [backends]\n  [backends.backend1]\n    [backends.backend1.maxconn]\n       amount = 10\n       extractorfunc =  request.host    backend1  will return  HTTP code 429 Too Many Requests  if there are already 10 requests in progress for the same Host header.  Another possible value for  extractorfunc  is  client.ip  which will categorize requests based on client source ip.  Lastly  extractorfunc  can take the value of  request.header.ANY_HEADER  which will categorize requests based on  ANY_HEADER  that you provide.   Sticky sessions are supported with both load balancers. When sticky sessions are enabled, a cookie called  _TRAEFIK_BACKEND  is set on the initial\nrequest. On subsequent requests, the client will be directed to the backend stored in the cookie if it is still healthy. If not, a new backend\nwill be assigned.  For example:  [backends]\n  [backends.backend1]\n    [backends.backend1.loadbalancer]\n      sticky = true  A health check can be configured in order to remove a backend from LB rotation\nas long as it keeps returning HTTP status codes other than 200 OK to HTTP GET\nrequests periodically carried out by Traefik. The check is defined by a path\nappended to the backend URL and an interval (given in a format understood by  time.ParseDuration ) specifying how\noften the health check should be executed (the default being 30 seconds). Each\nbackend must respond to the health check within 5 seconds.  A recovering backend returning 200 OK responses again is being returned to the\nLB rotation pool.  For example:  [backends]\n  [backends.backend1]\n    [backends.backend1.healthcheck]\n      path =  /health \n      interval =  10s", 
            "title": "Backends"
        }, 
        {
            "location": "/basics/#servers", 
            "text": "Servers are simply defined using a  URL . You can also apply a custom  weight  to each server (this will be used by load-balancing).  Here is an example of backends and servers definition:  [backends]\n  [backends.backend1]\n    [backends.backend1.circuitbreaker]\n      expression =  NetworkErrorRatio()   0.5 \n    [backends.backend1.servers.server1]\n    url =  http://172.17.0.2:80 \n    weight = 10\n    [backends.backend1.servers.server2]\n    url =  http://172.17.0.3:80 \n    weight = 1\n  [backends.backend2]\n    [backends.backend2.LoadBalancer]\n      method =  drr \n    [backends.backend2.servers.server1]\n    url =  http://172.17.0.4:80 \n    weight = 1\n    [backends.backend2.servers.server2]\n    url =  http://172.17.0.5:80 \n    weight = 2   Two backends are defined:  backend1  and  backend2  backend1  will forward the traffic to two servers:  http://172.17.0.2:80\"  with weight  10  and  http://172.17.0.3:80  with weight  1  using default  wrr  load-balancing strategy.  backend2  will forward the traffic to two servers:  http://172.17.0.4:80\"  with weight  1  and  http://172.17.0.5:80  with weight  2  using  drr  load-balancing strategy.  a circuit breaker is added on  backend1  using the expression  NetworkErrorRatio()   0.5 : watch error ratio over 10 second sliding window", 
            "title": "Servers"
        }, 
        {
            "location": "/basics/#configuration", 
            "text": "Tr\u00e6f\u026ak's configuration has two parts:    The  static Tr\u00e6f\u026ak configuration  which is loaded only at the beginning.   The  dynamic Tr\u00e6f\u026ak configuration  which can be hot-reloaded (no need to restart the process).", 
            "title": "Configuration"
        }, 
        {
            "location": "/basics/#static-trfk-configuration", 
            "text": "The static configuration is the global configuration which is setting up connections to configuration backends and entrypoints.   Tr\u00e6f\u026ak can be configured using many configuration sources with the following precedence order. \nEach item takes precedence over the item below it:   Key-value Store  Arguments  Configuration file  Default   It means that arguments override configuration file, and Key-value Store overrides arguments.", 
            "title": "Static Tr\u00e6f\u026ak configuration"
        }, 
        {
            "location": "/basics/#configuration-file", 
            "text": "By default, Tr\u00e6f\u026ak will try to find a  traefik.toml  in the following places:   /etc/traefik/  $HOME/.traefik/  .   the working directory   You can override this by setting a  configFile  argument:  $ traefik --configFile=foo/bar/myconfigfile.toml  Please refer to the  global configuration  section to get documentation on it.", 
            "title": "Configuration file"
        }, 
        {
            "location": "/basics/#arguments", 
            "text": "Each argument (and command) is described in the help section:  $ traefik --help  Note that all default values will be displayed as well.", 
            "title": "Arguments"
        }, 
        {
            "location": "/basics/#key-value-stores", 
            "text": "Tr\u00e6f\u026ak supports several Key-value stores:   Consul  etcd  ZooKeeper    boltdb   Please refer to the  User Guide Key-value store configuration  section to get documentation on it.", 
            "title": "Key-value stores"
        }, 
        {
            "location": "/basics/#dynamic-trfk-configuration", 
            "text": "The dynamic configuration concerns :    Frontends  Backends    Servers     Tr\u00e6f\u026ak can hot-reload those rules which could be provided by  multiple configuration backends .  We only need to enable  watch  option to make Tr\u00e6f\u026ak watch configuration backend changes and generate its configuration automatically.\nRoutes to services will be created and updated instantly at any changes.  Please refer to the  configuration backends  section to get documentation on it.", 
            "title": "Dynamic Tr\u00e6f\u026ak configuration"
        }, 
        {
            "location": "/basics/#commands", 
            "text": "Usage:  traefik\u00a0[command] [--flag=flag_argument]  List of Tr\u00e6f\u026ak available\u00a0commands with description :\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0   version  : Print\u00a0version\u00a0  storeconfig  : Store the static traefik configuration into a Key-value stores.\u00a0Please refer to the  Store Tr\u00e6f\u026ak configuration  section to get documentation on it.   Each command may have related flags. \nAll those related flags will be displayed with :  $ traefik\u00a0[command]\u00a0--help  Note that each command is described at the beginning of the help section:  $ traefik --help", 
            "title": "Commands"
        }, 
        {
            "location": "/toml/", 
            "text": "Global configuration\n\n\nMain section\n\n\n# traefik.toml\n################################################################\n# Global configuration\n################################################################\n\n# Timeout in seconds.\n# Duration to give active requests a chance to finish during hot-reloads\n#\n# Optional\n# Default: 10\n#\n# graceTimeOut = 10\n\n# Enable debug mode\n#\n# Optional\n# Default: false\n#\n# debug = true\n\n# Periodically check if a new version has been released\n#\n# Optional\n# Default: true\n#\n# checkNewVersion = false\n\n# Traefik logs file\n# If not defined, logs to stdout\n#\n# Optional\n#\n# traefikLogsFile = \nlog/traefik.log\n\n\n# Access logs file\n#\n# Optional\n#\n# accessLogsFile = \nlog/access.log\n\n\n# Log level\n#\n# Optional\n# Default: \nERROR\n\n# Accepted values, in order of severity: \nDEBUG\n, \nINFO\n, \nWARN\n, \nERROR\n, \nFATAL\n, \nPANIC\n\n# Messages at and above the selected level will be logged.\n#\n# logLevel = \nERROR\n\n\n# Backends throttle duration: minimum duration in seconds between 2 events from providers\n# before applying a new configuration. It avoids unnecessary reloads if multiples events\n# are sent in a short amount of time.\n#\n# Optional\n# Default: \n2\n\n#\n# ProvidersThrottleDuration = \n5\n\n\n# Controls the maximum idle (keep-alive) connections to keep per-host. If zero, DefaultMaxIdleConnsPerHost\n# from the Go standard library net/http module is used.\n# If you encounter 'too many open files' errors, you can either increase this\n# value or change the `ulimit`.\n#\n# Optional\n# Default: 200\n#\n# MaxIdleConnsPerHost = 200\n\n# If set to true invalid SSL certificates are accepted for backends.\n# Note: This disables detection of man-in-the-middle attacks so should only be used on secure backend networks.\n# Optional\n# Default: false\n#\n# InsecureSkipVerify = true\n\n# Entrypoints to be used by frontends that do not specify any entrypoint.\n# Each frontend can specify its own entrypoints.\n#\n# Optional\n# Default: [\nhttp\n]\n#\n# defaultEntryPoints = [\nhttp\n, \nhttps\n]\n\n\n\n\nConstraints\n\n\nIn a micro-service architecture, with a central service discovery, setting constraints limits Tr\u00e6f\u026ak scope to a smaller number of routes.\n\n\nTr\u00e6f\u026ak filters services according to service attributes/tags set in your configuration backends.\n\n\nSupported backends:\n\n\n\n\nDocker\n\n\nConsul K/V\n\n\nBoltDB\n\n\nZookeeper\n\n\nEtcd\n\n\nConsul Catalog\n\n\n\n\nSupported filters:\n\n\n\n\ntag\n\n\n\n\n# Constraints definition\n#\n# Optional\n#\n# Simple matching constraint\n# constraints = [\ntag==api\n]\n#\n# Simple mismatching constraint\n# constraints = [\ntag!=api\n]\n#\n# Globbing\n# constraints = [\ntag==us-*\n]\n#\n# Backend-specific constraint\n# [consulCatalog]\n#   endpoint = 127.0.0.1:8500\n#   constraints = [\ntag==api\n]\n#\n# Multiple constraints\n#   - \ntag==\n must match with at least one tag\n#   - \ntag!=\n must match with none of tags\n# constraints = [\ntag!=us-*\n, \ntag!=asia-*\n]\n# [consulCatalog]\n#   endpoint = 127.0.0.1:8500\n#   constraints = [\ntag==api\n, \ntag!=v*-beta\n]\n\n\n\n\nEntrypoints definition\n\n\n# Entrypoints definition\n#\n# Optional\n# Default:\n# [entryPoints]\n#   [entryPoints.http]\n#   address = \n:80\n\n#\n# To redirect an http entrypoint to an https entrypoint (with SNI support):\n# [entryPoints]\n#   [entryPoints.http]\n#   address = \n:80\n\n#     [entryPoints.http.redirect]\n#       entryPoint = \nhttps\n\n#   [entryPoints.https]\n#   address = \n:443\n\n#     [entryPoints.https.tls]\n#       [[entryPoints.https.tls.certificates]]\n#       CertFile = \nintegration/fixtures/https/snitest.com.cert\n\n#       KeyFile = \nintegration/fixtures/https/snitest.com.key\n\n#       [[entryPoints.https.tls.certificates]]\n#       CertFile = \nintegration/fixtures/https/snitest.org.cert\n\n#       KeyFile = \nintegration/fixtures/https/snitest.org.key\n\n#\n# To redirect an entrypoint rewriting the URL:\n# [entryPoints]\n#   [entryPoints.http]\n#   address = \n:80\n\n#     [entryPoints.http.redirect]\n#       regex = \n^http://localhost/(.*)\n\n#       replacement = \nhttp://mydomain/$1\n\n#\n# Only accept clients that present a certificate signed by a specified\n# Certificate Authority (CA)\n# ClientCAFiles can be configured with multiple CA:s in the same file or\n# use multiple files containing one or several CA:s. The CA:s has to be in PEM format.\n# All clients will be required to present a valid cert.\n# The requirement will apply to all server certs in the entrypoint\n# In the example below both snitest.com and snitest.org will require client certs\n#\n# [entryPoints]\n#   [entryPoints.https]\n#   address = \n:443\n\n#   [entryPoints.https.tls]\n#   ClientCAFiles = [\ntests/clientca1.crt\n, \ntests/clientca2.crt\n]\n#     [[entryPoints.https.tls.certificates]]\n#     CertFile = \nintegration/fixtures/https/snitest.com.cert\n\n#     KeyFile = \nintegration/fixtures/https/snitest.com.key\n\n#     [[entryPoints.https.tls.certificates]]\n#     CertFile = \nintegration/fixtures/https/snitest.org.cert\n\n#     KeyFile = \nintegration/fixtures/https/snitest.org.key\n\n#\n# To enable basic auth on an entrypoint\n# with 2 user/pass: test:test and test2:test2\n# Passwords can be encoded in MD5, SHA1 and BCrypt: you can use htpasswd to generate those ones\n# [entryPoints]\n#   [entryPoints.http]\n#   address = \n:80\n\n#   [entryPoints.http.auth.basic]\n#   users = [\ntest:$apr1$H6uskkkW$IgXLP6ewTrSuBkTrqE8wj/\n, \ntest2:$apr1$d9hr9HBB$4HxwgUir3HP4EsggP/QNo0\n]\n#\n# To enable digest auth on an entrypoint\n# with 2 user/realm/pass: test:traefik:test and test2:traefik:test2\n# You can use htdigest to generate those ones\n# [entryPoints]\n#   [entryPoints.http]\n#   address = \n:80\n\n#   [entryPoints.http.auth.basic]\n#   users = [\ntest:traefik:a2688e031edb4be6a3797f3882655c05 \n, \ntest2:traefik:518845800f9e2bfb1f1f740ec24f074e\n]\n#\n# To specify an https entrypoint with a minimum TLS version, and specifying an array of cipher suites (from crypto/tls):\n# [entryPoints]\n#   [entryPoints.https]\n#   address = \n:443\n\n#     [entryPoints.https.tls]\n#     MinVersion = \nVersionTLS12\n\n#     CipherSuites = [\nTLS_RSA_WITH_AES_256_GCM_SHA384\n]\n#       [[entryPoints.https.tls.certificates]]\n#       CertFile = \nintegration/fixtures/https/snitest.com.cert\n\n#       KeyFile = \nintegration/fixtures/https/snitest.com.key\n\n#       [[entryPoints.https.tls.certificates]]\n#       CertFile = \nintegration/fixtures/https/snitest.org.cert\n\n#       KeyFile = \nintegration/fixtures/https/snitest.org.key\n\n\n# To enable compression support using gzip format:\n# [entryPoints]\n#   [entryPoints.http]\n#   address = \n:80\n\n#   compress = true\n\n[entryPoints]\n  [entryPoints.http]\n  address = \n:80\n\n\n\n\n\nRetry configuration\n\n\n# Enable retry sending request if network error\n#\n# Optional\n#\n[retry]\n\n# Number of attempts\n#\n# Optional\n# Default: (number servers in backend) -1\n#\n# attempts = 3\n\n\n\n\nACME (Let's Encrypt) configuration\n\n\n# Sample entrypoint configuration when using ACME\n[entryPoints]\n  [entryPoints.https]\n  address = \n:443\n\n    [entryPoints.https.tls]\n\n# Enable ACME (Let's Encrypt): automatic SSL\n#\n# Optional\n#\n[acme]\n\n# Email address used for registration\n#\n# Required\n#\nemail = \ntest@traefik.io\n\n\n# File or key used for certificates storage.\n# WARNING, if you use Traefik in Docker, you have 2 options:\n#  - create a file on your host and mount it as a volume\n#      storageFile = \nacme.json\n\n#      $ docker run -v \n/my/host/acme.json:acme.json\n traefik\n#  - mount the folder containing the file as a volume\n#      storageFile = \n/etc/traefik/acme/acme.json\n\n#      $ docker run -v \n/my/host/acme:/etc/traefik/acme\n traefik\n#\n# Required\n#\nstorage = \nacme.json\n # or \ntraefik/acme/account\n if using KV store\n\n# Entrypoint to proxy acme challenge/apply certificates to.\n# WARNING, must point to an entrypoint on port 443\n#\n# Required\n#\nentryPoint = \nhttps\n\n\n# Use a DNS based acme challenge rather than external HTTPS access, e.g. for a firewalled server\n# Select the provider that matches the DNS domain that will host the challenge TXT record,\n# and provide environment variables with access keys to enable setting it:\n#  - cloudflare: CLOUDFLARE_EMAIL, CLOUDFLARE_API_KEY\n#  - digitalocean: DO_AUTH_TOKEN\n#  - dnsimple: DNSIMPLE_EMAIL, DNSIMPLE_API_KEY\n#  - dnsmadeeasy: DNSMADEEASY_API_KEY, DNSMADEEASY_API_SECRET\n#  - exoscale: EXOSCALE_API_KEY, EXOSCALE_API_SECRET\n#  - gandi: GANDI_API_KEY\n#  - linode: LINODE_API_KEY\n#  - manual: none, but run traefik interactively \n turn on acmeLogging to see instructions \n press Enter\n#  - namecheap: NAMECHEAP_API_USER, NAMECHEAP_API_KEY\n#  - rfc2136: RFC2136_TSIG_KEY, RFC2136_TSIG_SECRET, RFC2136_TSIG_ALGORITHM, RFC2136_NAMESERVER\n#  - route53: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, or configured user/instance IAM profile\n#  - dyn: DYN_CUSTOMER_NAME, DYN_USER_NAME, DYN_PASSWORD\n#  - vultr: VULTR_API_KEY\n#  - ovh: OVH_ENDPOINT, OVH_APPLICATION_KEY, OVH_APPLICATION_SECRET, OVH_CONSUMER_KEY\n#  - pdns: PDNS_API_KEY, PDNS_API_URL\n#\n# Optional\n#\n# dnsProvider = \ndigitalocean\n\n\n# By default, the dnsProvider will verify the TXT DNS challenge record before letting ACME verify\n# If delayDontCheckDNS is greater than zero, avoid this \n instead just wait so many seconds.\n# Useful if internal networks block external DNS queries\n#\n# Optional\n#\n# delayDontCheckDNS = 0\n\n# If true, display debug log messages from the acme client library\n#\n# Optional\n#\n# acmeLogging = true\n\n# Enable on demand certificate. This will request a certificate from Let's Encrypt during the first TLS handshake for a hostname that does not yet have a certificate.\n# WARNING, TLS handshakes will be slow when requesting a hostname certificate for the first time, this can leads to DoS attacks.\n# WARNING, Take note that Let's Encrypt have rate limiting: https://letsencrypt.org/docs/rate-limits\n#\n# Optional\n#\n# onDemand = true\n\n# Enable certificate generation on frontends Host rules. This will request a certificate from Let's Encrypt for each frontend with a Host rule.\n# For example, a rule Host:test1.traefik.io,test2.traefik.io will request a certificate with main domain test1.traefik.io and SAN test2.traefik.io.\n#\n# Optional\n#\n# OnHostRule = true\n\n# CA server to use\n# Uncomment the line to run on the staging let's encrypt server\n# Leave comment to go to prod\n#\n# Optional\n#\n# caServer = \nhttps://acme-staging.api.letsencrypt.org/directory\n\n\n# Domains list\n# You can provide SANs (alternative domains) to each main domain\n# All domains must have A/AAAA records pointing to Traefik\n# WARNING, Take note that Let's Encrypt have rate limiting: https://letsencrypt.org/docs/rate-limits\n# Each domain \n SANs will lead to a certificate request.\n#\n# [[acme.domains]]\n#   main = \nlocal1.com\n\n#   sans = [\ntest1.local1.com\n, \ntest2.local1.com\n]\n# [[acme.domains]]\n#   main = \nlocal2.com\n\n#   sans = [\ntest1.local2.com\n, \ntest2x.local2.com\n]\n# [[acme.domains]]\n#   main = \nlocal3.com\n\n# [[acme.domains]]\n#   main = \nlocal4.com\n\n[[acme.domains]]\n   main = \nlocal1.com\n\n   sans = [\ntest1.local1.com\n, \ntest2.local1.com\n]\n[[acme.domains]]\n   main = \nlocal3.com\n\n[[acme.domains]]\n   main = \nlocal4.com\n\n\n\n\n\nConfiguration backends\n\n\nFile backend\n\n\nLike any other reverse proxy, Tr\u00e6f\u026ak can be configured with a file. You have two choices:\n\n\n\n\nsimply add your configuration at the end of the global configuration file \ntraefik.toml\n:\n\n\n\n\n# traefik.toml\nlogLevel = \nDEBUG\n\ndefaultEntryPoints = [\nhttp\n, \nhttps\n]\n[entryPoints]\n  [entryPoints.http]\n  address = \n:80\n\n    [entryPoints.http.redirect]\n      entryPoint = \nhttps\n\n  [entryPoints.https]\n  address = \n:443\n\n    [entryPoints.https.tls]\n      [[entryPoints.https.tls.certificates]]\n      CertFile = \nintegration/fixtures/https/snitest.com.cert\n\n      KeyFile = \nintegration/fixtures/https/snitest.com.key\n\n      [[entryPoints.https.tls.certificates]]\n      CertFile = \nintegration/fixtures/https/snitest.org.cert\n\n      KeyFile = \nintegration/fixtures/https/snitest.org.key\n\n\n[file]\n\n# rules\n[backends]\n  [backends.backend1]\n    [backends.backend1.circuitbreaker]\n      expression = \nNetworkErrorRatio() \n 0.5\n\n    [backends.backend1.servers.server1]\n    url = \nhttp://172.17.0.2:80\n\n    weight = 10\n    [backends.backend1.servers.server2]\n    url = \nhttp://172.17.0.3:80\n\n    weight = 1\n  [backends.backend2]\n    [backends.backend1.maxconn]\n      amount = 10\n      extractorfunc = \nrequest.host\n\n    [backends.backend2.LoadBalancer]\n      method = \ndrr\n\n    [backends.backend2.servers.server1]\n    url = \nhttp://172.17.0.4:80\n\n    weight = 1\n    [backends.backend2.servers.server2]\n    url = \nhttp://172.17.0.5:80\n\n    weight = 2\n\n[frontends]\n  [frontends.frontend1]\n  backend = \nbackend2\n\n    [frontends.frontend1.routes.test_1]\n    rule = \nHost:test.localhost\n\n  [frontends.frontend2]\n  backend = \nbackend1\n\n  passHostHeader = true\n  priority = 10\n  entrypoints = [\nhttps\n] # overrides defaultEntryPoints\n    [frontends.frontend2.routes.test_1]\n    rule = \nHost:{subdomain:[a-z]+}.localhost\n\n  [frontends.frontend3]\n  entrypoints = [\nhttp\n, \nhttps\n] # overrides defaultEntryPoints\n  backend = \nbackend2\n\n    rule = \nPath:/test\n\n\n\n\n\n\n\nor put your rules in a separate file, for example \nrules.toml\n:\n\n\n\n\n# traefik.toml\nlogLevel = \nDEBUG\n\n[entryPoints]\n  [entryPoints.http]\n  address = \n:80\n\n    [entryPoints.http.redirect]\n      entryPoint = \nhttps\n\n  [entryPoints.https]\n  address = \n:443\n\n    [entryPoints.https.tls]\n      [[entryPoints.https.tls.certificates]]\n      CertFile = \nintegration/fixtures/https/snitest.com.cert\n\n      KeyFile = \nintegration/fixtures/https/snitest.com.key\n\n      [[entryPoints.https.tls.certificates]]\n      CertFile = \nintegration/fixtures/https/snitest.org.cert\n\n      KeyFile = \nintegration/fixtures/https/snitest.org.key\n\n\n[file]\nfilename = \nrules.toml\n\n\n\n\n\n# rules.toml\n[backends]\n  [backends.backend1]\n    [backends.backend1.circuitbreaker]\n      expression = \nNetworkErrorRatio() \n 0.5\n\n    [backends.backend1.servers.server1]\n    url = \nhttp://172.17.0.2:80\n\n    weight = 10\n    [backends.backend1.servers.server2]\n    url = \nhttp://172.17.0.3:80\n\n    weight = 1\n  [backends.backend2]\n    [backends.backend1.maxconn]\n      amount = 10\n      extractorfunc = \nrequest.host\n\n    [backends.backend2.LoadBalancer]\n      method = \ndrr\n\n    [backends.backend2.servers.server1]\n    url = \nhttp://172.17.0.4:80\n\n    weight = 1\n    [backends.backend2.servers.server2]\n    url = \nhttp://172.17.0.5:80\n\n    weight = 2\n\n[frontends]\n  [frontends.frontend1]\n  backend = \nbackend2\n\n    [frontends.frontend1.routes.test_1]\n    rule = \nHost:test.localhost\n\n  [frontends.frontend2]\n  backend = \nbackend1\n\n  passHostHeader = true\n  priority = 10\n  entrypoints = [\nhttps\n] # overrides defaultEntryPoints\n    [frontends.frontend2.routes.test_1]\n    rule = \nHost:{subdomain:[a-z]+}.localhost\n\n  [frontends.frontend3]\n  entrypoints = [\nhttp\n, \nhttps\n] # overrides defaultEntryPoints\n  backend = \nbackend2\n\n    rule = \nPath:/test\n\n\n\n\n\nIf you want Tr\u00e6f\u026ak to watch file changes automatically, just add:\n\n\n[file]\nwatch = true\n\n\n\n\nAPI backend\n\n\nTr\u00e6fik can be configured using a RESTful api.\nTo enable it:\n\n\n[web]\naddress = \n:8080\n\n\n# SSL certificate and key used\n#\n# Optional\n#\n# CertFile = \ntraefik.crt\n\n# KeyFile = \ntraefik.key\n\n#\n# Set REST API to read-only mode\n#\n# Optional\n# ReadOnly = false\n#\n# To enable more detailed statistics\n# [web.statistics]\n#   RecentErrors = 10\n#\n# To enable Traefik to export internal metrics to Prometheus\n# [web.metrics.prometheus]\n#   Buckets=[0.1,0.3,1.2,5]\n#\n# To enable basic auth on the webui\n# with 2 user/pass: test:test and test2:test2\n# Passwords can be encoded in MD5, SHA1 and BCrypt: you can use htpasswd to generate those ones\n#   [web.auth.basic]\n#     users = [\ntest:$apr1$H6uskkkW$IgXLP6ewTrSuBkTrqE8wj/\n, \ntest2:$apr1$d9hr9HBB$4HxwgUir3HP4EsggP/QNo0\n]\n# To enable digest auth on the webui\n# with 2 user/realm/pass: test:traefik:test and test2:traefik:test2\n# You can use htdigest to generate those ones\n#   [web.auth.digest]\n#     users = [\ntest:traefik:a2688e031edb4be6a3797f3882655c05 \n, \ntest2:traefik:518845800f9e2bfb1f1f740ec24f074e\n]\n\n\n\n\n\n\n\n/\n: provides a simple HTML frontend of Tr\u00e6fik\n\n\n\n\n\n\n\n\n\n\n/ping\n: \nGET\n simple endpoint to check for Tr\u00e6fik process liveness.\n\n\n\n\n$ curl -sv \nhttp://localhost:8080/ping\n\n*   Trying ::1...\n* Connected to localhost (::1) port 8080 (#0)\n\n GET /ping HTTP/1.1\n\n Host: localhost:8080\n\n User-Agent: curl/7.43.0\n\n Accept: */*\n\n\n\n HTTP/1.1 200 OK\n\n Date: Thu, 25 Aug 2016 01:35:36 GMT\n\n Content-Length: 2\n\n Content-Type: text/plain; charset=utf-8\n\n\n* Connection #0 to host localhost left intact\nOK\n\n\n\n\n\n\n/health\n: \nGET\n json metrics\n\n\n\n\n$ curl -s \nhttp://localhost:8080/health\n | jq .\n{\n  // Tr\u00e6f\u026ak PID\n  \npid\n: 2458,\n  // Tr\u00e6f\u026ak server uptime (formated time)\n  \nuptime\n: \n39m6.885931127s\n,\n  //  Tr\u00e6f\u026ak server uptime in seconds\n  \nuptime_sec\n: 2346.885931127,\n  // current server date\n  \ntime\n: \n2015-10-07 18:32:24.362238909 +0200 CEST\n,\n  // current server date in seconds\n  \nunixtime\n: 1444235544,\n  // count HTTP response status code in realtime\n  \nstatus_code_count\n: {\n    \n502\n: 1\n  },\n  // count HTTP response status code since Tr\u00e6f\u026ak started\n  \ntotal_status_code_count\n: {\n    \n200\n: 7,\n    \n404\n: 21,\n    \n502\n: 13\n  },\n  // count HTTP response\n  \ncount\n: 1,\n  // count HTTP response\n  \ntotal_count\n: 41,\n  // sum of all response time (formated time)\n  \ntotal_response_time\n: \n35.456865605s\n,\n  // sum of all response time in seconds\n  \ntotal_response_time_sec\n: 35.456865605,\n  // average response time (formated time)\n  \naverage_response_time\n: \n864.8016ms\n,\n  // average response time in seconds\n  \naverage_response_time_sec\n: 0.8648016000000001,\n\n  // request statistics [requires --web.statistics to be set]\n  // ten most recent requests with 4xx and 5xx status codes\n  \nrecent_errors\n: [\n    {\n      // status code\n      \nstatus_code\n: 500,\n      // description of status code\n      \nstatus\n: \nInternal Server Error\n,\n      // request HTTP method\n      \nmethod\n: \nGET\n,\n      // request hostname\n      \nhost\n: \nlocalhost\n,\n      // request path\n      \npath\n: \n/path\n,\n      // RFC 3339 formatted date/time\n      \ntime\n: \n2016-10-21T16:59:15.418495872-07:00\n\n    }\n  ]\n}\n\n\n\n\n\n\n/api\n: \nGET\n configuration for all providers\n\n\n\n\n$ curl -s \nhttp://localhost:8080/api\n | jq .\n{\n  \nfile\n: {\n    \nfrontends\n: {\n      \nfrontend2\n: {\n        \nroutes\n: {\n          \ntest_2\n: {\n            \nrule\n: \nPath:/test\n\n          }\n        },\n        \nbackend\n: \nbackend1\n\n      },\n      \nfrontend1\n: {\n        \nroutes\n: {\n          \ntest_1\n: {\n            \nrule\n: \nHost:test.localhost\n\n          }\n        },\n        \nbackend\n: \nbackend2\n\n      }\n    },\n    \nbackends\n: {\n      \nbackend2\n: {\n        \nloadBalancer\n: {\n          \nmethod\n: \ndrr\n\n        },\n        \nservers\n: {\n          \nserver2\n: {\n            \nweight\n: 2,\n            \nURL\n: \nhttp://172.17.0.5:80\n\n          },\n          \nserver1\n: {\n            \nweight\n: 1,\n            \nurl\n: \nhttp://172.17.0.4:80\n\n          }\n        }\n      },\n      \nbackend1\n: {\n        \nloadBalancer\n: {\n          \nmethod\n: \nwrr\n\n        },\n        \ncircuitBreaker\n: {\n          \nexpression\n: \nNetworkErrorRatio() \n 0.5\n\n        },\n        \nservers\n: {\n          \nserver2\n: {\n            \nweight\n: 1,\n            \nurl\n: \nhttp://172.17.0.3:80\n\n          },\n          \nserver1\n: {\n            \nweight\n: 10,\n            \nurl\n: \nhttp://172.17.0.2:80\n\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\n\n\n/api/providers\n: \nGET\n providers\n\n\n/api/providers/{provider}\n: \nGET\n or \nPUT\n provider\n\n\n/api/providers/{provider}/backends\n: \nGET\n backends\n\n\n/api/providers/{provider}/backends/{backend}\n: \nGET\n a backend\n\n\n/api/providers/{provider}/backends/{backend}/servers\n: \nGET\n servers in a backend\n\n\n/api/providers/{provider}/backends/{backend}/servers/{server}\n: \nGET\n a server in a backend\n\n\n/api/providers/{provider}/frontends\n: \nGET\n frontends\n\n\n/api/providers/{provider}/frontends/{frontend}\n: \nGET\n a frontend\n\n\n/api/providers/{provider}/frontends/{frontend}/routes\n: \nGET\n routes in a frontend\n\n\n\n\n/api/providers/{provider}/frontends/{frontend}/routes/{route}\n: \nGET\n a route in a frontend\n\n\n\n\n\n\n/metrics\n: You can enable Traefik to export internal metrics to different monitoring systems (Only Prometheus is supported at the moment).\n\n\n\n\n\n\n$ traefik --web.metrics.prometheus --web.metrics.prometheus.buckets=\n0.1,0.3,1.2,5\n\n\n\n\n\nDocker backend\n\n\nTr\u00e6f\u026ak can be configured to use Docker as a backend configuration:\n\n\n################################################################\n# Docker configuration backend\n################################################################\n\n# Enable Docker configuration backend\n#\n# Optional\n#\n[docker]\n\n# Docker server endpoint. Can be a tcp or a unix socket endpoint.\n#\n# Required\n#\nendpoint = \nunix:///var/run/docker.sock\n\n\n# Default domain used.\n# Can be overridden by setting the \ntraefik.domain\n label on a container.\n#\n# Required\n#\ndomain = \ndocker.localhost\n\n\n# Enable watch docker changes\n#\n# Optional\n#\nwatch = true\n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename = \ndocker.tmpl\n\n\n# Expose containers by default in traefik\n# If set to false, containers that don't have `traefik.enable=true` will be ignored\n#\n# Optional\n# Default: true\n#\nexposedbydefault = true\n\n# Use the IP address from the binded port instead of the inner network one. For specific use-case :)\n\n#\n# Optional\n# Default: false\n#\nusebindportip = true\n# Use Swarm Mode services as data provider\n#\n# Optional\n# Default: false\n#\nswarmmode = false\n\n\n# Enable docker TLS connection\n#\n#  [docker.tls]\n#  ca = \n/etc/ssl/ca.crt\n\n#  cert = \n/etc/ssl/docker.crt\n\n#  key = \n/etc/ssl/docker.key\n\n#  insecureskipverify = true\n\n\n\n\nLabels can be used on containers to override default behaviour:\n\n\n\n\ntraefik.backend=foo\n: assign the container to \nfoo\n backend\n\n\ntraefik.backend.maxconn.amount=10\n: set a maximum number of connections to the backend. Must be used in conjunction with the below label to take effect.\n\n\ntraefik.backend.maxconn.extractorfunc=client.ip\n: set the function to be used against the request to determine what to limit maximum connections to the backend by. Must be used in conjunction with the above label to take effect.\n\n\ntraefik.backend.loadbalancer.method=drr\n: override the default \nwrr\n load balancer algorithm\n\n\ntraefik.backend.loadbalancer.sticky=true\n: enable backend sticky sessions\n\n\ntraefik.backend.loadbalancer.swarm=true\n: use Swarm's inbuilt load balancer (only relevant under Swarm Mode).\n\n\ntraefik.backend.circuitbreaker.expression=NetworkErrorRatio() \n 0.5\n: create a \ncircuit breaker\n to be used against the backend\n\n\ntraefik.port=80\n: register this port. Useful when the container exposes multiples ports.\n\n\ntraefik.protocol=https\n: override the default \nhttp\n protocol\n\n\ntraefik.weight=10\n: assign this weight to the container\n\n\ntraefik.enable=false\n: disable this container in Tr\u00e6f\u026ak\n\n\ntraefik.frontend.rule=Host:test.traefik.io\n: override the default frontend rule (Default: \nHost:{containerName}.{domain}\n).\n\n\ntraefik.frontend.passHostHeader=true\n: forward client \nHost\n header to the backend.\n\n\ntraefik.frontend.priority=10\n: override default frontend priority\n\n\ntraefik.frontend.entryPoints=http,https\n: assign this frontend to entry points \nhttp\n and \nhttps\n. Overrides \ndefaultEntryPoints\n.\n\n\ntraefik.docker.network\n: Set the docker network to use for connections to this container. If a container is linked to several networks, be sure to set the proper network name (you can check with docker inspect \n) otherwise it will randomly pick one (depending on how docker is returning them). For instance when deploying docker \nstack\n from compose files, the compose defined networks will be prefixed with the \nstack\n name.\n\n\n\n\nNB: when running inside a container, Tr\u00e6f\u026ak will need network access through \ndocker network connect \nnetwork\n \ntraefik-container\n\n\nMarathon backend\n\n\nTr\u00e6f\u026ak can be configured to use Marathon as a backend configuration:\n\n\n################################################################\n# Mesos/Marathon configuration backend\n################################################################\n\n# Enable Marathon configuration backend\n#\n# Optional\n#\n[marathon]\n\n# Marathon server endpoint.\n# You can also specify multiple endpoint for Marathon:\n# endpoint := \nhttp://10.241.1.71:8080,10.241.1.72:8080,10.241.1.73:8080\n\n#\n# Required\n#\nendpoint = \nhttp://127.0.0.1:8080\n\n\n# Enable watch Marathon changes\n#\n# Optional\n#\nwatch = true\n\n# Default domain used.\n#\n# Required\n#\ndomain = \nmarathon.localhost\n\n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename = \nmarathon.tmpl\n\n\n# Expose Marathon apps by default in traefik\n#\n# Optional\n# Default: true\n#\n# exposedByDefault = true\n\n# Convert Marathon groups to subdomains\n# Default behavior: /foo/bar/myapp =\n foo-bar-myapp.{defaultDomain}\n# with groupsAsSubDomains enabled: /foo/bar/myapp =\n myapp.bar.foo.{defaultDomain}\n#\n# Optional\n# Default: false\n#\n# groupsAsSubDomains = true\n\n#\u00a0Enable compatibility with marathon-lb labels\n#\n# Optional\n# Default: false\n#\n# marathonLBCompatibility = true\n\n# Enable Marathon basic authentication\n#\n# Optional\n#\n#  [marathon.basic]\n#  httpBasicAuthUser = \nfoo\n\n#  httpBasicPassword = \nbar\n\n\n# TLS client configuration. https://golang.org/pkg/crypto/tls/#Config\n#\n# Optional\n#\n# [marathon.TLS]\n# CA = \n/etc/ssl/ca.crt\n\n# Cert = \n/etc/ssl/marathon.cert\n\n# Key = \n/etc/ssl/marathon.key\n\n# InsecureSkipVerify = true\n\n# DCOSToken for DCOS environment, This will override the Authorization header\n#\n# Optional\n#\n# dcosToken = \nxxxxxx\n\n\n# Override DialerTimeout\n# Amount of time in seconds to allow the Marathon provider to wait to open a TCP\n# connection to a Marathon master\n#\n# Optional\n# Default: 60\n# dialerTimeout = 5\n\n# Set the TCP Keep Alive interval (in seconds) for the Marathon HTTP Client\n#\n# Optional\n# Default: 10\n#\n# keepAlive = 10\n\n\n\n\nLabels can be used on containers to override default behaviour:\n\n\n\n\ntraefik.backend=foo\n: assign the application to \nfoo\n backend\n\n\ntraefik.backend.maxconn.amount=10\n: set a maximum number of connections to the backend. Must be used in conjunction with the below label to take effect.\n\n\ntraefik.backend.maxconn.extractorfunc=client.ip\n: set the function to be used against the request to determine what to limit maximum connections to the backend by. Must be used in conjunction with the above label to take effect.\n\n\ntraefik.backend.loadbalancer.method=drr\n: override the default \nwrr\n load balancer algorithm\n\n\ntraefik.backend.loadbalancer.sticky=true\n: enable backend sticky sessions\n\n\ntraefik.backend.circuitbreaker.expression=NetworkErrorRatio() \n 0.5\n: create a \ncircuit breaker\n to be used against the backend\n\n\ntraefik.portIndex=1\n: register port by index in the application's ports array. Useful when the application exposes multiple ports.\n\n\ntraefik.port=80\n: register the explicit application port value. Cannot be used alongside \ntraefik.portIndex\n.\n\n\ntraefik.protocol=https\n: override the default \nhttp\n protocol\n\n\ntraefik.weight=10\n: assign this weight to the application\n\n\ntraefik.enable=false\n: disable this application in Tr\u00e6f\u026ak\n\n\ntraefik.frontend.rule=Host:test.traefik.io\n: override the default frontend rule (Default: \nHost:{containerName}.{domain}\n).\n\n\ntraefik.frontend.passHostHeader=true\n: forward client \nHost\n header to the backend.\n\n\ntraefik.frontend.priority=10\n: override default frontend priority\n\n\ntraefik.frontend.entryPoints=http,https\n: assign this frontend to entry points \nhttp\n and \nhttps\n. Overrides \ndefaultEntryPoints\n.\n\n\n\n\nMesos generic backend\n\n\nTr\u00e6f\u026ak can be configured to use Mesos as a backend configuration:\n\n\n################################################################\n# Mesos configuration backend\n################################################################\n\n# Enable Mesos configuration backend\n#\n# Optional\n#\n[mesos]\n\n# Mesos server endpoint.\n# You can also specify multiple endpoint for Mesos:\n# endpoint = \n192.168.35.40:5050,192.168.35.41:5050,192.168.35.42:5050\n\n# endpoint = \nzk://192.168.35.20:2181,192.168.35.21:2181,192.168.35.22:2181/mesos\n\n#\n# Required\n#\nendpoint = \nhttp://127.0.0.1:8080\n\n\n# Enable watch Mesos changes\n#\n# Optional\n#\nwatch = true\n\n# Default domain used.\n# Can be overridden by setting the \ntraefik.domain\n label on an application.\n#\n# Required\n#\ndomain = \nmesos.localhost\n\n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename = \nmesos.tmpl\n\n\n# Expose Mesos apps by default in traefik\n#\n# Optional\n# Default: false\n#\n# ExposedByDefault = true\n\n# TLS client configuration. https://golang.org/pkg/crypto/tls/#Config\n#\n# Optional\n#\n# [mesos.TLS]\n# InsecureSkipVerify = true\n\n# Zookeeper timeout (in seconds)\n#\n# Optional\n# Default: 30\n#\n# ZkDetectionTimeout = 30\n\n# Polling interval (in seconds)\n#\n# Optional\n# Default: 30\n#\n# RefreshSeconds = 30\n\n# IP sources (e.g. host, docker, mesos, rkt)\n#\n# Optional\n#\n# IPSources = \nhost\n\n\n# HTTP Timeout (in seconds)\n#\n# Optional\n# Default: 30\n#\n# StateTimeoutSecond = \n30\n\n\n\n\n\nKubernetes Ingress backend\n\n\nTr\u00e6f\u026ak can be configured to use Kubernetes Ingress as a backend configuration:\n\n\n################################################################\n# Kubernetes Ingress configuration backend\n################################################################\n# Enable Kubernetes Ingress configuration backend\n#\n# Optional\n#\n[kubernetes]\n\n# Kubernetes server endpoint\n#\n# When deployed as a replication controller in Kubernetes,\n# Traefik will use env variable KUBERNETES_SERVICE_HOST\n# and KUBERNETES_SERVICE_PORT_HTTPS as endpoint\n# Secure token will be found in /var/run/secrets/kubernetes.io/serviceaccount/token\n# and SSL CA cert in /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n#\n# Optional\n#\n# endpoint = \nhttp://localhost:8080\n\n# namespaces = [\ndefault\n,\nproduction\n]\n#\n# See: http://kubernetes.io/docs/user-guide/labels/#list-and-watch-filtering\n# labelselector = \nA and not B\n\n#\n\n\n\n\nAnnotations can be used on containers to override default behaviour for the whole Ingress resource:\n\n\n\n\ntraefik.frontend.rule.type: PathPrefixStrip\n: override the default frontend rule type (Default: \nPathPrefix\n).\n\n\n\n\nAnnotations can be used on the Kubernetes service to override default behaviour:\n\n\n\n\ntraefik.backend.loadbalancer.method=drr\n: override the default \nwrr\n load balancer algorithm\n\n\ntraefik.backend.loadbalancer.sticky=true\n: enable backend sticky sessions\n\n\n\n\nYou can find here an example \ningress\n and \nreplication controller\n.\n\n\nAdditionally, an annotation can be used on Kubernetes services to set the \ncircuit breaker expression\n for a backend.\n\n\n\n\ntraefik.backend.circuitbreaker: \nexpression\n: set the circuit breaker expression for the backend (Default: nil).\n\n\n\n\nConsul backend\n\n\nTr\u00e6f\u026ak can be configured to use Consul as a backend configuration:\n\n\n################################################################\n# Consul KV configuration backend\n################################################################\n\n# Enable Consul KV configuration backend\n#\n# Optional\n#\n[consul]\n\n# Consul server endpoint\n#\n# Required\n#\nendpoint = \n127.0.0.1:8500\n\n\n# Enable watch Consul changes\n#\n# Optional\n#\nwatch = true\n\n# Prefix used for KV store.\n#\n# Optional\n#\nprefix = \ntraefik\n\n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename = \nconsul.tmpl\n\n\n# Enable consul TLS connection\n#\n# Optional\n#\n# [consul.tls]\n# ca = \n/etc/ssl/ca.crt\n\n# cert = \n/etc/ssl/consul.crt\n\n# key = \n/etc/ssl/consul.key\n\n# insecureskipverify = true\n\n\n\n\nPlease refer to the \nKey Value storage structure\n section to get documentation on traefik KV structure.\n\n\nConsul catalog backend\n\n\nTr\u00e6f\u026ak can be configured to use service discovery catalog of Consul as a backend configuration:\n\n\n################################################################\n# Consul Catalog configuration backend\n################################################################\n\n# Enable Consul Catalog configuration backend\n#\n# Optional\n#\n[consulCatalog]\n\n# Consul server endpoint\n#\n# Required\n#\nendpoint = \n127.0.0.1:8500\n\n\n# Default domain used.\n#\n# Optional\n#\ndomain = \nconsul.localhost\n\n\n# Prefix for Consul catalog tags\n#\n# Optional\n#\nprefix = \ntraefik\n\n\n\n\n\nThis backend will create routes matching on hostname based on the service name\nused in consul.\n\n\nAdditional settings can be defined using Consul Catalog tags:\n\n\n\n\ntraefik.enable=false\n: disable this container in Tr\u00e6f\u026ak\n\n\ntraefik.protocol=https\n: override the default \nhttp\n protocol\n\n\ntraefik.backend.weight=10\n: assign this weight to the container\n\n\ntraefik.backend.circuitbreaker=NetworkErrorRatio() \n 0.5\n\n\ntraefik.backend.loadbalancer=drr\n: override the default load balancing mode\n\n\ntraefik.backend.maxconn.amount=10\n: set a maximum number of connections to the backend. Must be used in conjunction with the below label to take effect.\n\n\ntraefik.backend.maxconn.extractorfunc=client.ip\n: set the function to be used against the request to determine what to limit maximum connections to the backend by. Must be used in conjunction with the above label to take effect.\n\n\ntraefik.frontend.rule=Host:test.traefik.io\n: override the default frontend rule (Default: \nHost:{containerName}.{domain}\n).\n\n\ntraefik.frontend.passHostHeader=true\n: forward client \nHost\n header to the backend.\n\n\ntraefik.frontend.priority=10\n: override default frontend priority\n\n\ntraefik.frontend.entryPoints=http,https\n: assign this frontend to entry points \nhttp\n and \nhttps\n. Overrides \ndefaultEntryPoints\n.\n\n\n\n\nEtcd backend\n\n\nTr\u00e6f\u026ak can be configured to use Etcd as a backend configuration:\n\n\n################################################################\n# Etcd configuration backend\n################################################################\n\n# Enable Etcd configuration backend\n#\n# Optional\n#\n[etcd]\n\n# Etcd server endpoint\n#\n# Required\n#\nendpoint = \n127.0.0.1:2379\n\n\n# Enable watch Etcd changes\n#\n# Optional\n#\nwatch = true\n\n# Prefix used for KV store.\n#\n# Optional\n#\nprefix = \n/traefik\n\n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename = \netcd.tmpl\n\n\n# Enable etcd TLS connection\n#\n# Optional\n#\n# [etcd.tls]\n# ca = \n/etc/ssl/ca.crt\n\n# cert = \n/etc/ssl/etcd.crt\n\n# key = \n/etc/ssl/etcd.key\n\n# insecureskipverify = true\n\n\n\n\nPlease refer to the \nKey Value storage structure\n section to get documentation on traefik KV structure.\n\n\nZookeeper backend\n\n\nTr\u00e6f\u026ak can be configured to use Zookeeper as a backend configuration:\n\n\n################################################################\n# Zookeeper configuration backend\n################################################################\n\n# Enable Zookeeperconfiguration backend\n#\n# Optional\n#\n[zookeeper]\n\n# Zookeeper server endpoint\n#\n# Required\n#\nendpoint = \n127.0.0.1:2181\n\n\n# Enable watch Zookeeper changes\n#\n# Optional\n#\nwatch = true\n\n# Prefix used for KV store.\n#\n# Optional\n#\nprefix = \ntraefik\n\n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename = \nzookeeper.tmpl\n\n\n\n\n\nPlease refer to the \nKey Value storage structure\n section to get documentation on traefik KV structure.\n\n\nBoltDB backend\n\n\nTr\u00e6f\u026ak can be configured to use BoltDB as a backend configuration:\n\n\n################################################################\n# BoltDB configuration backend\n################################################################\n\n# Enable BoltDB configuration backend\n#\n# Optional\n#\n[boltdb]\n\n# BoltDB file\n#\n# Required\n#\nendpoint = \n/my.db\n\n\n# Enable watch BoltDB changes\n#\n# Optional\n#\nwatch = true\n\n# Prefix used for KV store.\n#\n# Optional\n#\nprefix = \n/traefik\n\n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename = \nboltdb.tmpl\n\n\n\n\n\nEureka backend\n\n\nTr\u00e6f\u026ak can be configured to use Eureka as a backend configuration:\n\n\n################################################################\n# Eureka configuration backend\n################################################################\n\n# Enable Eureka configuration backend\n#\n# Optional\n#\n[eureka]\n\n# Eureka server endpoint.\n# endpoint := \nhttp://my.eureka.server/eureka\n\n#\n# Required\n#\nendpoint = \nhttp://my.eureka.server/eureka\n\n\n# Override default configuration time between refresh\n#\n# Optional\n# default 30s\ndelay = \n1m\n\n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename = \neureka.tmpl\n\n\n\n\n\nPlease refer to the \nKey Value storage structure\n section to get documentation on traefik KV structure.\n\n\nECS backend\n\n\nTr\u00e6f\u026ak can be configured to use Amazon ECS as a backend configuration:\n\n\n################################################################\n# ECS configuration backend\n################################################################\n\n# Enable ECS configuration backend\n#\n# Optional\n#\n[ecs]\n\n# ECS Cluster Name\n#\n# Optional\n# Default: \ndefault\n\n#\nCluster = \ndefault\n\n\n# Enable watch ECS changes\n#\n# Optional\n# Default: true\n#\nWatch = true\n\n# Polling interval (in seconds)\n#\n# Optional\n# Default: 15\n#\nRefreshSeconds = 15\n\n# Expose ECS services by default in traefik\n#\n# Optional\n# Default: true\n#\nExposedByDefault = false\n\n# Region to use when connecting to AWS\n#\n# Optional\n#\n# Region = \nus-east-1\n\n\n# AccessKeyID to use when connecting to AWS\n#\n# Optional\n#\n# AccessKeyID = \nabc\n\n\n# SecretAccessKey to use when connecting to AWS\n#\n# Optional\n#\n# SecretAccessKey = \n123\n\n\n\n\n\n\nLabels can be used on task containers to override default behaviour:\n\n\n\n\ntraefik.protocol=https\n: override the default \nhttp\n protocol\n\n\ntraefik.weight=10\n: assign this weight to the container\n\n\ntraefik.enable=false\n: disable this container in Tr\u00e6f\u026ak\n\n\ntraefik.frontend.rule=Host:test.traefik.io\n: override the default frontend rule (Default: \nHost:{containerName}.{domain}\n).\n\n\ntraefik.frontend.passHostHeader=true\n: forward client \nHost\n header to the backend.\n\n\ntraefik.frontend.priority=10\n: override default frontend priority\n\n\ntraefik.frontend.entryPoints=http,https\n: assign this frontend to entry points \nhttp\n and \nhttps\n. Overrides \ndefaultEntryPoints\n.\n\n\n\n\nIf \nAccessKeyID\n/\nSecretAccessKey\n is not given credentials will be resolved in the following order:\n\n\n\n\nFrom environment variables; \nAWS_ACCESS_KEY_ID\n, \nAWS_SECRET_ACCESS_KEY\n, and \nAWS_SESSION_TOKEN\n.\n\n\nShared credentials, determined by \nAWS_PROFILE\n and \nAWS_SHARED_CREDENTIALS_FILE\n, defaults to \ndefault\n and \n~/.aws/credentials\n.\n\n\nEC2 instance role or ECS task role\n\n\n\n\nTr\u00e6f\u026ak needs the following policy to read ECS information:\n\n\n{\n    \nVersion\n: \n2012-10-17\n,\n    \nStatement\n: [\n        {\n            \nSid\n: \nTraefik ECS read access\n,\n            \nEffect\n: \nAllow\n,\n            \nAction\n: [\n                \necs:ListTasks\n,\n                \necs:DescribeTasks\n,\n                \necs:DescribeContainerInstances\n,\n                \necs:DescribeTaskDefinition\n,\n                \nec2:DescribeInstances\n\n            ],\n            \nResource\n: [\n                \n*\n\n            ]\n        }\n    ]\n}\n\n\n\n\nRancher backend\n\n\nTr\u00e6f\u026ak can be configured to use Rancher as a backend configuration:\n\n\n################################################################\n# Rancher configuration backend\n################################################################\n\n# Enable Rancher configuration backend\n#\n# Optional\n#\n[rancher]\n\n# Default domain used.\n# Can be overridden by setting the \ntraefik.domain\n label on an service.\n#\n# Required\n#\ndomain = \nrancher.localhost\n\n\n# Enable watch Rancher changes\n#\n# Optional\n# Default: true\n#\nWatch = true\n\n# Expose Rancher services by default in traefik\n#\n# Optional\n# Default: true\n#\nExposedByDefault = false\n\n# Endpoint to use when connecting to Rancher\n#\n# Optional\n# Endpoint = \nhttp://rancherserver.example.com\n\n\n# AccessKey to use when connecting to Rancher\n#\n# Optional\n# AccessKey = \nXXXXXXXXX\n\n\n# SecretKey to use when connecting to Rancher\n#\n# Optional\n# SecretKey = \nXXXXXXXXXXX\n\n\n\n\n\n\nIf you're deploying traefik as a service within rancher, you can alternatively set these labels on the service to let it only fetch data of its current environment. The settings \nendpoint\n, \naccesskey\n and \nsecretkey\n can be omitted then.\n\n\n\n\nio.rancher.container.create_agent=true\n\n\nio.rancher.container.agent.role=environment\n\n\n\n\nLabels can be used on task containers to override default behaviour:\n\n\n\n\ntraefik.protocol=https\n: override the default \nhttp\n protocol\n\n\ntraefik.weight=10\n: assign this weight to the container\n\n\ntraefik.enable=false\n: disable this container in Tr\u00e6f\u026ak\n\n\ntraefik.frontend.rule=Host:test.traefik.io\n: override the default frontend rule (Default: \nHost:{containerName}.{domain}\n).\n\n\ntraefik.frontend.passHostHeader=true\n: forward client \nHost\n header to the backend.\n\n\ntraefik.frontend.priority=10\n: override default frontend priority\n\n\ntraefik.frontend.entryPoints=http,https\n: assign this frontend to entry points \nhttp\n and \nhttps\n. Overrides \ndefaultEntryPoints\n.", 
            "title": "traefik.toml"
        }, 
        {
            "location": "/toml/#global-configuration", 
            "text": "", 
            "title": "Global configuration"
        }, 
        {
            "location": "/toml/#main-section", 
            "text": "# traefik.toml\n################################################################\n# Global configuration\n################################################################\n\n# Timeout in seconds.\n# Duration to give active requests a chance to finish during hot-reloads\n#\n# Optional\n# Default: 10\n#\n# graceTimeOut = 10\n\n# Enable debug mode\n#\n# Optional\n# Default: false\n#\n# debug = true\n\n# Periodically check if a new version has been released\n#\n# Optional\n# Default: true\n#\n# checkNewVersion = false\n\n# Traefik logs file\n# If not defined, logs to stdout\n#\n# Optional\n#\n# traefikLogsFile =  log/traefik.log \n\n# Access logs file\n#\n# Optional\n#\n# accessLogsFile =  log/access.log \n\n# Log level\n#\n# Optional\n# Default:  ERROR \n# Accepted values, in order of severity:  DEBUG ,  INFO ,  WARN ,  ERROR ,  FATAL ,  PANIC \n# Messages at and above the selected level will be logged.\n#\n# logLevel =  ERROR \n\n# Backends throttle duration: minimum duration in seconds between 2 events from providers\n# before applying a new configuration. It avoids unnecessary reloads if multiples events\n# are sent in a short amount of time.\n#\n# Optional\n# Default:  2 \n#\n# ProvidersThrottleDuration =  5 \n\n# Controls the maximum idle (keep-alive) connections to keep per-host. If zero, DefaultMaxIdleConnsPerHost\n# from the Go standard library net/http module is used.\n# If you encounter 'too many open files' errors, you can either increase this\n# value or change the `ulimit`.\n#\n# Optional\n# Default: 200\n#\n# MaxIdleConnsPerHost = 200\n\n# If set to true invalid SSL certificates are accepted for backends.\n# Note: This disables detection of man-in-the-middle attacks so should only be used on secure backend networks.\n# Optional\n# Default: false\n#\n# InsecureSkipVerify = true\n\n# Entrypoints to be used by frontends that do not specify any entrypoint.\n# Each frontend can specify its own entrypoints.\n#\n# Optional\n# Default: [ http ]\n#\n# defaultEntryPoints = [ http ,  https ]", 
            "title": "Main section"
        }, 
        {
            "location": "/toml/#constraints", 
            "text": "In a micro-service architecture, with a central service discovery, setting constraints limits Tr\u00e6f\u026ak scope to a smaller number of routes.  Tr\u00e6f\u026ak filters services according to service attributes/tags set in your configuration backends.  Supported backends:   Docker  Consul K/V  BoltDB  Zookeeper  Etcd  Consul Catalog   Supported filters:   tag   # Constraints definition\n#\n# Optional\n#\n# Simple matching constraint\n# constraints = [ tag==api ]\n#\n# Simple mismatching constraint\n# constraints = [ tag!=api ]\n#\n# Globbing\n# constraints = [ tag==us-* ]\n#\n# Backend-specific constraint\n# [consulCatalog]\n#   endpoint = 127.0.0.1:8500\n#   constraints = [ tag==api ]\n#\n# Multiple constraints\n#   -  tag==  must match with at least one tag\n#   -  tag!=  must match with none of tags\n# constraints = [ tag!=us-* ,  tag!=asia-* ]\n# [consulCatalog]\n#   endpoint = 127.0.0.1:8500\n#   constraints = [ tag==api ,  tag!=v*-beta ]", 
            "title": "Constraints"
        }, 
        {
            "location": "/toml/#entrypoints-definition", 
            "text": "# Entrypoints definition\n#\n# Optional\n# Default:\n# [entryPoints]\n#   [entryPoints.http]\n#   address =  :80 \n#\n# To redirect an http entrypoint to an https entrypoint (with SNI support):\n# [entryPoints]\n#   [entryPoints.http]\n#   address =  :80 \n#     [entryPoints.http.redirect]\n#       entryPoint =  https \n#   [entryPoints.https]\n#   address =  :443 \n#     [entryPoints.https.tls]\n#       [[entryPoints.https.tls.certificates]]\n#       CertFile =  integration/fixtures/https/snitest.com.cert \n#       KeyFile =  integration/fixtures/https/snitest.com.key \n#       [[entryPoints.https.tls.certificates]]\n#       CertFile =  integration/fixtures/https/snitest.org.cert \n#       KeyFile =  integration/fixtures/https/snitest.org.key \n#\n# To redirect an entrypoint rewriting the URL:\n# [entryPoints]\n#   [entryPoints.http]\n#   address =  :80 \n#     [entryPoints.http.redirect]\n#       regex =  ^http://localhost/(.*) \n#       replacement =  http://mydomain/$1 \n#\n# Only accept clients that present a certificate signed by a specified\n# Certificate Authority (CA)\n# ClientCAFiles can be configured with multiple CA:s in the same file or\n# use multiple files containing one or several CA:s. The CA:s has to be in PEM format.\n# All clients will be required to present a valid cert.\n# The requirement will apply to all server certs in the entrypoint\n# In the example below both snitest.com and snitest.org will require client certs\n#\n# [entryPoints]\n#   [entryPoints.https]\n#   address =  :443 \n#   [entryPoints.https.tls]\n#   ClientCAFiles = [ tests/clientca1.crt ,  tests/clientca2.crt ]\n#     [[entryPoints.https.tls.certificates]]\n#     CertFile =  integration/fixtures/https/snitest.com.cert \n#     KeyFile =  integration/fixtures/https/snitest.com.key \n#     [[entryPoints.https.tls.certificates]]\n#     CertFile =  integration/fixtures/https/snitest.org.cert \n#     KeyFile =  integration/fixtures/https/snitest.org.key \n#\n# To enable basic auth on an entrypoint\n# with 2 user/pass: test:test and test2:test2\n# Passwords can be encoded in MD5, SHA1 and BCrypt: you can use htpasswd to generate those ones\n# [entryPoints]\n#   [entryPoints.http]\n#   address =  :80 \n#   [entryPoints.http.auth.basic]\n#   users = [ test:$apr1$H6uskkkW$IgXLP6ewTrSuBkTrqE8wj/ ,  test2:$apr1$d9hr9HBB$4HxwgUir3HP4EsggP/QNo0 ]\n#\n# To enable digest auth on an entrypoint\n# with 2 user/realm/pass: test:traefik:test and test2:traefik:test2\n# You can use htdigest to generate those ones\n# [entryPoints]\n#   [entryPoints.http]\n#   address =  :80 \n#   [entryPoints.http.auth.basic]\n#   users = [ test:traefik:a2688e031edb4be6a3797f3882655c05  ,  test2:traefik:518845800f9e2bfb1f1f740ec24f074e ]\n#\n# To specify an https entrypoint with a minimum TLS version, and specifying an array of cipher suites (from crypto/tls):\n# [entryPoints]\n#   [entryPoints.https]\n#   address =  :443 \n#     [entryPoints.https.tls]\n#     MinVersion =  VersionTLS12 \n#     CipherSuites = [ TLS_RSA_WITH_AES_256_GCM_SHA384 ]\n#       [[entryPoints.https.tls.certificates]]\n#       CertFile =  integration/fixtures/https/snitest.com.cert \n#       KeyFile =  integration/fixtures/https/snitest.com.key \n#       [[entryPoints.https.tls.certificates]]\n#       CertFile =  integration/fixtures/https/snitest.org.cert \n#       KeyFile =  integration/fixtures/https/snitest.org.key \n\n# To enable compression support using gzip format:\n# [entryPoints]\n#   [entryPoints.http]\n#   address =  :80 \n#   compress = true\n\n[entryPoints]\n  [entryPoints.http]\n  address =  :80", 
            "title": "Entrypoints definition"
        }, 
        {
            "location": "/toml/#retry-configuration", 
            "text": "# Enable retry sending request if network error\n#\n# Optional\n#\n[retry]\n\n# Number of attempts\n#\n# Optional\n# Default: (number servers in backend) -1\n#\n# attempts = 3", 
            "title": "Retry configuration"
        }, 
        {
            "location": "/toml/#acme-lets-encrypt-configuration", 
            "text": "# Sample entrypoint configuration when using ACME\n[entryPoints]\n  [entryPoints.https]\n  address =  :443 \n    [entryPoints.https.tls]\n\n# Enable ACME (Let's Encrypt): automatic SSL\n#\n# Optional\n#\n[acme]\n\n# Email address used for registration\n#\n# Required\n#\nemail =  test@traefik.io \n\n# File or key used for certificates storage.\n# WARNING, if you use Traefik in Docker, you have 2 options:\n#  - create a file on your host and mount it as a volume\n#      storageFile =  acme.json \n#      $ docker run -v  /my/host/acme.json:acme.json  traefik\n#  - mount the folder containing the file as a volume\n#      storageFile =  /etc/traefik/acme/acme.json \n#      $ docker run -v  /my/host/acme:/etc/traefik/acme  traefik\n#\n# Required\n#\nstorage =  acme.json  # or  traefik/acme/account  if using KV store\n\n# Entrypoint to proxy acme challenge/apply certificates to.\n# WARNING, must point to an entrypoint on port 443\n#\n# Required\n#\nentryPoint =  https \n\n# Use a DNS based acme challenge rather than external HTTPS access, e.g. for a firewalled server\n# Select the provider that matches the DNS domain that will host the challenge TXT record,\n# and provide environment variables with access keys to enable setting it:\n#  - cloudflare: CLOUDFLARE_EMAIL, CLOUDFLARE_API_KEY\n#  - digitalocean: DO_AUTH_TOKEN\n#  - dnsimple: DNSIMPLE_EMAIL, DNSIMPLE_API_KEY\n#  - dnsmadeeasy: DNSMADEEASY_API_KEY, DNSMADEEASY_API_SECRET\n#  - exoscale: EXOSCALE_API_KEY, EXOSCALE_API_SECRET\n#  - gandi: GANDI_API_KEY\n#  - linode: LINODE_API_KEY\n#  - manual: none, but run traefik interactively   turn on acmeLogging to see instructions   press Enter\n#  - namecheap: NAMECHEAP_API_USER, NAMECHEAP_API_KEY\n#  - rfc2136: RFC2136_TSIG_KEY, RFC2136_TSIG_SECRET, RFC2136_TSIG_ALGORITHM, RFC2136_NAMESERVER\n#  - route53: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, or configured user/instance IAM profile\n#  - dyn: DYN_CUSTOMER_NAME, DYN_USER_NAME, DYN_PASSWORD\n#  - vultr: VULTR_API_KEY\n#  - ovh: OVH_ENDPOINT, OVH_APPLICATION_KEY, OVH_APPLICATION_SECRET, OVH_CONSUMER_KEY\n#  - pdns: PDNS_API_KEY, PDNS_API_URL\n#\n# Optional\n#\n# dnsProvider =  digitalocean \n\n# By default, the dnsProvider will verify the TXT DNS challenge record before letting ACME verify\n# If delayDontCheckDNS is greater than zero, avoid this   instead just wait so many seconds.\n# Useful if internal networks block external DNS queries\n#\n# Optional\n#\n# delayDontCheckDNS = 0\n\n# If true, display debug log messages from the acme client library\n#\n# Optional\n#\n# acmeLogging = true\n\n# Enable on demand certificate. This will request a certificate from Let's Encrypt during the first TLS handshake for a hostname that does not yet have a certificate.\n# WARNING, TLS handshakes will be slow when requesting a hostname certificate for the first time, this can leads to DoS attacks.\n# WARNING, Take note that Let's Encrypt have rate limiting: https://letsencrypt.org/docs/rate-limits\n#\n# Optional\n#\n# onDemand = true\n\n# Enable certificate generation on frontends Host rules. This will request a certificate from Let's Encrypt for each frontend with a Host rule.\n# For example, a rule Host:test1.traefik.io,test2.traefik.io will request a certificate with main domain test1.traefik.io and SAN test2.traefik.io.\n#\n# Optional\n#\n# OnHostRule = true\n\n# CA server to use\n# Uncomment the line to run on the staging let's encrypt server\n# Leave comment to go to prod\n#\n# Optional\n#\n# caServer =  https://acme-staging.api.letsencrypt.org/directory \n\n# Domains list\n# You can provide SANs (alternative domains) to each main domain\n# All domains must have A/AAAA records pointing to Traefik\n# WARNING, Take note that Let's Encrypt have rate limiting: https://letsencrypt.org/docs/rate-limits\n# Each domain   SANs will lead to a certificate request.\n#\n# [[acme.domains]]\n#   main =  local1.com \n#   sans = [ test1.local1.com ,  test2.local1.com ]\n# [[acme.domains]]\n#   main =  local2.com \n#   sans = [ test1.local2.com ,  test2x.local2.com ]\n# [[acme.domains]]\n#   main =  local3.com \n# [[acme.domains]]\n#   main =  local4.com \n[[acme.domains]]\n   main =  local1.com \n   sans = [ test1.local1.com ,  test2.local1.com ]\n[[acme.domains]]\n   main =  local3.com \n[[acme.domains]]\n   main =  local4.com", 
            "title": "ACME (Let's Encrypt) configuration"
        }, 
        {
            "location": "/toml/#configuration-backends", 
            "text": "", 
            "title": "Configuration backends"
        }, 
        {
            "location": "/toml/#file-backend", 
            "text": "Like any other reverse proxy, Tr\u00e6f\u026ak can be configured with a file. You have two choices:   simply add your configuration at the end of the global configuration file  traefik.toml :   # traefik.toml\nlogLevel =  DEBUG \ndefaultEntryPoints = [ http ,  https ]\n[entryPoints]\n  [entryPoints.http]\n  address =  :80 \n    [entryPoints.http.redirect]\n      entryPoint =  https \n  [entryPoints.https]\n  address =  :443 \n    [entryPoints.https.tls]\n      [[entryPoints.https.tls.certificates]]\n      CertFile =  integration/fixtures/https/snitest.com.cert \n      KeyFile =  integration/fixtures/https/snitest.com.key \n      [[entryPoints.https.tls.certificates]]\n      CertFile =  integration/fixtures/https/snitest.org.cert \n      KeyFile =  integration/fixtures/https/snitest.org.key \n\n[file]\n\n# rules\n[backends]\n  [backends.backend1]\n    [backends.backend1.circuitbreaker]\n      expression =  NetworkErrorRatio()   0.5 \n    [backends.backend1.servers.server1]\n    url =  http://172.17.0.2:80 \n    weight = 10\n    [backends.backend1.servers.server2]\n    url =  http://172.17.0.3:80 \n    weight = 1\n  [backends.backend2]\n    [backends.backend1.maxconn]\n      amount = 10\n      extractorfunc =  request.host \n    [backends.backend2.LoadBalancer]\n      method =  drr \n    [backends.backend2.servers.server1]\n    url =  http://172.17.0.4:80 \n    weight = 1\n    [backends.backend2.servers.server2]\n    url =  http://172.17.0.5:80 \n    weight = 2\n\n[frontends]\n  [frontends.frontend1]\n  backend =  backend2 \n    [frontends.frontend1.routes.test_1]\n    rule =  Host:test.localhost \n  [frontends.frontend2]\n  backend =  backend1 \n  passHostHeader = true\n  priority = 10\n  entrypoints = [ https ] # overrides defaultEntryPoints\n    [frontends.frontend2.routes.test_1]\n    rule =  Host:{subdomain:[a-z]+}.localhost \n  [frontends.frontend3]\n  entrypoints = [ http ,  https ] # overrides defaultEntryPoints\n  backend =  backend2 \n    rule =  Path:/test    or put your rules in a separate file, for example  rules.toml :   # traefik.toml\nlogLevel =  DEBUG \n[entryPoints]\n  [entryPoints.http]\n  address =  :80 \n    [entryPoints.http.redirect]\n      entryPoint =  https \n  [entryPoints.https]\n  address =  :443 \n    [entryPoints.https.tls]\n      [[entryPoints.https.tls.certificates]]\n      CertFile =  integration/fixtures/https/snitest.com.cert \n      KeyFile =  integration/fixtures/https/snitest.com.key \n      [[entryPoints.https.tls.certificates]]\n      CertFile =  integration/fixtures/https/snitest.org.cert \n      KeyFile =  integration/fixtures/https/snitest.org.key \n\n[file]\nfilename =  rules.toml   # rules.toml\n[backends]\n  [backends.backend1]\n    [backends.backend1.circuitbreaker]\n      expression =  NetworkErrorRatio()   0.5 \n    [backends.backend1.servers.server1]\n    url =  http://172.17.0.2:80 \n    weight = 10\n    [backends.backend1.servers.server2]\n    url =  http://172.17.0.3:80 \n    weight = 1\n  [backends.backend2]\n    [backends.backend1.maxconn]\n      amount = 10\n      extractorfunc =  request.host \n    [backends.backend2.LoadBalancer]\n      method =  drr \n    [backends.backend2.servers.server1]\n    url =  http://172.17.0.4:80 \n    weight = 1\n    [backends.backend2.servers.server2]\n    url =  http://172.17.0.5:80 \n    weight = 2\n\n[frontends]\n  [frontends.frontend1]\n  backend =  backend2 \n    [frontends.frontend1.routes.test_1]\n    rule =  Host:test.localhost \n  [frontends.frontend2]\n  backend =  backend1 \n  passHostHeader = true\n  priority = 10\n  entrypoints = [ https ] # overrides defaultEntryPoints\n    [frontends.frontend2.routes.test_1]\n    rule =  Host:{subdomain:[a-z]+}.localhost \n  [frontends.frontend3]\n  entrypoints = [ http ,  https ] # overrides defaultEntryPoints\n  backend =  backend2 \n    rule =  Path:/test   If you want Tr\u00e6f\u026ak to watch file changes automatically, just add:  [file]\nwatch = true", 
            "title": "File backend"
        }, 
        {
            "location": "/toml/#api-backend", 
            "text": "Tr\u00e6fik can be configured using a RESTful api.\nTo enable it:  [web]\naddress =  :8080 \n\n# SSL certificate and key used\n#\n# Optional\n#\n# CertFile =  traefik.crt \n# KeyFile =  traefik.key \n#\n# Set REST API to read-only mode\n#\n# Optional\n# ReadOnly = false\n#\n# To enable more detailed statistics\n# [web.statistics]\n#   RecentErrors = 10\n#\n# To enable Traefik to export internal metrics to Prometheus\n# [web.metrics.prometheus]\n#   Buckets=[0.1,0.3,1.2,5]\n#\n# To enable basic auth on the webui\n# with 2 user/pass: test:test and test2:test2\n# Passwords can be encoded in MD5, SHA1 and BCrypt: you can use htpasswd to generate those ones\n#   [web.auth.basic]\n#     users = [ test:$apr1$H6uskkkW$IgXLP6ewTrSuBkTrqE8wj/ ,  test2:$apr1$d9hr9HBB$4HxwgUir3HP4EsggP/QNo0 ]\n# To enable digest auth on the webui\n# with 2 user/realm/pass: test:traefik:test and test2:traefik:test2\n# You can use htdigest to generate those ones\n#   [web.auth.digest]\n#     users = [ test:traefik:a2688e031edb4be6a3797f3882655c05  ,  test2:traefik:518845800f9e2bfb1f1f740ec24f074e ]   / : provides a simple HTML frontend of Tr\u00e6fik      /ping :  GET  simple endpoint to check for Tr\u00e6fik process liveness.   $ curl -sv  http://localhost:8080/ping \n*   Trying ::1...\n* Connected to localhost (::1) port 8080 (#0)  GET /ping HTTP/1.1  Host: localhost:8080  User-Agent: curl/7.43.0  Accept: */*   HTTP/1.1 200 OK  Date: Thu, 25 Aug 2016 01:35:36 GMT  Content-Length: 2  Content-Type: text/plain; charset=utf-8 \n* Connection #0 to host localhost left intact\nOK   /health :  GET  json metrics   $ curl -s  http://localhost:8080/health  | jq .\n{\n  // Tr\u00e6f\u026ak PID\n   pid : 2458,\n  // Tr\u00e6f\u026ak server uptime (formated time)\n   uptime :  39m6.885931127s ,\n  //  Tr\u00e6f\u026ak server uptime in seconds\n   uptime_sec : 2346.885931127,\n  // current server date\n   time :  2015-10-07 18:32:24.362238909 +0200 CEST ,\n  // current server date in seconds\n   unixtime : 1444235544,\n  // count HTTP response status code in realtime\n   status_code_count : {\n     502 : 1\n  },\n  // count HTTP response status code since Tr\u00e6f\u026ak started\n   total_status_code_count : {\n     200 : 7,\n     404 : 21,\n     502 : 13\n  },\n  // count HTTP response\n   count : 1,\n  // count HTTP response\n   total_count : 41,\n  // sum of all response time (formated time)\n   total_response_time :  35.456865605s ,\n  // sum of all response time in seconds\n   total_response_time_sec : 35.456865605,\n  // average response time (formated time)\n   average_response_time :  864.8016ms ,\n  // average response time in seconds\n   average_response_time_sec : 0.8648016000000001,\n\n  // request statistics [requires --web.statistics to be set]\n  // ten most recent requests with 4xx and 5xx status codes\n   recent_errors : [\n    {\n      // status code\n       status_code : 500,\n      // description of status code\n       status :  Internal Server Error ,\n      // request HTTP method\n       method :  GET ,\n      // request hostname\n       host :  localhost ,\n      // request path\n       path :  /path ,\n      // RFC 3339 formatted date/time\n       time :  2016-10-21T16:59:15.418495872-07:00 \n    }\n  ]\n}   /api :  GET  configuration for all providers   $ curl -s  http://localhost:8080/api  | jq .\n{\n   file : {\n     frontends : {\n       frontend2 : {\n         routes : {\n           test_2 : {\n             rule :  Path:/test \n          }\n        },\n         backend :  backend1 \n      },\n       frontend1 : {\n         routes : {\n           test_1 : {\n             rule :  Host:test.localhost \n          }\n        },\n         backend :  backend2 \n      }\n    },\n     backends : {\n       backend2 : {\n         loadBalancer : {\n           method :  drr \n        },\n         servers : {\n           server2 : {\n             weight : 2,\n             URL :  http://172.17.0.5:80 \n          },\n           server1 : {\n             weight : 1,\n             url :  http://172.17.0.4:80 \n          }\n        }\n      },\n       backend1 : {\n         loadBalancer : {\n           method :  wrr \n        },\n         circuitBreaker : {\n           expression :  NetworkErrorRatio()   0.5 \n        },\n         servers : {\n           server2 : {\n             weight : 1,\n             url :  http://172.17.0.3:80 \n          },\n           server1 : {\n             weight : 10,\n             url :  http://172.17.0.2:80 \n          }\n        }\n      }\n    }\n  }\n}   /api/providers :  GET  providers  /api/providers/{provider} :  GET  or  PUT  provider  /api/providers/{provider}/backends :  GET  backends  /api/providers/{provider}/backends/{backend} :  GET  a backend  /api/providers/{provider}/backends/{backend}/servers :  GET  servers in a backend  /api/providers/{provider}/backends/{backend}/servers/{server} :  GET  a server in a backend  /api/providers/{provider}/frontends :  GET  frontends  /api/providers/{provider}/frontends/{frontend} :  GET  a frontend  /api/providers/{provider}/frontends/{frontend}/routes :  GET  routes in a frontend   /api/providers/{provider}/frontends/{frontend}/routes/{route} :  GET  a route in a frontend    /metrics : You can enable Traefik to export internal metrics to different monitoring systems (Only Prometheus is supported at the moment).    $ traefik --web.metrics.prometheus --web.metrics.prometheus.buckets= 0.1,0.3,1.2,5", 
            "title": "API backend"
        }, 
        {
            "location": "/toml/#docker-backend", 
            "text": "Tr\u00e6f\u026ak can be configured to use Docker as a backend configuration:  ################################################################\n# Docker configuration backend\n################################################################\n\n# Enable Docker configuration backend\n#\n# Optional\n#\n[docker]\n\n# Docker server endpoint. Can be a tcp or a unix socket endpoint.\n#\n# Required\n#\nendpoint =  unix:///var/run/docker.sock \n\n# Default domain used.\n# Can be overridden by setting the  traefik.domain  label on a container.\n#\n# Required\n#\ndomain =  docker.localhost \n\n# Enable watch docker changes\n#\n# Optional\n#\nwatch = true\n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename =  docker.tmpl \n\n# Expose containers by default in traefik\n# If set to false, containers that don't have `traefik.enable=true` will be ignored\n#\n# Optional\n# Default: true\n#\nexposedbydefault = true\n\n# Use the IP address from the binded port instead of the inner network one. For specific use-case :)\n\n#\n# Optional\n# Default: false\n#\nusebindportip = true\n# Use Swarm Mode services as data provider\n#\n# Optional\n# Default: false\n#\nswarmmode = false\n\n\n# Enable docker TLS connection\n#\n#  [docker.tls]\n#  ca =  /etc/ssl/ca.crt \n#  cert =  /etc/ssl/docker.crt \n#  key =  /etc/ssl/docker.key \n#  insecureskipverify = true  Labels can be used on containers to override default behaviour:   traefik.backend=foo : assign the container to  foo  backend  traefik.backend.maxconn.amount=10 : set a maximum number of connections to the backend. Must be used in conjunction with the below label to take effect.  traefik.backend.maxconn.extractorfunc=client.ip : set the function to be used against the request to determine what to limit maximum connections to the backend by. Must be used in conjunction with the above label to take effect.  traefik.backend.loadbalancer.method=drr : override the default  wrr  load balancer algorithm  traefik.backend.loadbalancer.sticky=true : enable backend sticky sessions  traefik.backend.loadbalancer.swarm=true : use Swarm's inbuilt load balancer (only relevant under Swarm Mode).  traefik.backend.circuitbreaker.expression=NetworkErrorRatio()   0.5 : create a  circuit breaker  to be used against the backend  traefik.port=80 : register this port. Useful when the container exposes multiples ports.  traefik.protocol=https : override the default  http  protocol  traefik.weight=10 : assign this weight to the container  traefik.enable=false : disable this container in Tr\u00e6f\u026ak  traefik.frontend.rule=Host:test.traefik.io : override the default frontend rule (Default:  Host:{containerName}.{domain} ).  traefik.frontend.passHostHeader=true : forward client  Host  header to the backend.  traefik.frontend.priority=10 : override default frontend priority  traefik.frontend.entryPoints=http,https : assign this frontend to entry points  http  and  https . Overrides  defaultEntryPoints .  traefik.docker.network : Set the docker network to use for connections to this container. If a container is linked to several networks, be sure to set the proper network name (you can check with docker inspect  ) otherwise it will randomly pick one (depending on how docker is returning them). For instance when deploying docker  stack  from compose files, the compose defined networks will be prefixed with the  stack  name.   NB: when running inside a container, Tr\u00e6f\u026ak will need network access through  docker network connect  network   traefik-container", 
            "title": "Docker backend"
        }, 
        {
            "location": "/toml/#marathon-backend", 
            "text": "Tr\u00e6f\u026ak can be configured to use Marathon as a backend configuration:  ################################################################\n# Mesos/Marathon configuration backend\n################################################################\n\n# Enable Marathon configuration backend\n#\n# Optional\n#\n[marathon]\n\n# Marathon server endpoint.\n# You can also specify multiple endpoint for Marathon:\n# endpoint :=  http://10.241.1.71:8080,10.241.1.72:8080,10.241.1.73:8080 \n#\n# Required\n#\nendpoint =  http://127.0.0.1:8080 \n\n# Enable watch Marathon changes\n#\n# Optional\n#\nwatch = true\n\n# Default domain used.\n#\n# Required\n#\ndomain =  marathon.localhost \n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename =  marathon.tmpl \n\n# Expose Marathon apps by default in traefik\n#\n# Optional\n# Default: true\n#\n# exposedByDefault = true\n\n# Convert Marathon groups to subdomains\n# Default behavior: /foo/bar/myapp =  foo-bar-myapp.{defaultDomain}\n# with groupsAsSubDomains enabled: /foo/bar/myapp =  myapp.bar.foo.{defaultDomain}\n#\n# Optional\n# Default: false\n#\n# groupsAsSubDomains = true\n\n#\u00a0Enable compatibility with marathon-lb labels\n#\n# Optional\n# Default: false\n#\n# marathonLBCompatibility = true\n\n# Enable Marathon basic authentication\n#\n# Optional\n#\n#  [marathon.basic]\n#  httpBasicAuthUser =  foo \n#  httpBasicPassword =  bar \n\n# TLS client configuration. https://golang.org/pkg/crypto/tls/#Config\n#\n# Optional\n#\n# [marathon.TLS]\n# CA =  /etc/ssl/ca.crt \n# Cert =  /etc/ssl/marathon.cert \n# Key =  /etc/ssl/marathon.key \n# InsecureSkipVerify = true\n\n# DCOSToken for DCOS environment, This will override the Authorization header\n#\n# Optional\n#\n# dcosToken =  xxxxxx \n\n# Override DialerTimeout\n# Amount of time in seconds to allow the Marathon provider to wait to open a TCP\n# connection to a Marathon master\n#\n# Optional\n# Default: 60\n# dialerTimeout = 5\n\n# Set the TCP Keep Alive interval (in seconds) for the Marathon HTTP Client\n#\n# Optional\n# Default: 10\n#\n# keepAlive = 10  Labels can be used on containers to override default behaviour:   traefik.backend=foo : assign the application to  foo  backend  traefik.backend.maxconn.amount=10 : set a maximum number of connections to the backend. Must be used in conjunction with the below label to take effect.  traefik.backend.maxconn.extractorfunc=client.ip : set the function to be used against the request to determine what to limit maximum connections to the backend by. Must be used in conjunction with the above label to take effect.  traefik.backend.loadbalancer.method=drr : override the default  wrr  load balancer algorithm  traefik.backend.loadbalancer.sticky=true : enable backend sticky sessions  traefik.backend.circuitbreaker.expression=NetworkErrorRatio()   0.5 : create a  circuit breaker  to be used against the backend  traefik.portIndex=1 : register port by index in the application's ports array. Useful when the application exposes multiple ports.  traefik.port=80 : register the explicit application port value. Cannot be used alongside  traefik.portIndex .  traefik.protocol=https : override the default  http  protocol  traefik.weight=10 : assign this weight to the application  traefik.enable=false : disable this application in Tr\u00e6f\u026ak  traefik.frontend.rule=Host:test.traefik.io : override the default frontend rule (Default:  Host:{containerName}.{domain} ).  traefik.frontend.passHostHeader=true : forward client  Host  header to the backend.  traefik.frontend.priority=10 : override default frontend priority  traefik.frontend.entryPoints=http,https : assign this frontend to entry points  http  and  https . Overrides  defaultEntryPoints .", 
            "title": "Marathon backend"
        }, 
        {
            "location": "/toml/#mesos-generic-backend", 
            "text": "Tr\u00e6f\u026ak can be configured to use Mesos as a backend configuration:  ################################################################\n# Mesos configuration backend\n################################################################\n\n# Enable Mesos configuration backend\n#\n# Optional\n#\n[mesos]\n\n# Mesos server endpoint.\n# You can also specify multiple endpoint for Mesos:\n# endpoint =  192.168.35.40:5050,192.168.35.41:5050,192.168.35.42:5050 \n# endpoint =  zk://192.168.35.20:2181,192.168.35.21:2181,192.168.35.22:2181/mesos \n#\n# Required\n#\nendpoint =  http://127.0.0.1:8080 \n\n# Enable watch Mesos changes\n#\n# Optional\n#\nwatch = true\n\n# Default domain used.\n# Can be overridden by setting the  traefik.domain  label on an application.\n#\n# Required\n#\ndomain =  mesos.localhost \n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename =  mesos.tmpl \n\n# Expose Mesos apps by default in traefik\n#\n# Optional\n# Default: false\n#\n# ExposedByDefault = true\n\n# TLS client configuration. https://golang.org/pkg/crypto/tls/#Config\n#\n# Optional\n#\n# [mesos.TLS]\n# InsecureSkipVerify = true\n\n# Zookeeper timeout (in seconds)\n#\n# Optional\n# Default: 30\n#\n# ZkDetectionTimeout = 30\n\n# Polling interval (in seconds)\n#\n# Optional\n# Default: 30\n#\n# RefreshSeconds = 30\n\n# IP sources (e.g. host, docker, mesos, rkt)\n#\n# Optional\n#\n# IPSources =  host \n\n# HTTP Timeout (in seconds)\n#\n# Optional\n# Default: 30\n#\n# StateTimeoutSecond =  30", 
            "title": "Mesos generic backend"
        }, 
        {
            "location": "/toml/#kubernetes-ingress-backend", 
            "text": "Tr\u00e6f\u026ak can be configured to use Kubernetes Ingress as a backend configuration:  ################################################################\n# Kubernetes Ingress configuration backend\n################################################################\n# Enable Kubernetes Ingress configuration backend\n#\n# Optional\n#\n[kubernetes]\n\n# Kubernetes server endpoint\n#\n# When deployed as a replication controller in Kubernetes,\n# Traefik will use env variable KUBERNETES_SERVICE_HOST\n# and KUBERNETES_SERVICE_PORT_HTTPS as endpoint\n# Secure token will be found in /var/run/secrets/kubernetes.io/serviceaccount/token\n# and SSL CA cert in /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n#\n# Optional\n#\n# endpoint =  http://localhost:8080 \n# namespaces = [ default , production ]\n#\n# See: http://kubernetes.io/docs/user-guide/labels/#list-and-watch-filtering\n# labelselector =  A and not B \n#  Annotations can be used on containers to override default behaviour for the whole Ingress resource:   traefik.frontend.rule.type: PathPrefixStrip : override the default frontend rule type (Default:  PathPrefix ).   Annotations can be used on the Kubernetes service to override default behaviour:   traefik.backend.loadbalancer.method=drr : override the default  wrr  load balancer algorithm  traefik.backend.loadbalancer.sticky=true : enable backend sticky sessions   You can find here an example  ingress  and  replication controller .  Additionally, an annotation can be used on Kubernetes services to set the  circuit breaker expression  for a backend.   traefik.backend.circuitbreaker:  expression : set the circuit breaker expression for the backend (Default: nil).", 
            "title": "Kubernetes Ingress backend"
        }, 
        {
            "location": "/toml/#consul-backend", 
            "text": "Tr\u00e6f\u026ak can be configured to use Consul as a backend configuration:  ################################################################\n# Consul KV configuration backend\n################################################################\n\n# Enable Consul KV configuration backend\n#\n# Optional\n#\n[consul]\n\n# Consul server endpoint\n#\n# Required\n#\nendpoint =  127.0.0.1:8500 \n\n# Enable watch Consul changes\n#\n# Optional\n#\nwatch = true\n\n# Prefix used for KV store.\n#\n# Optional\n#\nprefix =  traefik \n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename =  consul.tmpl \n\n# Enable consul TLS connection\n#\n# Optional\n#\n# [consul.tls]\n# ca =  /etc/ssl/ca.crt \n# cert =  /etc/ssl/consul.crt \n# key =  /etc/ssl/consul.key \n# insecureskipverify = true  Please refer to the  Key Value storage structure  section to get documentation on traefik KV structure.", 
            "title": "Consul backend"
        }, 
        {
            "location": "/toml/#consul-catalog-backend", 
            "text": "Tr\u00e6f\u026ak can be configured to use service discovery catalog of Consul as a backend configuration:  ################################################################\n# Consul Catalog configuration backend\n################################################################\n\n# Enable Consul Catalog configuration backend\n#\n# Optional\n#\n[consulCatalog]\n\n# Consul server endpoint\n#\n# Required\n#\nendpoint =  127.0.0.1:8500 \n\n# Default domain used.\n#\n# Optional\n#\ndomain =  consul.localhost \n\n# Prefix for Consul catalog tags\n#\n# Optional\n#\nprefix =  traefik   This backend will create routes matching on hostname based on the service name\nused in consul.  Additional settings can be defined using Consul Catalog tags:   traefik.enable=false : disable this container in Tr\u00e6f\u026ak  traefik.protocol=https : override the default  http  protocol  traefik.backend.weight=10 : assign this weight to the container  traefik.backend.circuitbreaker=NetworkErrorRatio()   0.5  traefik.backend.loadbalancer=drr : override the default load balancing mode  traefik.backend.maxconn.amount=10 : set a maximum number of connections to the backend. Must be used in conjunction with the below label to take effect.  traefik.backend.maxconn.extractorfunc=client.ip : set the function to be used against the request to determine what to limit maximum connections to the backend by. Must be used in conjunction with the above label to take effect.  traefik.frontend.rule=Host:test.traefik.io : override the default frontend rule (Default:  Host:{containerName}.{domain} ).  traefik.frontend.passHostHeader=true : forward client  Host  header to the backend.  traefik.frontend.priority=10 : override default frontend priority  traefik.frontend.entryPoints=http,https : assign this frontend to entry points  http  and  https . Overrides  defaultEntryPoints .", 
            "title": "Consul catalog backend"
        }, 
        {
            "location": "/toml/#etcd-backend", 
            "text": "Tr\u00e6f\u026ak can be configured to use Etcd as a backend configuration:  ################################################################\n# Etcd configuration backend\n################################################################\n\n# Enable Etcd configuration backend\n#\n# Optional\n#\n[etcd]\n\n# Etcd server endpoint\n#\n# Required\n#\nendpoint =  127.0.0.1:2379 \n\n# Enable watch Etcd changes\n#\n# Optional\n#\nwatch = true\n\n# Prefix used for KV store.\n#\n# Optional\n#\nprefix =  /traefik \n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename =  etcd.tmpl \n\n# Enable etcd TLS connection\n#\n# Optional\n#\n# [etcd.tls]\n# ca =  /etc/ssl/ca.crt \n# cert =  /etc/ssl/etcd.crt \n# key =  /etc/ssl/etcd.key \n# insecureskipverify = true  Please refer to the  Key Value storage structure  section to get documentation on traefik KV structure.", 
            "title": "Etcd backend"
        }, 
        {
            "location": "/toml/#zookeeper-backend", 
            "text": "Tr\u00e6f\u026ak can be configured to use Zookeeper as a backend configuration:  ################################################################\n# Zookeeper configuration backend\n################################################################\n\n# Enable Zookeeperconfiguration backend\n#\n# Optional\n#\n[zookeeper]\n\n# Zookeeper server endpoint\n#\n# Required\n#\nendpoint =  127.0.0.1:2181 \n\n# Enable watch Zookeeper changes\n#\n# Optional\n#\nwatch = true\n\n# Prefix used for KV store.\n#\n# Optional\n#\nprefix =  traefik \n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename =  zookeeper.tmpl   Please refer to the  Key Value storage structure  section to get documentation on traefik KV structure.", 
            "title": "Zookeeper backend"
        }, 
        {
            "location": "/toml/#boltdb-backend", 
            "text": "Tr\u00e6f\u026ak can be configured to use BoltDB as a backend configuration:  ################################################################\n# BoltDB configuration backend\n################################################################\n\n# Enable BoltDB configuration backend\n#\n# Optional\n#\n[boltdb]\n\n# BoltDB file\n#\n# Required\n#\nendpoint =  /my.db \n\n# Enable watch BoltDB changes\n#\n# Optional\n#\nwatch = true\n\n# Prefix used for KV store.\n#\n# Optional\n#\nprefix =  /traefik \n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename =  boltdb.tmpl", 
            "title": "BoltDB backend"
        }, 
        {
            "location": "/toml/#eureka-backend", 
            "text": "Tr\u00e6f\u026ak can be configured to use Eureka as a backend configuration:  ################################################################\n# Eureka configuration backend\n################################################################\n\n# Enable Eureka configuration backend\n#\n# Optional\n#\n[eureka]\n\n# Eureka server endpoint.\n# endpoint :=  http://my.eureka.server/eureka \n#\n# Required\n#\nendpoint =  http://my.eureka.server/eureka \n\n# Override default configuration time between refresh\n#\n# Optional\n# default 30s\ndelay =  1m \n\n# Override default configuration template. For advanced users :)\n#\n# Optional\n#\n# filename =  eureka.tmpl   Please refer to the  Key Value storage structure  section to get documentation on traefik KV structure.", 
            "title": "Eureka backend"
        }, 
        {
            "location": "/toml/#ecs-backend", 
            "text": "Tr\u00e6f\u026ak can be configured to use Amazon ECS as a backend configuration:  ################################################################\n# ECS configuration backend\n################################################################\n\n# Enable ECS configuration backend\n#\n# Optional\n#\n[ecs]\n\n# ECS Cluster Name\n#\n# Optional\n# Default:  default \n#\nCluster =  default \n\n# Enable watch ECS changes\n#\n# Optional\n# Default: true\n#\nWatch = true\n\n# Polling interval (in seconds)\n#\n# Optional\n# Default: 15\n#\nRefreshSeconds = 15\n\n# Expose ECS services by default in traefik\n#\n# Optional\n# Default: true\n#\nExposedByDefault = false\n\n# Region to use when connecting to AWS\n#\n# Optional\n#\n# Region =  us-east-1 \n\n# AccessKeyID to use when connecting to AWS\n#\n# Optional\n#\n# AccessKeyID =  abc \n\n# SecretAccessKey to use when connecting to AWS\n#\n# Optional\n#\n# SecretAccessKey =  123   Labels can be used on task containers to override default behaviour:   traefik.protocol=https : override the default  http  protocol  traefik.weight=10 : assign this weight to the container  traefik.enable=false : disable this container in Tr\u00e6f\u026ak  traefik.frontend.rule=Host:test.traefik.io : override the default frontend rule (Default:  Host:{containerName}.{domain} ).  traefik.frontend.passHostHeader=true : forward client  Host  header to the backend.  traefik.frontend.priority=10 : override default frontend priority  traefik.frontend.entryPoints=http,https : assign this frontend to entry points  http  and  https . Overrides  defaultEntryPoints .   If  AccessKeyID / SecretAccessKey  is not given credentials will be resolved in the following order:   From environment variables;  AWS_ACCESS_KEY_ID ,  AWS_SECRET_ACCESS_KEY , and  AWS_SESSION_TOKEN .  Shared credentials, determined by  AWS_PROFILE  and  AWS_SHARED_CREDENTIALS_FILE , defaults to  default  and  ~/.aws/credentials .  EC2 instance role or ECS task role   Tr\u00e6f\u026ak needs the following policy to read ECS information:  {\n     Version :  2012-10-17 ,\n     Statement : [\n        {\n             Sid :  Traefik ECS read access ,\n             Effect :  Allow ,\n             Action : [\n                 ecs:ListTasks ,\n                 ecs:DescribeTasks ,\n                 ecs:DescribeContainerInstances ,\n                 ecs:DescribeTaskDefinition ,\n                 ec2:DescribeInstances \n            ],\n             Resource : [\n                 * \n            ]\n        }\n    ]\n}", 
            "title": "ECS backend"
        }, 
        {
            "location": "/toml/#rancher-backend", 
            "text": "Tr\u00e6f\u026ak can be configured to use Rancher as a backend configuration:  ################################################################\n# Rancher configuration backend\n################################################################\n\n# Enable Rancher configuration backend\n#\n# Optional\n#\n[rancher]\n\n# Default domain used.\n# Can be overridden by setting the  traefik.domain  label on an service.\n#\n# Required\n#\ndomain =  rancher.localhost \n\n# Enable watch Rancher changes\n#\n# Optional\n# Default: true\n#\nWatch = true\n\n# Expose Rancher services by default in traefik\n#\n# Optional\n# Default: true\n#\nExposedByDefault = false\n\n# Endpoint to use when connecting to Rancher\n#\n# Optional\n# Endpoint =  http://rancherserver.example.com \n\n# AccessKey to use when connecting to Rancher\n#\n# Optional\n# AccessKey =  XXXXXXXXX \n\n# SecretKey to use when connecting to Rancher\n#\n# Optional\n# SecretKey =  XXXXXXXXXXX   If you're deploying traefik as a service within rancher, you can alternatively set these labels on the service to let it only fetch data of its current environment. The settings  endpoint ,  accesskey  and  secretkey  can be omitted then.   io.rancher.container.create_agent=true  io.rancher.container.agent.role=environment   Labels can be used on task containers to override default behaviour:   traefik.protocol=https : override the default  http  protocol  traefik.weight=10 : assign this weight to the container  traefik.enable=false : disable this container in Tr\u00e6f\u026ak  traefik.frontend.rule=Host:test.traefik.io : override the default frontend rule (Default:  Host:{containerName}.{domain} ).  traefik.frontend.passHostHeader=true : forward client  Host  header to the backend.  traefik.frontend.priority=10 : override default frontend priority  traefik.frontend.entryPoints=http,https : assign this frontend to entry points  http  and  https . Overrides  defaultEntryPoints .", 
            "title": "Rancher backend"
        }, 
        {
            "location": "/user-guide/examples/", 
            "text": "Examples\n\n\nYou will find here some configuration examples of Tr\u00e6f\u026ak.\n\n\nHTTP only\n\n\ndefaultEntryPoints = [\nhttp\n]\n[entryPoints]\n  [entryPoints.http]\n  address = \n:80\n\n\n\n\n\nHTTP + HTTPS (with SNI)\n\n\ndefaultEntryPoints = [\nhttp\n, \nhttps\n]\n[entryPoints]\n  [entryPoints.http]\n  address = \n:80\n\n  [entryPoints.https]\n  address = \n:443\n\n    [entryPoints.https.tls]\n      [[entryPoints.https.tls.certificates]]\n      CertFile = \nintegration/fixtures/https/snitest.com.cert\n\n      KeyFile = \nintegration/fixtures/https/snitest.com.key\n\n      [[entryPoints.https.tls.certificates]]\n      CertFile = \nintegration/fixtures/https/snitest.org.cert\n\n      KeyFile = \nintegration/fixtures/https/snitest.org.key\n\n\n\n\n\nNote that we can either give path to certificate file or directly the file content itself (\nlike in this TOML example\n).\n\n\nHTTP redirect on HTTPS\n\n\ndefaultEntryPoints = [\nhttp\n, \nhttps\n]\n[entryPoints]\n  [entryPoints.http]\n  address = \n:80\n\n    [entryPoints.http.redirect]\n    entryPoint = \nhttps\n\n  [entryPoints.https]\n  address = \n:443\n\n    [entryPoints.https.tls]\n      [[entryPoints.https.tls.certificates]]\n      certFile = \ntests/traefik.crt\n\n      keyFile = \ntests/traefik.key\n\n\n\n\n\nLet's Encrypt support\n\n\n[entryPoints]\n  [entryPoints.https]\n  address = \n:443\n\n    [entryPoints.https.tls]\n      # certs used as default certs\n      [[entryPoints.https.tls.certificates]]\n      certFile = \ntests/traefik.crt\n\n      keyFile = \ntests/traefik.key\n\n[acme]\nemail = \ntest@traefik.io\n\nstorageFile = \nacme.json\n\nonDemand = true\ncaServer = \nhttp://172.18.0.1:4000/directory\n\nentryPoint = \nhttps\n\n\n[[acme.domains]]\n  main = \nlocal1.com\n\n  sans = [\ntest1.local1.com\n, \ntest2.local1.com\n]\n[[acme.domains]]\n  main = \nlocal2.com\n\n  sans = [\ntest1.local2.com\n, \ntest2x.local2.com\n]\n[[acme.domains]]\n  main = \nlocal3.com\n\n[[acme.domains]]\n  main = \nlocal4.com\n\n\n\n\n\nOverride entrypoints in frontends\n\n\n[frontends]\n  [frontends.frontend1]\n  backend = \nbackend2\n\n    [frontends.frontend1.routes.test_1]\n    rule = \nHost:test.localhost\n\n  [frontends.frontend2]\n  backend = \nbackend1\n\n  passHostHeader = true\n  entrypoints = [\nhttps\n] # overrides defaultEntryPoints\n    [frontends.frontend2.routes.test_1]\n    rule = \nHost:{subdomain:[a-z]+}.localhost\n\n  [frontends.frontend3]\n  entrypoints = [\nhttp\n, \nhttps\n] # overrides defaultEntryPoints\n  backend = \nbackend2\n\n    rule = \nPath:/test\n\n\n\n\n\nEnable Basic authentication in an entrypoint\n\n\nWith two user/pass:\n\n\n\n\ntest\n:\ntest\n\n\ntest2\n:\ntest2\n\n\n\n\nPasswords are encoded in MD5: you can use htpasswd to generate those ones.\n\n\ndefaultEntryPoints = [\nhttp\n]\n[entryPoints]\n  [entryPoints.http]\n  address = \n:80\n\n  [entryPoints.http.auth.basic]\n  users = [\ntest:$apr1$H6uskkkW$IgXLP6ewTrSuBkTrqE8wj/\n, \ntest2:$apr1$d9hr9HBB$4HxwgUir3HP4EsggP/QNo0\n]\n\n\n\n\nPass Authenticated user to application via headers\n\n\nProviding an authentication method as described above, it is possible to pass the user to the application\nvia a configurable header value\n\n\ndefaultEntryPoints = [\nhttp\n]\n[entryPoints]\n  [entryPoints.http]\n  address = \n:80\n\n  [entryPoints.http.auth]\n    headerField = \nX-WebAuth-User\n\n    [entryPoints.http.auth.basic]\n    users = [\ntest:$apr1$H6uskkkW$IgXLP6ewTrSuBkTrqE8wj/\n, \ntest2:$apr1$d9hr9HBB$4HxwgUir3HP4EsggP/QNo0\n]", 
            "title": "Configuration examples"
        }, 
        {
            "location": "/user-guide/examples/#examples", 
            "text": "You will find here some configuration examples of Tr\u00e6f\u026ak.", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/examples/#http-only", 
            "text": "defaultEntryPoints = [ http ]\n[entryPoints]\n  [entryPoints.http]\n  address =  :80", 
            "title": "HTTP only"
        }, 
        {
            "location": "/user-guide/examples/#http-https-with-sni", 
            "text": "defaultEntryPoints = [ http ,  https ]\n[entryPoints]\n  [entryPoints.http]\n  address =  :80 \n  [entryPoints.https]\n  address =  :443 \n    [entryPoints.https.tls]\n      [[entryPoints.https.tls.certificates]]\n      CertFile =  integration/fixtures/https/snitest.com.cert \n      KeyFile =  integration/fixtures/https/snitest.com.key \n      [[entryPoints.https.tls.certificates]]\n      CertFile =  integration/fixtures/https/snitest.org.cert \n      KeyFile =  integration/fixtures/https/snitest.org.key   Note that we can either give path to certificate file or directly the file content itself ( like in this TOML example ).", 
            "title": "HTTP + HTTPS (with SNI)"
        }, 
        {
            "location": "/user-guide/examples/#http-redirect-on-https", 
            "text": "defaultEntryPoints = [ http ,  https ]\n[entryPoints]\n  [entryPoints.http]\n  address =  :80 \n    [entryPoints.http.redirect]\n    entryPoint =  https \n  [entryPoints.https]\n  address =  :443 \n    [entryPoints.https.tls]\n      [[entryPoints.https.tls.certificates]]\n      certFile =  tests/traefik.crt \n      keyFile =  tests/traefik.key", 
            "title": "HTTP redirect on HTTPS"
        }, 
        {
            "location": "/user-guide/examples/#lets-encrypt-support", 
            "text": "[entryPoints]\n  [entryPoints.https]\n  address =  :443 \n    [entryPoints.https.tls]\n      # certs used as default certs\n      [[entryPoints.https.tls.certificates]]\n      certFile =  tests/traefik.crt \n      keyFile =  tests/traefik.key \n[acme]\nemail =  test@traefik.io \nstorageFile =  acme.json \nonDemand = true\ncaServer =  http://172.18.0.1:4000/directory \nentryPoint =  https \n\n[[acme.domains]]\n  main =  local1.com \n  sans = [ test1.local1.com ,  test2.local1.com ]\n[[acme.domains]]\n  main =  local2.com \n  sans = [ test1.local2.com ,  test2x.local2.com ]\n[[acme.domains]]\n  main =  local3.com \n[[acme.domains]]\n  main =  local4.com", 
            "title": "Let's Encrypt support"
        }, 
        {
            "location": "/user-guide/examples/#override-entrypoints-in-frontends", 
            "text": "[frontends]\n  [frontends.frontend1]\n  backend =  backend2 \n    [frontends.frontend1.routes.test_1]\n    rule =  Host:test.localhost \n  [frontends.frontend2]\n  backend =  backend1 \n  passHostHeader = true\n  entrypoints = [ https ] # overrides defaultEntryPoints\n    [frontends.frontend2.routes.test_1]\n    rule =  Host:{subdomain:[a-z]+}.localhost \n  [frontends.frontend3]\n  entrypoints = [ http ,  https ] # overrides defaultEntryPoints\n  backend =  backend2 \n    rule =  Path:/test", 
            "title": "Override entrypoints in frontends"
        }, 
        {
            "location": "/user-guide/examples/#enable-basic-authentication-in-an-entrypoint", 
            "text": "With two user/pass:   test : test  test2 : test2   Passwords are encoded in MD5: you can use htpasswd to generate those ones.  defaultEntryPoints = [ http ]\n[entryPoints]\n  [entryPoints.http]\n  address =  :80 \n  [entryPoints.http.auth.basic]\n  users = [ test:$apr1$H6uskkkW$IgXLP6ewTrSuBkTrqE8wj/ ,  test2:$apr1$d9hr9HBB$4HxwgUir3HP4EsggP/QNo0 ]", 
            "title": "Enable Basic authentication in an entrypoint"
        }, 
        {
            "location": "/user-guide/examples/#pass-authenticated-user-to-application-via-headers", 
            "text": "Providing an authentication method as described above, it is possible to pass the user to the application\nvia a configurable header value  defaultEntryPoints = [ http ]\n[entryPoints]\n  [entryPoints.http]\n  address =  :80 \n  [entryPoints.http.auth]\n    headerField =  X-WebAuth-User \n    [entryPoints.http.auth.basic]\n    users = [ test:$apr1$H6uskkkW$IgXLP6ewTrSuBkTrqE8wj/ ,  test2:$apr1$d9hr9HBB$4HxwgUir3HP4EsggP/QNo0 ]", 
            "title": "Pass Authenticated user to application via headers"
        }, 
        {
            "location": "/user-guide/swarm/", 
            "text": "Swarm cluster\n\n\nThis section explains how to create a multi-host \nswarm\n cluster using \ndocker-machine\n and how to deploy Tr\u00e6f\u026ak on it.\nThe cluster consists of:\n\n\n\n\n2 servers\n\n\n1 swarm master\n\n\n2 swarm nodes\n\n\n1 \noverlay\n network (multi-host networking)\n\n\n\n\nPrerequisites\n\n\n\n\nYou need to install \ndocker-machine\n\n\nYou need the latest \nVirtualBox\n\n\n\n\nCluster provisioning\n\n\nWe first follow \nthis guide\n to create the cluster.\n\n\nCreate machine \nmh-keystore\n\n\nThis machine is the service registry of our cluster.\n\n\ndocker-machine create -d virtualbox mh-keystore\n\n\n\n\nThen we install the service registry \nConsul\n on this machine:\n\n\neval \n$(docker-machine env mh-keystore)\n\ndocker run -d \\\n    -p \n8500:8500\n \\\n    -h \nconsul\n \\\n    progrium/consul -server -bootstrap\n\n\n\n\nCreate machine \nmhs-demo0\n\n\nThis machine is a swarm master and a swarm agent on it.\n\n\ndocker-machine create -d virtualbox \\\n    --swarm --swarm-master \\\n    --swarm-discovery=\nconsul://$(docker-machine ip mh-keystore):8500\n \\\n    --engine-opt=\ncluster-store=consul://$(docker-machine ip mh-keystore):8500\n \\\n    --engine-opt=\ncluster-advertise=eth1:2376\n \\\n    mhs-demo0\n\n\n\n\nCreate machine \nmhs-demo1\n\n\nThis machine have a swarm agent on it.\n\n\ndocker-machine create -d virtualbox \\\n    --swarm \\\n    --swarm-discovery=\nconsul://$(docker-machine ip mh-keystore):8500\n \\\n    --engine-opt=\ncluster-store=consul://$(docker-machine ip mh-keystore):8500\n \\\n    --engine-opt=\ncluster-advertise=eth1:2376\n \\\n    mhs-demo1\n\n\n\n\nCreate the overlay Network\n\n\nCreate the overlay network on the swarm master:\n\n\neval $(docker-machine env --swarm mhs-demo0)\ndocker network create --driver overlay --subnet=10.0.9.0/24 my-net\n\n\n\n\nDeploy Tr\u00e6f\u026ak\n\n\nDeploy Tr\u00e6f\u026ak:\n\n\ndocker $(docker-machine config mhs-demo0) run \\\n    -d \\\n    -p 80:80 -p 8080:8080 \\\n    --net=my-net \\\n    -v /var/lib/boot2docker/:/ssl \\\n    traefik \\\n    -l DEBUG \\\n    -c /dev/null \\\n    --docker \\\n    --docker.domain=traefik \\\n    --docker.endpoint=tcp://$(docker-machine ip mhs-demo0):3376 \\\n    --docker.tls \\\n    --docker.tls.ca=/ssl/ca.pem \\\n    --docker.tls.cert=/ssl/server.pem \\\n    --docker.tls.key=/ssl/server-key.pem \\\n    --docker.tls.insecureSkipVerify \\\n    --docker.watch \\\n    --web\n\n\n\n\nLet's explain this command:\n\n\n\n\n-p 80:80 -p 8080:8080\n: we bind ports 80 and 8080\n\n\n--net=my-net\n: run the container on the network my-net\n\n\n-v /var/lib/boot2docker/:/ssl\n: mount the ssl keys generated by docker-machine\n\n\n-c /dev/null\n: empty config file\n\n\n--docker\n: enable docker backend\n\n\n--docker.endpoint=tcp://172.18.0.1:3376\n: connect to the swarm master using the docker_gwbridge network\n\n\n--docker.tls\n: enable TLS using the docker-machine keys\n\n\n--web\n: activate the webUI on port 8080\n\n\n\n\nDeploy your apps\n\n\nWe can now deploy our app on the cluster, here \nwhoami\n, a simple web server in GO, on the network \nmy-net\n:\n\n\neval $(docker-machine env --swarm mhs-demo0)\ndocker run -d --name=whoami0 --net=my-net --env=\nconstraint:node==mhs-demo0\n emilevauge/whoami\ndocker run -d --name=whoami1 --net=my-net --env=\nconstraint:node==mhs-demo1\n emilevauge/whoami\n\n\n\n\nCheck that everything is started:\n\n\ndocker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                      NAMES\nba2c21488299        emilevauge/whoami   \n/whoamI\n                8 seconds ago       Up 9 seconds        80/tcp                                                     mhs-demo1/whoami1\n8147a7746e7a        emilevauge/whoami   \n/whoamI\n                19 seconds ago      Up 20 seconds       80/tcp                                                     mhs-demo0/whoami0\n8fbc39271b4c        traefik             \n/traefik -l DEBUG -c\n   36 seconds ago      Up 37 seconds       192.168.99.101:80-\n80/tcp, 192.168.99.101:8080-\n8080/tcp   mhs-demo0/serene_bhabha\n\n\n\n\nAccess to your apps through Tr\u00e6f\u026ak\n\n\ncurl -H Host:whoami0.traefik http://$(docker-machine ip mhs-demo0)\nHostname: 8147a7746e7a\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.3\nIP: fe80::42:aff:fe00:903\nIP: 172.18.0.3\nIP: fe80::42:acff:fe12:3\nGET / HTTP/1.1\nHost: 10.0.9.3:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.3:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c\n\ncurl -H Host:whoami1.traefik http://$(docker-machine ip mhs-demo0)\nHostname: ba2c21488299\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.4\nIP: fe80::42:aff:fe00:904\nIP: 172.18.0.2\nIP: fe80::42:acff:fe12:2\nGET / HTTP/1.1\nHost: 10.0.9.4:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.4:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c", 
            "title": "Swarm cluster"
        }, 
        {
            "location": "/user-guide/swarm/#swarm-cluster", 
            "text": "This section explains how to create a multi-host  swarm  cluster using  docker-machine  and how to deploy Tr\u00e6f\u026ak on it.\nThe cluster consists of:   2 servers  1 swarm master  2 swarm nodes  1  overlay  network (multi-host networking)", 
            "title": "Swarm cluster"
        }, 
        {
            "location": "/user-guide/swarm/#prerequisites", 
            "text": "You need to install  docker-machine  You need the latest  VirtualBox", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/user-guide/swarm/#cluster-provisioning", 
            "text": "We first follow  this guide  to create the cluster.", 
            "title": "Cluster provisioning"
        }, 
        {
            "location": "/user-guide/swarm/#create-machine-mh-keystore", 
            "text": "This machine is the service registry of our cluster.  docker-machine create -d virtualbox mh-keystore  Then we install the service registry  Consul  on this machine:  eval  $(docker-machine env mh-keystore) \ndocker run -d \\\n    -p  8500:8500  \\\n    -h  consul  \\\n    progrium/consul -server -bootstrap", 
            "title": "Create machine mh-keystore"
        }, 
        {
            "location": "/user-guide/swarm/#create-machine-mhs-demo0", 
            "text": "This machine is a swarm master and a swarm agent on it.  docker-machine create -d virtualbox \\\n    --swarm --swarm-master \\\n    --swarm-discovery= consul://$(docker-machine ip mh-keystore):8500  \\\n    --engine-opt= cluster-store=consul://$(docker-machine ip mh-keystore):8500  \\\n    --engine-opt= cluster-advertise=eth1:2376  \\\n    mhs-demo0", 
            "title": "Create machine mhs-demo0"
        }, 
        {
            "location": "/user-guide/swarm/#create-machine-mhs-demo1", 
            "text": "This machine have a swarm agent on it.  docker-machine create -d virtualbox \\\n    --swarm \\\n    --swarm-discovery= consul://$(docker-machine ip mh-keystore):8500  \\\n    --engine-opt= cluster-store=consul://$(docker-machine ip mh-keystore):8500  \\\n    --engine-opt= cluster-advertise=eth1:2376  \\\n    mhs-demo1", 
            "title": "Create machine mhs-demo1"
        }, 
        {
            "location": "/user-guide/swarm/#create-the-overlay-network", 
            "text": "Create the overlay network on the swarm master:  eval $(docker-machine env --swarm mhs-demo0)\ndocker network create --driver overlay --subnet=10.0.9.0/24 my-net", 
            "title": "Create the overlay Network"
        }, 
        {
            "location": "/user-guide/swarm/#deploy-trfk", 
            "text": "Deploy Tr\u00e6f\u026ak:  docker $(docker-machine config mhs-demo0) run \\\n    -d \\\n    -p 80:80 -p 8080:8080 \\\n    --net=my-net \\\n    -v /var/lib/boot2docker/:/ssl \\\n    traefik \\\n    -l DEBUG \\\n    -c /dev/null \\\n    --docker \\\n    --docker.domain=traefik \\\n    --docker.endpoint=tcp://$(docker-machine ip mhs-demo0):3376 \\\n    --docker.tls \\\n    --docker.tls.ca=/ssl/ca.pem \\\n    --docker.tls.cert=/ssl/server.pem \\\n    --docker.tls.key=/ssl/server-key.pem \\\n    --docker.tls.insecureSkipVerify \\\n    --docker.watch \\\n    --web  Let's explain this command:   -p 80:80 -p 8080:8080 : we bind ports 80 and 8080  --net=my-net : run the container on the network my-net  -v /var/lib/boot2docker/:/ssl : mount the ssl keys generated by docker-machine  -c /dev/null : empty config file  --docker : enable docker backend  --docker.endpoint=tcp://172.18.0.1:3376 : connect to the swarm master using the docker_gwbridge network  --docker.tls : enable TLS using the docker-machine keys  --web : activate the webUI on port 8080", 
            "title": "Deploy Tr\u00e6f\u026ak"
        }, 
        {
            "location": "/user-guide/swarm/#deploy-your-apps", 
            "text": "We can now deploy our app on the cluster, here  whoami , a simple web server in GO, on the network  my-net :  eval $(docker-machine env --swarm mhs-demo0)\ndocker run -d --name=whoami0 --net=my-net --env= constraint:node==mhs-demo0  emilevauge/whoami\ndocker run -d --name=whoami1 --net=my-net --env= constraint:node==mhs-demo1  emilevauge/whoami  Check that everything is started:  docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                      NAMES\nba2c21488299        emilevauge/whoami    /whoamI                 8 seconds ago       Up 9 seconds        80/tcp                                                     mhs-demo1/whoami1\n8147a7746e7a        emilevauge/whoami    /whoamI                 19 seconds ago      Up 20 seconds       80/tcp                                                     mhs-demo0/whoami0\n8fbc39271b4c        traefik              /traefik -l DEBUG -c    36 seconds ago      Up 37 seconds       192.168.99.101:80- 80/tcp, 192.168.99.101:8080- 8080/tcp   mhs-demo0/serene_bhabha", 
            "title": "Deploy your apps"
        }, 
        {
            "location": "/user-guide/swarm/#access-to-your-apps-through-trfk", 
            "text": "curl -H Host:whoami0.traefik http://$(docker-machine ip mhs-demo0)\nHostname: 8147a7746e7a\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.3\nIP: fe80::42:aff:fe00:903\nIP: 172.18.0.3\nIP: fe80::42:acff:fe12:3\nGET / HTTP/1.1\nHost: 10.0.9.3:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.3:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c\n\ncurl -H Host:whoami1.traefik http://$(docker-machine ip mhs-demo0)\nHostname: ba2c21488299\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.4\nIP: fe80::42:aff:fe00:904\nIP: 172.18.0.2\nIP: fe80::42:acff:fe12:2\nGET / HTTP/1.1\nHost: 10.0.9.4:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.4:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c", 
            "title": "Access to your apps through Tr\u00e6f\u026ak"
        }, 
        {
            "location": "/user-guide/swarm-mode/", 
            "text": "Docker Swarm (mode) cluster\n\n\nThis section explains how to create a multi-host docker cluster with\nswarm mode using \ndocker-machine\n and\nhow to deploy Tr\u00e6f\u026ak on it.\n\n\nThe cluster consists of:\n\n\n\n\n3 servers\n\n\n1 manager\n\n\n2 workers\n\n\n1 \noverlay\n network\n(multi-host networking)\n\n\n\n\nPrerequisites\n\n\n\n\nYou will need to install \ndocker-machine\n\n\nYou will need the latest \nVirtualBox\n\n\n\n\nCluster provisioning\n\n\nFirst, let's create all the required nodes. It's a shorter version of\nthe \nswarm tutorial\n.\n\n\ndocker-machine create -d virtualbox manager\ndocker-machine create -d virtualbox worker1\ndocker-machine create -d virtualbox worker2\n\n\n\n\nThen, let's setup the cluster, in order :\n\n\n\n\ninitialize the cluster\n\n\nget the token for other host to join\n\n\non both workers, join the cluster with the token\n\n\n\n\ndocker-machine ssh manager \ndocker swarm init \\\n    --listen-addr $(docker-machine ip manager) \\\n    --advertise-addr $(docker-machine ip manager)\n\n\nexport worker_token=$(docker-machine ssh manager \ndocker swarm \\\njoin-token worker -q\n)\n\ndocker-machine ssh worker1 \ndocker swarm join \\\n    --token=${worker_token} \\\n    --listen-addr $(docker-machine ip worker1) \\\n    --advertise-addr $(docker-machine ip worker1) \\\n    $(docker-machine ip manager)\n\n\ndocker-machine ssh worker2 \ndocker swarm join \\\n    --token=${worker_token} \\\n    --listen-addr $(docker-machine ip worker2) \\\n    --advertise-addr $(docker-machine ip worker2) \\\n    $(docker-machine ip manager)\n\n\n\n\n\nLet's validate the cluster is up and running.\n\n\ndocker-machine ssh manager docker node ls\nID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS\n2a770ov9vixeadep674265u1n    worker1   Ready   Active\ndbi3or4q8ii8elbws70g4hkdh *  manager   Ready   Active        Leader\nesbhhy6vnqv90xomjaomdgy46    worker2   Ready   Active\n\n\n\n\nFinally, let's create a network for Tr\u00e6fik to use.\n\n\ndocker-machine ssh manager \ndocker network create --driver=overlay traefik-net\n\n\n\n\n\nDeploy Tr\u00e6fik\n\n\nLet's deploy Tr\u00e6fik as a docker service in our cluster. The only\nrequirement for Tr\u00e6fik to work with swarm mode is that it needs to run\non a manager node \u2014 we are going to use a\n\nconstraint\n for\nthat.\n\n\ndocker-machine ssh manager \ndocker service create \\\n    --name traefik \\\n    --constraint=node.role==manager \\\n    --publish 80:80 --publish 8080:8080 \\\n    --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \\\n    --network traefik-net \\\n    traefik \\\n    --docker \\\n    --docker.swarmmode \\\n    --docker.domain=traefik \\\n    --docker.watch \\\n    --web\n\n\n\n\n\nLet's explain this command:\n\n\n\n\n--publish 80:80 --publish 8080:8080\n: we publish port \n80\n and\n  \n8080\n on the cluster.\n\n\n--constraint=node.role==manager\n: we ask docker to schedule Tr\u00e6fik\n  on a manager node.\n\n\n--mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock\n:\n  we bind mount the docker socket where Tr\u00e6fik is scheduled to be able\n  to speak to the daemon.\n\n\n--network traefik-net\n: we attach the Tr\u00e6fik service (and thus\n  the underlying container) to the \ntraefik-net\n network.\n\n\n--docker\n: enable docker backend, and \n--docker.swarmmode\n to\n  enable the swarm mode on Tr\u00e6fik.\n\n\n--web\n: activate the webUI on port 8080\n\n\n\n\nDeploy your apps\n\n\nWe can now deploy our app on the cluster,\nhere \nwhoami\n, a simple web\nserver in Go. We start 2 services, on the \ntraefik-net\n network.\n\n\ndocker-machine ssh manager \ndocker service create \\\n    --name whoami0 \\\n    --label traefik.port=80 \\\n    --network traefik-net \\\n    emilevauge/whoami\n\n\ndocker-machine ssh manager \ndocker service create \\\n    --name whoami1 \\\n    --label traefik.port=80 \\\n    --network traefik-net \\\n    --label traefik.backend.loadbalancer.sticky=true \\\n    emilevauge/whoami\n\n\n\n\n\nNote that we set whoami1 to use sticky sessions (\n--label traefik.backend.loadbalancer.sticky=true\n).  We'll demonstrate that later.\nIf using \ndocker stack deploy\n, there is \na specific way that the labels must be defined in the docker-compose file\n.\n\n\nCheck that everything is scheduled and started:\n\n\ndocker-machine ssh manager \ndocker service ls\n\nID            NAME     REPLICAS  IMAGE              COMMAND\nab046gpaqtln  whoami0  1/1       emilevauge/whoami\ncgfg5ifzrpgm  whoami1  1/1       emilevauge/whoami\ndtpl249tfghc  traefik  1/1       traefik            --docker --docker.swarmmode --docker.domain=traefik --docker.watch --web\n\n\n\n\nAccess to your apps through Tr\u00e6f\u026ak\n\n\ncurl -H Host:whoami0.traefik http://$(docker-machine ip manager)\nHostname: 8147a7746e7a\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.3\nIP: fe80::42:aff:fe00:903\nIP: 172.18.0.3\nIP: fe80::42:acff:fe12:3\nGET / HTTP/1.1\nHost: 10.0.9.3:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.3:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c\n\ncurl -H Host:whoami1.traefik http://$(docker-machine ip manager)\nHostname: ba2c21488299\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.4\nIP: fe80::42:aff:fe00:904\nIP: 172.18.0.2\nIP: fe80::42:acff:fe12:2\nGET / HTTP/1.1\nHost: 10.0.9.4:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.4:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c\n\n\n\n\nNote that as Tr\u00e6fik is published, you can access it from any machine\nand not only the manager.\n\n\ncurl -H Host:whoami0.traefik http://$(docker-machine ip worker1)\nHostname: 8147a7746e7a\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.3\nIP: fe80::42:aff:fe00:903\nIP: 172.18.0.3\nIP: fe80::42:acff:fe12:3\nGET / HTTP/1.1\nHost: 10.0.9.3:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.3:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c\n\ncurl -H Host:whoami1.traefik http://$(docker-machine ip worker2)\nHostname: ba2c21488299\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.4\nIP: fe80::42:aff:fe00:904\nIP: 172.18.0.2\nIP: fe80::42:acff:fe12:2\nGET / HTTP/1.1\nHost: 10.0.9.4:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.4:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c\n\n\n\n\nScale both services\n\n\ndocker-machine ssh manager \ndocker service scale whoami0=5\n\n\ndocker-machine ssh manager \ndocker service scale whoami1=5\n\n\n\n\n\nCheck that we now have 5 replicas of each \nwhoami\n service:\n\n\ndocker-machine ssh manager \ndocker service ls\n\nID            NAME     REPLICAS  IMAGE              COMMAND\nab046gpaqtln  whoami0  5/5       emilevauge/whoami\ncgfg5ifzrpgm  whoami1  5/5       emilevauge/whoami\ndtpl249tfghc  traefik  1/1       traefik            --docker --docker.swarmmode --docker.domain=traefik --docker.watch --web\n\n\n\n\nAccess to your whoami0 through Tr\u00e6f\u026ak multiple times.\n\n\nRepeat the following command multiple times and note that the Hostname changes each time as Traefik load balances each request against the 5 tasks.\n\n\ncurl -H Host:whoami0.traefik http://$(docker-machine ip manager)\nHostname: 8147a7746e7a\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.3\nIP: fe80::42:aff:fe00:903\nIP: 172.18.0.3\nIP: fe80::42:acff:fe12:3\nGET / HTTP/1.1\nHost: 10.0.9.3:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.3:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c\n\n\n\n\nDo the same against whoami1.  \n\n\ncurl -H Host:whoami1.traefik http://$(docker-machine ip manager)\nHostname: ba2c21488299\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.4\nIP: fe80::42:aff:fe00:904\nIP: 172.18.0.2\nIP: fe80::42:acff:fe12:2\nGET / HTTP/1.1\nHost: 10.0.9.4:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.4:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c\n\n\n\n\nWait, I thought we added the sticky flag to whoami1?  Traefik relies on a cookie to maintain stickyness so you'll need to test this with a browser.\n\n\nFirst you need to add whoami1.traefik to your hosts file:\n\n\nif [ -n \n$(grep whoami1.traefik /etc/hosts)\n ];  \nthen \necho \nwhoami1.traefik already exists (make sure the ip is current)\n; \nelse \nsudo -- sh -c -e \necho '$(docker-machine ip manager)\\twhoami1.traefik' \n\n /etc/hosts\n; \nfi\n\n\n\n\nNow open your browser and go to http://whoami1.traefik/\n\n\nYou will now see that stickyness is maintained.", 
            "title": "Swarm mode cluster"
        }, 
        {
            "location": "/user-guide/swarm-mode/#docker-swarm-mode-cluster", 
            "text": "This section explains how to create a multi-host docker cluster with\nswarm mode using  docker-machine  and\nhow to deploy Tr\u00e6f\u026ak on it.  The cluster consists of:   3 servers  1 manager  2 workers  1  overlay  network\n(multi-host networking)", 
            "title": "Docker Swarm (mode) cluster"
        }, 
        {
            "location": "/user-guide/swarm-mode/#prerequisites", 
            "text": "You will need to install  docker-machine  You will need the latest  VirtualBox", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/user-guide/swarm-mode/#cluster-provisioning", 
            "text": "First, let's create all the required nodes. It's a shorter version of\nthe  swarm tutorial .  docker-machine create -d virtualbox manager\ndocker-machine create -d virtualbox worker1\ndocker-machine create -d virtualbox worker2  Then, let's setup the cluster, in order :   initialize the cluster  get the token for other host to join  on both workers, join the cluster with the token   docker-machine ssh manager  docker swarm init \\\n    --listen-addr $(docker-machine ip manager) \\\n    --advertise-addr $(docker-machine ip manager) \n\nexport worker_token=$(docker-machine ssh manager  docker swarm \\\njoin-token worker -q )\n\ndocker-machine ssh worker1  docker swarm join \\\n    --token=${worker_token} \\\n    --listen-addr $(docker-machine ip worker1) \\\n    --advertise-addr $(docker-machine ip worker1) \\\n    $(docker-machine ip manager) \n\ndocker-machine ssh worker2  docker swarm join \\\n    --token=${worker_token} \\\n    --listen-addr $(docker-machine ip worker2) \\\n    --advertise-addr $(docker-machine ip worker2) \\\n    $(docker-machine ip manager)   Let's validate the cluster is up and running.  docker-machine ssh manager docker node ls\nID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS\n2a770ov9vixeadep674265u1n    worker1   Ready   Active\ndbi3or4q8ii8elbws70g4hkdh *  manager   Ready   Active        Leader\nesbhhy6vnqv90xomjaomdgy46    worker2   Ready   Active  Finally, let's create a network for Tr\u00e6fik to use.  docker-machine ssh manager  docker network create --driver=overlay traefik-net", 
            "title": "Cluster provisioning"
        }, 
        {
            "location": "/user-guide/swarm-mode/#deploy-trfik", 
            "text": "Let's deploy Tr\u00e6fik as a docker service in our cluster. The only\nrequirement for Tr\u00e6fik to work with swarm mode is that it needs to run\non a manager node \u2014 we are going to use a constraint  for\nthat.  docker-machine ssh manager  docker service create \\\n    --name traefik \\\n    --constraint=node.role==manager \\\n    --publish 80:80 --publish 8080:8080 \\\n    --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \\\n    --network traefik-net \\\n    traefik \\\n    --docker \\\n    --docker.swarmmode \\\n    --docker.domain=traefik \\\n    --docker.watch \\\n    --web   Let's explain this command:   --publish 80:80 --publish 8080:8080 : we publish port  80  and\n   8080  on the cluster.  --constraint=node.role==manager : we ask docker to schedule Tr\u00e6fik\n  on a manager node.  --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock :\n  we bind mount the docker socket where Tr\u00e6fik is scheduled to be able\n  to speak to the daemon.  --network traefik-net : we attach the Tr\u00e6fik service (and thus\n  the underlying container) to the  traefik-net  network.  --docker : enable docker backend, and  --docker.swarmmode  to\n  enable the swarm mode on Tr\u00e6fik.  --web : activate the webUI on port 8080", 
            "title": "Deploy Tr\u00e6fik"
        }, 
        {
            "location": "/user-guide/swarm-mode/#deploy-your-apps", 
            "text": "We can now deploy our app on the cluster,\nhere  whoami , a simple web\nserver in Go. We start 2 services, on the  traefik-net  network.  docker-machine ssh manager  docker service create \\\n    --name whoami0 \\\n    --label traefik.port=80 \\\n    --network traefik-net \\\n    emilevauge/whoami \n\ndocker-machine ssh manager  docker service create \\\n    --name whoami1 \\\n    --label traefik.port=80 \\\n    --network traefik-net \\\n    --label traefik.backend.loadbalancer.sticky=true \\\n    emilevauge/whoami   Note that we set whoami1 to use sticky sessions ( --label traefik.backend.loadbalancer.sticky=true ).  We'll demonstrate that later.\nIf using  docker stack deploy , there is  a specific way that the labels must be defined in the docker-compose file .  Check that everything is scheduled and started:  docker-machine ssh manager  docker service ls \nID            NAME     REPLICAS  IMAGE              COMMAND\nab046gpaqtln  whoami0  1/1       emilevauge/whoami\ncgfg5ifzrpgm  whoami1  1/1       emilevauge/whoami\ndtpl249tfghc  traefik  1/1       traefik            --docker --docker.swarmmode --docker.domain=traefik --docker.watch --web", 
            "title": "Deploy your apps"
        }, 
        {
            "location": "/user-guide/swarm-mode/#access-to-your-apps-through-trfk", 
            "text": "curl -H Host:whoami0.traefik http://$(docker-machine ip manager)\nHostname: 8147a7746e7a\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.3\nIP: fe80::42:aff:fe00:903\nIP: 172.18.0.3\nIP: fe80::42:acff:fe12:3\nGET / HTTP/1.1\nHost: 10.0.9.3:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.3:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c\n\ncurl -H Host:whoami1.traefik http://$(docker-machine ip manager)\nHostname: ba2c21488299\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.4\nIP: fe80::42:aff:fe00:904\nIP: 172.18.0.2\nIP: fe80::42:acff:fe12:2\nGET / HTTP/1.1\nHost: 10.0.9.4:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.4:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c  Note that as Tr\u00e6fik is published, you can access it from any machine\nand not only the manager.  curl -H Host:whoami0.traefik http://$(docker-machine ip worker1)\nHostname: 8147a7746e7a\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.3\nIP: fe80::42:aff:fe00:903\nIP: 172.18.0.3\nIP: fe80::42:acff:fe12:3\nGET / HTTP/1.1\nHost: 10.0.9.3:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.3:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c\n\ncurl -H Host:whoami1.traefik http://$(docker-machine ip worker2)\nHostname: ba2c21488299\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.4\nIP: fe80::42:aff:fe00:904\nIP: 172.18.0.2\nIP: fe80::42:acff:fe12:2\nGET / HTTP/1.1\nHost: 10.0.9.4:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.4:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c", 
            "title": "Access to your apps through Tr\u00e6f\u026ak"
        }, 
        {
            "location": "/user-guide/swarm-mode/#scale-both-services", 
            "text": "docker-machine ssh manager  docker service scale whoami0=5 \n\ndocker-machine ssh manager  docker service scale whoami1=5   Check that we now have 5 replicas of each  whoami  service:  docker-machine ssh manager  docker service ls \nID            NAME     REPLICAS  IMAGE              COMMAND\nab046gpaqtln  whoami0  5/5       emilevauge/whoami\ncgfg5ifzrpgm  whoami1  5/5       emilevauge/whoami\ndtpl249tfghc  traefik  1/1       traefik            --docker --docker.swarmmode --docker.domain=traefik --docker.watch --web", 
            "title": "Scale both services"
        }, 
        {
            "location": "/user-guide/swarm-mode/#access-to-your-whoami0-through-trfk-multiple-times", 
            "text": "Repeat the following command multiple times and note that the Hostname changes each time as Traefik load balances each request against the 5 tasks.  curl -H Host:whoami0.traefik http://$(docker-machine ip manager)\nHostname: 8147a7746e7a\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.3\nIP: fe80::42:aff:fe00:903\nIP: 172.18.0.3\nIP: fe80::42:acff:fe12:3\nGET / HTTP/1.1\nHost: 10.0.9.3:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.3:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c  Do the same against whoami1.    curl -H Host:whoami1.traefik http://$(docker-machine ip manager)\nHostname: ba2c21488299\nIP: 127.0.0.1\nIP: ::1\nIP: 10.0.9.4\nIP: fe80::42:aff:fe00:904\nIP: 172.18.0.2\nIP: fe80::42:acff:fe12:2\nGET / HTTP/1.1\nHost: 10.0.9.4:80\nUser-Agent: curl/7.35.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 192.168.99.1\nX-Forwarded-Host: 10.0.9.4:80\nX-Forwarded-Proto: http\nX-Forwarded-Server: 8fbc39271b4c  Wait, I thought we added the sticky flag to whoami1?  Traefik relies on a cookie to maintain stickyness so you'll need to test this with a browser.  First you need to add whoami1.traefik to your hosts file:  if [ -n  $(grep whoami1.traefik /etc/hosts)  ];  \nthen \necho  whoami1.traefik already exists (make sure the ip is current) ; \nelse \nsudo -- sh -c -e  echo '$(docker-machine ip manager)\\twhoami1.traefik'   /etc/hosts ; \nfi  Now open your browser and go to http://whoami1.traefik/  You will now see that stickyness is maintained.", 
            "title": "Access to your whoami0 through Tr\u00e6f\u026ak multiple times."
        }, 
        {
            "location": "/user-guide/kubernetes/", 
            "text": "Kubernetes Ingress Controller\n\n\nThis guide explains how to use Tr\u00e6f\u026ak as an Ingress controller in a Kubernetes cluster.\nIf you are not familiar with Ingresses in Kubernetes you might want to read the \nKubernetes user guide\n\n\nThe config files used in this guide can be found in the \nexamples directory\n\n\nPrerequisites\n\n\n\n\n\n\nA working Kubernetes cluster. If you want to follow along with this guide, you should setup \nminikube\n\non your machine, as it is the quickest way to get a local Kubernetes cluster setup for experimentation and development.\n\n\n\n\n\n\nThe \nkubectl\n binary should be \ninstalled on your workstation\n.\n\n\n\n\n\n\nDeploy Tr\u00e6f\u026ak\n\n\nWe are going to deploy Tr\u00e6f\u026ak with a\n\nDeployment\n, as this will\nallow you to easily roll out config changes or update the image.\n\n\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: traefik-ingress-controller\n  namespace: kube-system\n  labels:\n    k8s-app: traefik-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: traefik-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: traefik-ingress-lb\n        name: traefik-ingress-lb\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - image: traefik\n        name: traefik-ingress-lb\n        resources:\n          limits:\n            cpu: 200m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 8080\n        args:\n        - --web\n        - --kubernetes\n\n\n\n\nexamples/k8s/traefik.yaml\n\n\n\n\nnotice that we binding port 80 on the Tr\u00e6f\u026ak container to port 80 on the host.\nWith a multi node cluster we might expose Tr\u00e6f\u026ak with a NodePort or LoadBalancer service\nand run more than 1 replica of Tr\u00e6f\u026ak for high availability.\n\n\n\n\nTo deploy Tr\u00e6f\u026ak to your cluster start by submitting the deployment to the cluster with \nkubectl\n:\n\n\nkubectl apply -f examples/k8s/traefik.yaml\n\n\n\n\nCheck the deployment\n\n\nNow lets check if our deployment was successful.\n\n\nStart by listing the pods in the \nkube-system\n namespace:\n\n\n$kubectl --namespace=kube-system get pods\n\nNAME                                         READY     STATUS    RESTARTS   AGE\nkube-addon-manager-minikubevm                1/1       Running   0          4h\nkubernetes-dashboard-s8krj                   1/1       Running   0          4h\ntraefik-ingress-controller-678226159-eqseo   1/1       Running   0          7m\n\n\n\n\nYou should see that after submitting the Deployment to Kubernetes it has launched\na pod, and it is now running. \nIt might take a few moments for kubernetes to pull\nthe Tr\u00e6f\u026ak image and start the container.\n\n\n\n\nYou could also check the deployment with the Kubernetes dashboard, run\n\nminikube dashboard\n to open it in your browser, then choose the \nkube-system\n\nnamespace from the menu at the top right of the screen.\n\n\n\n\nYou should now be able to access Tr\u00e6f\u026ak on port 80 of your minikube instance.\n\n\ncurl $(minikube ip)\n404 page not found\n\n\n\n\n\n\nWe expect to see a 404 response here as we haven't yet given Tr\u00e6f\u026ak any configuration.\n\n\n\n\nSubmitting An Ingress to the cluster.\n\n\nLets start by creating a Service and an Ingress that will expose the\n\nTr\u00e6f\u026ak Web UI\n.\n\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: traefik-web-ui\n  namespace: kube-system\nspec:\n  selector:\n    k8s-app: traefik-ingress-lb\n  ports:\n  - port: 80\n    targetPort: 8080\n---\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: traefik-web-ui\n  namespace: kube-system\nspec:\n  rules:\n  - host: traefik-ui.local\n    http:\n      paths:\n      - backend:\n          serviceName: traefik-web-ui\n          servicePort: 80\n\n\n\n\nexamples/k8s/ui.yaml\n\n\nkubectl apply -f examples/k8s/ui.yaml\n\n\n\n\nNow lets setup an entry in our /etc/hosts file to route \ntraefik-ui.local\n\nto our cluster.\n\n\n\n\nIn production you would want to set up real dns entries.\n\n\nYou can get the ip address of your minikube instance by running \nminikube ip\n\n\n\n\necho \n$(minikube ip) traefik-ui.local\n | sudo tee -a /etc/hosts\n\n\n\n\nWe should now be able to visit \ntraefik-ui.local\n in the browser and view the Tr\u00e6f\u026ak Web UI.\n\n\nName based routing\n\n\nIn this example we are going to setup websites for 3 of the United Kingdoms\nbest loved cheeses, Cheddar, Stilton and Wensleydale.\n\n\nFirst lets start by launching the 3 pods for the cheese websites.\n\n\n---\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: stilton\n  labels:\n    app: cheese\n    cheese: stilton\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cheese\n      task: stilton\n  template:\n    metadata:\n      labels:\n        app: cheese\n        task: stilton\n        version: v0.0.1\n    spec:\n      containers:\n      - name: cheese\n        image: errm/cheese:stilton\n        resources:\n          requests:\n            cpu: 100m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 80\n---\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: cheddar\n  labels:\n    app: cheese\n    cheese: cheddar\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cheese\n      task: cheddar\n  template:\n    metadata:\n      labels:\n        app: cheese\n        task: cheddar\n        version: v0.0.1\n    spec:\n      containers:\n      - name: cheese\n        image: errm/cheese:cheddar\n        resources:\n          requests:\n            cpu: 100m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 80\n---\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: wensleydale\n  labels:\n    app: cheese\n    cheese: wensleydale\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cheese\n      task: wensleydale\n  template:\n    metadata:\n      labels:\n        app: cheese\n        task: wensleydale\n        version: v0.0.1\n    spec:\n      containers:\n      - name: cheese\n        image: errm/cheese:wensleydale\n        resources:\n          requests:\n            cpu: 100m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 80\n\n\n\n\nexamples/k8s/cheese-deployments.yaml\n\n\nkubectl apply -f examples/k8s/cheese-deployments.yaml\n\n\n\n\nNext we need to setup a service for each of the cheese pods.\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: stilton\nspec:\n  ports:\n  - name: http\n    targetPort: 80\n    port: 80\n  selector:\n    app: cheese\n    task: stilton\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: cheddar\nspec:\n  ports:\n  - name: http\n    targetPort: 80\n    port: 80\n  selector:\n    app: cheese\n    task: cheddar\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: wensleydale\n  annotations:\n    traefik.backend.circuitbreaker: \nNetworkErrorRatio() \n 0.5\n\nspec:\n  ports:\n  - name: http\n    targetPort: 80\n    port: 80\n  selector:\n    app: cheese\n    task: wensleydale\n\n\n\n\n\n\nNotice that we also set a \ncircuit breaker expression\n for one of the backends\nby setting the \ntraefik.backend.circuitbreaker\n annotation on the service.\n\n\n\n\nexamples/k8s/cheese-services.yaml\n\n\nkubectl apply -f examples/k8s/cheese-services.yaml\n\n\n\n\nNow we can submit an ingress for the cheese websites.\n\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: cheese\nspec:\n  rules:\n  - host: stilton.local\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: stilton\n          servicePort: http\n  - host: cheddar.local\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: cheddar\n          servicePort: http\n  - host: wensleydale.local\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: wensleydale\n          servicePort: http\n\n\n\n\nexamples/k8s/cheese-ingress.yaml\n\n\n\n\nNotice that we list each hostname, and add a backend service.\n\n\n\n\nkubectl apply -f examples/k8s/cheese-ingress.yaml\n\n\n\n\nNow visit the \nTr\u00e6f\u026ak dashboard\n and you should\nsee a frontend for each host. Along with a backend listing for each service\nwith a Server set up for each pod.\n\n\nIf you edit your \n/etc/hosts\n again you should be able to access the cheese\nwebsites in your browser.\n\n\necho \n$(minikube ip) stilton.local cheddar.local wensleydale.local\n | sudo tee -a /etc/hosts\n\n\n\n\n\n\nStilton\n\n\nCheddar\n\n\nWensleydale\n\n\n\n\nPath based routing\n\n\nNow lets suppose that our fictional client has decided that while they are\nsuper happy about our cheesy web design, when they asked for 3 websites\nthey had not really bargained on having to buy 3 domain names.\n\n\nNo problem, we say, why don't we reconfigure the sites to host all 3 under one domain.\n\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: cheeses\n  annotations:\n    traefik.frontend.rule.type: pathprefixstrip\nspec:\n  rules:\n  - host: cheeses.local\n    http:\n      paths:\n      - path: /stilton\n        backend:\n          serviceName: stilton\n          servicePort: http\n      - path: /cheddar\n        backend:\n          serviceName: cheddar\n          servicePort: http\n      - path: /wensleydale\n        backend:\n          serviceName: wensleydale\n          servicePort: http\n\n\n\n\nexamples/k8s/cheeses-ingress.yaml\n\n\n\n\nNotice that we are configuring Tr\u00e6f\u026ak to strip the prefix from the url path\nwith the \ntraefik.frontend.rule.type\n annotation so that we can use\nthe containers from the previous example without modification.\n\n\n\n\nkubectl apply -f examples/k8s/cheeses-ingress.yaml\n\n\n\n\necho \n$(minikube ip) cheeses.local\n | sudo tee -a /etc/hosts\n\n\n\n\nYou should now be able to visit the websites in your browser.\n\n\n\n\ncheeses.local/stilton\n\n\ncheeses.local/cheddar\n\n\ncheeses.local/wensleydale", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/user-guide/kubernetes/#kubernetes-ingress-controller", 
            "text": "This guide explains how to use Tr\u00e6f\u026ak as an Ingress controller in a Kubernetes cluster.\nIf you are not familiar with Ingresses in Kubernetes you might want to read the  Kubernetes user guide  The config files used in this guide can be found in the  examples directory", 
            "title": "Kubernetes Ingress Controller"
        }, 
        {
            "location": "/user-guide/kubernetes/#prerequisites", 
            "text": "A working Kubernetes cluster. If you want to follow along with this guide, you should setup  minikube \non your machine, as it is the quickest way to get a local Kubernetes cluster setup for experimentation and development.    The  kubectl  binary should be  installed on your workstation .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/user-guide/kubernetes/#deploy-trfk", 
            "text": "We are going to deploy Tr\u00e6f\u026ak with a Deployment , as this will\nallow you to easily roll out config changes or update the image.  kind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: traefik-ingress-controller\n  namespace: kube-system\n  labels:\n    k8s-app: traefik-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: traefik-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: traefik-ingress-lb\n        name: traefik-ingress-lb\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - image: traefik\n        name: traefik-ingress-lb\n        resources:\n          limits:\n            cpu: 200m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 8080\n        args:\n        - --web\n        - --kubernetes  examples/k8s/traefik.yaml   notice that we binding port 80 on the Tr\u00e6f\u026ak container to port 80 on the host.\nWith a multi node cluster we might expose Tr\u00e6f\u026ak with a NodePort or LoadBalancer service\nand run more than 1 replica of Tr\u00e6f\u026ak for high availability.   To deploy Tr\u00e6f\u026ak to your cluster start by submitting the deployment to the cluster with  kubectl :  kubectl apply -f examples/k8s/traefik.yaml", 
            "title": "Deploy Tr\u00e6f\u026ak"
        }, 
        {
            "location": "/user-guide/kubernetes/#check-the-deployment", 
            "text": "Now lets check if our deployment was successful.  Start by listing the pods in the  kube-system  namespace:  $kubectl --namespace=kube-system get pods\n\nNAME                                         READY     STATUS    RESTARTS   AGE\nkube-addon-manager-minikubevm                1/1       Running   0          4h\nkubernetes-dashboard-s8krj                   1/1       Running   0          4h\ntraefik-ingress-controller-678226159-eqseo   1/1       Running   0          7m  You should see that after submitting the Deployment to Kubernetes it has launched\na pod, and it is now running.  It might take a few moments for kubernetes to pull\nthe Tr\u00e6f\u026ak image and start the container.   You could also check the deployment with the Kubernetes dashboard, run minikube dashboard  to open it in your browser, then choose the  kube-system \nnamespace from the menu at the top right of the screen.   You should now be able to access Tr\u00e6f\u026ak on port 80 of your minikube instance.  curl $(minikube ip)\n404 page not found   We expect to see a 404 response here as we haven't yet given Tr\u00e6f\u026ak any configuration.", 
            "title": "Check the deployment"
        }, 
        {
            "location": "/user-guide/kubernetes/#submitting-an-ingress-to-the-cluster", 
            "text": "Lets start by creating a Service and an Ingress that will expose the Tr\u00e6f\u026ak Web UI .  apiVersion: v1\nkind: Service\nmetadata:\n  name: traefik-web-ui\n  namespace: kube-system\nspec:\n  selector:\n    k8s-app: traefik-ingress-lb\n  ports:\n  - port: 80\n    targetPort: 8080\n---\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: traefik-web-ui\n  namespace: kube-system\nspec:\n  rules:\n  - host: traefik-ui.local\n    http:\n      paths:\n      - backend:\n          serviceName: traefik-web-ui\n          servicePort: 80  examples/k8s/ui.yaml  kubectl apply -f examples/k8s/ui.yaml  Now lets setup an entry in our /etc/hosts file to route  traefik-ui.local \nto our cluster.   In production you would want to set up real dns entries.  You can get the ip address of your minikube instance by running  minikube ip   echo  $(minikube ip) traefik-ui.local  | sudo tee -a /etc/hosts  We should now be able to visit  traefik-ui.local  in the browser and view the Tr\u00e6f\u026ak Web UI.", 
            "title": "Submitting An Ingress to the cluster."
        }, 
        {
            "location": "/user-guide/kubernetes/#name-based-routing", 
            "text": "In this example we are going to setup websites for 3 of the United Kingdoms\nbest loved cheeses, Cheddar, Stilton and Wensleydale.  First lets start by launching the 3 pods for the cheese websites.  ---\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: stilton\n  labels:\n    app: cheese\n    cheese: stilton\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cheese\n      task: stilton\n  template:\n    metadata:\n      labels:\n        app: cheese\n        task: stilton\n        version: v0.0.1\n    spec:\n      containers:\n      - name: cheese\n        image: errm/cheese:stilton\n        resources:\n          requests:\n            cpu: 100m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 80\n---\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: cheddar\n  labels:\n    app: cheese\n    cheese: cheddar\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cheese\n      task: cheddar\n  template:\n    metadata:\n      labels:\n        app: cheese\n        task: cheddar\n        version: v0.0.1\n    spec:\n      containers:\n      - name: cheese\n        image: errm/cheese:cheddar\n        resources:\n          requests:\n            cpu: 100m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 80\n---\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: wensleydale\n  labels:\n    app: cheese\n    cheese: wensleydale\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cheese\n      task: wensleydale\n  template:\n    metadata:\n      labels:\n        app: cheese\n        task: wensleydale\n        version: v0.0.1\n    spec:\n      containers:\n      - name: cheese\n        image: errm/cheese:wensleydale\n        resources:\n          requests:\n            cpu: 100m\n            memory: 50Mi\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        ports:\n        - containerPort: 80  examples/k8s/cheese-deployments.yaml  kubectl apply -f examples/k8s/cheese-deployments.yaml  Next we need to setup a service for each of the cheese pods.  ---\napiVersion: v1\nkind: Service\nmetadata:\n  name: stilton\nspec:\n  ports:\n  - name: http\n    targetPort: 80\n    port: 80\n  selector:\n    app: cheese\n    task: stilton\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: cheddar\nspec:\n  ports:\n  - name: http\n    targetPort: 80\n    port: 80\n  selector:\n    app: cheese\n    task: cheddar\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: wensleydale\n  annotations:\n    traefik.backend.circuitbreaker:  NetworkErrorRatio()   0.5 \nspec:\n  ports:\n  - name: http\n    targetPort: 80\n    port: 80\n  selector:\n    app: cheese\n    task: wensleydale   Notice that we also set a  circuit breaker expression  for one of the backends\nby setting the  traefik.backend.circuitbreaker  annotation on the service.   examples/k8s/cheese-services.yaml  kubectl apply -f examples/k8s/cheese-services.yaml  Now we can submit an ingress for the cheese websites.  apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: cheese\nspec:\n  rules:\n  - host: stilton.local\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: stilton\n          servicePort: http\n  - host: cheddar.local\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: cheddar\n          servicePort: http\n  - host: wensleydale.local\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: wensleydale\n          servicePort: http  examples/k8s/cheese-ingress.yaml   Notice that we list each hostname, and add a backend service.   kubectl apply -f examples/k8s/cheese-ingress.yaml  Now visit the  Tr\u00e6f\u026ak dashboard  and you should\nsee a frontend for each host. Along with a backend listing for each service\nwith a Server set up for each pod.  If you edit your  /etc/hosts  again you should be able to access the cheese\nwebsites in your browser.  echo  $(minikube ip) stilton.local cheddar.local wensleydale.local  | sudo tee -a /etc/hosts   Stilton  Cheddar  Wensleydale", 
            "title": "Name based routing"
        }, 
        {
            "location": "/user-guide/kubernetes/#path-based-routing", 
            "text": "Now lets suppose that our fictional client has decided that while they are\nsuper happy about our cheesy web design, when they asked for 3 websites\nthey had not really bargained on having to buy 3 domain names.  No problem, we say, why don't we reconfigure the sites to host all 3 under one domain.  apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: cheeses\n  annotations:\n    traefik.frontend.rule.type: pathprefixstrip\nspec:\n  rules:\n  - host: cheeses.local\n    http:\n      paths:\n      - path: /stilton\n        backend:\n          serviceName: stilton\n          servicePort: http\n      - path: /cheddar\n        backend:\n          serviceName: cheddar\n          servicePort: http\n      - path: /wensleydale\n        backend:\n          serviceName: wensleydale\n          servicePort: http  examples/k8s/cheeses-ingress.yaml   Notice that we are configuring Tr\u00e6f\u026ak to strip the prefix from the url path\nwith the  traefik.frontend.rule.type  annotation so that we can use\nthe containers from the previous example without modification.   kubectl apply -f examples/k8s/cheeses-ingress.yaml  echo  $(minikube ip) cheeses.local  | sudo tee -a /etc/hosts  You should now be able to visit the websites in your browser.   cheeses.local/stilton  cheeses.local/cheddar  cheeses.local/wensleydale", 
            "title": "Path based routing"
        }, 
        {
            "location": "/user-guide/kv-config/", 
            "text": "Key-value store configuration\n\n\nBoth \nstatic global configuration\n and \ndynamic\n configuration can be sorted in a Key-value store.\n\n\nThis section explains how to launch Tr\u00e6f\u026ak using a configuration loaded from a Key-value store.\n\n\nTr\u00e6f\u026ak supports several Key-value stores:\n\n\n\n\nConsul\n\n\netcd\n\n\nZooKeeper\n \n\n\nboltdb\n\n\n\n\nStatic configuration in Key-value store\n\n\nWe will see the steps to set it up with an easy example. \nNote that we could do the same with any other Key-value Store.\n\n\ndocker-compose file for Consul\n\n\nThe Tr\u00e6f\u026ak global configuration will be getted from a \nConsul\n store. \n\n\nFirst we have to launch Consul in a container. \nThe \ndocker-compose file\n allows us to launch Consul and four instances of the trivial app \nemilevauge/whoamI\n : \n\n\nconsul:\n  image: progrium/consul\n  command: -server -bootstrap -log-level debug -ui-dir /ui\n  ports:\n    - \n8400:8400\n\n    - \n8500:8500\n\n    - \n8600:53/udp\n\n  expose:\n    - \n8300\n\n    - \n8301\n\n    - \n8301/udp\n\n    - \n8302\n\n    - \n8302/udp\n  \n\nwhoami1:\n  image: emilevauge/whoami\n\nwhoami2:\n  image: emilevauge/whoami\n\nwhoami3:\n  image: emilevauge/whoami\n\nwhoami4:\n  image: emilevauge/whoami\n\n\n\n\nUpload the configuration in the Key-value store\n\n\nWe should now fill the store with the Tr\u00e6f\u026ak global configuration, as we do with a \nTOML file configuration\n.\nTo do that, we can send the Key-value pairs via \ncurl commands\n or via the \nWeb UI\n.\n\n\nHopefully, Tr\u00e6f\u026ak allows automation of this process using the \nstoreconfig\n subcommand.\nPlease refer to the \nstore Tr\u00e6f\u026ak configuration\n section to get documentation on it.\n\n\nHere is the toml configuration we would like to store in the Key-value Store  :\n\n\nlogLevel = \nDEBUG\n\n\ndefaultEntryPoints = [\nhttp\n, \nhttps\n]\n\n[entryPoints]\n  [entryPoints.http]\n  address = \n:80\n\n  [entryPoints.https]\n  address = \n:443\n\n    [entryPoints.https.tls]\n      [[entryPoints.https.tls.certificates]]\n      CertFile = \nintegration/fixtures/https/snitest.com.cert\n\n      KeyFile = \nintegration/fixtures/https/snitest.com.key\n\n      [[entryPoints.https.tls.certificates]]\n      CertFile = \n-----BEGIN CERTIFICATE-----\n                      \ncert file content\n\n                      -----END CERTIFICATE-----\n\n      KeyFile = \n-----BEGIN CERTIFICATE-----\n                      \nkey file content\n\n                      -----END CERTIFICATE-----\n\n\n\n[consul]\n  endpoint = \n127.0.0.1:8500\n\n  watch = true\n  prefix = \ntraefik\n\n\n[web]\n  address = \n:8081\n\n\n\n\n\nAnd there, the same global configuration in the Key-value Store (using \nprefix = \"traefik\"\n): \n\n\n\n\n\n\n\n\nKey\n\n\nValue\n\n\n\n\n\n\n\n\n\n\n/traefik/loglevel\n\n\nDEBUG\n\n\n\n\n\n\n/traefik/defaultentrypoints/0\n\n\nhttp\n\n\n\n\n\n\n/traefik/defaultentrypoints/1\n\n\nhttps\n\n\n\n\n\n\n/traefik/entrypoints/http/address\n\n\n:80\n\n\n\n\n\n\n/traefik/entrypoints/https/address\n\n\n:443\n\n\n\n\n\n\n/traefik/entrypoints/https/tls/certificates/0/certfile\n\n\nintegration/fixtures/https/snitest.com.cert\n\n\n\n\n\n\n/traefik/entrypoints/https/tls/certificates/0/keyfile\n\n\nintegration/fixtures/https/snitest.com.key\n\n\n\n\n\n\n/traefik/entrypoints/https/tls/certificates/1/certfile\n\n\n--BEGIN CERTIFICATE--\ncert file content\n--END CERTIFICATE--\n\n\n\n\n\n\n/traefik/entrypoints/https/tls/certificates/1/keyfile\n\n\n--BEGIN CERTIFICATE--\nkey file content\n--END CERTIFICATE--\n\n\n\n\n\n\n/traefik/consul/endpoint\n\n\n127.0.0.1:8500\n\n\n\n\n\n\n/traefik/consul/watch\n\n\ntrue\n\n\n\n\n\n\n/traefik/consul/prefix\n\n\ntraefik\n\n\n\n\n\n\n/traefik/web/address\n\n\n:8081\n\n\n\n\n\n\n\n\nIn case you are setting key values manually,:\n - Remember to specify the indexes (\n0\n,\n1\n, \n2\n, ... ) under prefixes \n/traefik/defaultentrypoints/\n and \n/traefik/entrypoints/https/tls/certificates/\n in order to match the global configuration structure.\n - Be careful to give the correct IP address and port on the key \n/traefik/consul/endpoint\n.\n\n\nNote that we can either give path to certificate file or directly the file content itself.\n\n\nLaunch Tr\u00e6f\u026ak\n\n\nWe will now launch Tr\u00e6f\u026ak in a container.\nWe use CLI flags to setup the connection between Tr\u00e6f\u026ak and Consul.\nAll the rest of the global configuration is stored in Consul.\n\n\nHere is the \ndocker-compose file\n :\n\n\ntraefik:\n  image: traefik\n  command: --consul --consul.endpoint=127.0.0.1:8500\n  ports:\n    - \n80:80\n\n    - \n8080:8080\n\n\n\n\n\nNB : Be careful to give the correct IP address and port in the flag \n--consul.endpoint\n.\n\n\nTLS support\n\n\nSo far, only \nConsul\n and \netcd\n support TLS connections. \nTo set it up, we should enable \nconsul security\n (or \netcd security\n).\n\n\nThen, we have to provide CA, Cert and Key to Tr\u00e6f\u026ak using \nconsul\n flags :\n\n\n\n\n--consul.tls\n\n\n--consul.tls.ca=path/to/the/file\n\n\n--consul.tls.cert=path/to/the/file\n\n\n--consul.tls.key=path/to/the/file\n \n\n\n\n\nOr etcd flags :\n\n\n\n\n--etcd.tls\n\n\n--etcd.tls.ca=path/to/the/file\n\n\n--etcd.tls.cert=path/to/the/file\n\n\n--etcd.tls.key=path/to/the/file\n\n\n\n\nNote that we can either give directly directly the file content itself (instead of the path to certificate) in a TOML file configuration.\n\n\nRemember the command \ntraefik --help\n to display the updated list of flags.\n\n\nDynamic configuration in Key-value store\n\n\nFollowing our example, we will provide backends/frontends rules to Tr\u00e6f\u026ak.\n\n\nNote that this section is independent of the way Tr\u00e6f\u026ak got its static configuration. \nIt means that the static configuration can either come from the same Key-value store or from any other sources.\n\n\nKey-value storage structure\n\n\nHere is the toml configuration we would like to store in the store :\n\n\n[file]\n\n# rules\n[backends]\n  [backends.backend1]\n    [backends.backend1.circuitbreaker]\n      expression = \nNetworkErrorRatio() \n 0.5\n\n    [backends.backend1.servers.server1]\n    url = \nhttp://172.17.0.2:80\n\n    weight = 10\n    [backends.backend1.servers.server2]\n    url = \nhttp://172.17.0.3:80\n\n    weight = 1\n  [backends.backend2]\n    [backends.backend1.maxconn]\n      amount = 10\n      extractorfunc = \nrequest.host\n\n    [backends.backend2.LoadBalancer]\n      method = \ndrr\n\n    [backends.backend2.servers.server1]\n    url = \nhttp://172.17.0.4:80\n\n    weight = 1\n    [backends.backend2.servers.server2]\n    url = \nhttp://172.17.0.5:80\n\n    weight = 2\n\n[frontends]\n  [frontends.frontend1]\n  backend = \nbackend2\n\n    [frontends.frontend1.routes.test_1]\n    rule = \nHost:test.localhost\n\n  [frontends.frontend2]\n  backend = \nbackend1\n\n  passHostHeader = true\n  priority = 10\n  entrypoints = [\nhttps\n] # overrides defaultEntryPoints\n    [frontends.frontend2.routes.test_1]\n    rule = \nHost:{subdomain:[a-z]+}.localhost\n\n  [frontends.frontend3]\n  entrypoints = [\nhttp\n, \nhttps\n] # overrides defaultEntryPoints\n  backend = \nbackend2\n\n    rule = \nPath:/test\n\n\n\n\n\nAnd there, the same dynamic configuration in a KV Store (using \nprefix = \"traefik\"\n): \n\n\n\n\nbackend 1\n\n\n\n\n\n\n\n\n\n\nKey\n\n\nValue\n\n\n\n\n\n\n\n\n\n\n/traefik/backends/backend1/circuitbreaker/expression\n\n\nNetworkErrorRatio() \n 0.5\n\n\n\n\n\n\n/traefik/backends/backend1/servers/server1/url\n\n\nhttp://172.17.0.2:80\n\n\n\n\n\n\n/traefik/backends/backend1/servers/server1/weight\n\n\n10\n\n\n\n\n\n\n/traefik/backends/backend1/servers/server2/url\n\n\nhttp://172.17.0.3:80\n\n\n\n\n\n\n/traefik/backends/backend1/servers/server2/weight\n\n\n1\n\n\n\n\n\n\n/traefik/backends/backend1/servers/server2/tags\n\n\napi,helloworld\n\n\n\n\n\n\n\n\n\n\nbackend 2\n\n\n\n\n\n\n\n\n\n\nKey\n\n\nValue\n\n\n\n\n\n\n\n\n\n\n/traefik/backends/backend2/maxconn/amount\n\n\n10\n\n\n\n\n\n\n/traefik/backends/backend2/maxconn/extractorfunc\n\n\nrequest.host\n\n\n\n\n\n\n/traefik/backends/backend2/loadbalancer/method\n\n\ndrr\n\n\n\n\n\n\n/traefik/backends/backend2/servers/server1/url\n\n\nhttp://172.17.0.4:80\n\n\n\n\n\n\n/traefik/backends/backend2/servers/server1/weight\n\n\n1\n\n\n\n\n\n\n/traefik/backends/backend2/servers/server2/url\n\n\nhttp://172.17.0.5:80\n\n\n\n\n\n\n/traefik/backends/backend2/servers/server2/weight\n\n\n2\n\n\n\n\n\n\n/traefik/backends/backend2/servers/server2/tags\n\n\nweb\n\n\n\n\n\n\n\n\n\n\nfrontend 1\n\n\n\n\n\n\n\n\n\n\nKey\n\n\nValue\n\n\n\n\n\n\n\n\n\n\n/traefik/frontends/frontend1/backend\n\n\nbackend2\n\n\n\n\n\n\n/traefik/frontends/frontend1/routes/test_1/rule\n\n\nHost:test.localhost\n\n\n\n\n\n\n\n\n\n\nfrontend 2\n\n\n\n\n\n\n\n\n\n\nKey\n\n\nValue\n\n\n\n\n\n\n\n\n\n\n/traefik/frontends/frontend2/backend\n\n\nbackend1\n\n\n\n\n\n\n/traefik/frontends/frontend2/passHostHeader\n\n\ntrue\n\n\n\n\n\n\n/traefik/frontends/frontend2/priority\n\n\n10\n\n\n\n\n\n\n/traefik/frontends/frontend2/entrypoints\n\n\nhttp,https\n\n\n\n\n\n\n/traefik/frontends/frontend2/routes/test_2/rule\n\n\nPathPrefix:/test\n\n\n\n\n\n\n\n\nAtomic configuration changes\n\n\nTr\u00e6f\u026ak can watch the backends/frontends configuration changes and generate its configuration automatically. \n\n\nNote that only backends/frontends rules are dynamic, the rest of the Tr\u00e6f\u026ak configuration stay static. \n\n\nThe \nEtcd\n and \nConsul\n backends do not support updating multiple keys atomically. As a result, it may be possible for Tr\u00e6f\u026ak to read an intermediate configuration state despite judicious use of the \n--providersThrottleDuration\n flag. To solve this problem, Tr\u00e6f\u026ak supports a special key called \n/traefik/alias\n. If set, Tr\u00e6f\u026ak use the value as an alternative key prefix.\n\n\nGiven the key structure below, Tr\u00e6f\u026ak will use the \nhttp://172.17.0.2:80\n as its only backend (frontend keys have been omitted for brevity).\n\n\n\n\n\n\n\n\nKey\n\n\nValue\n\n\n\n\n\n\n\n\n\n\n/traefik/alias\n\n\n/traefik_configurations/1\n\n\n\n\n\n\n/traefik_configurations/1/backends/backend1/servers/server1/url\n\n\nhttp://172.17.0.2:80\n\n\n\n\n\n\n/traefik_configurations/1/backends/backend1/servers/server1/weight\n\n\n10\n\n\n\n\n\n\n\n\nWhen an atomic configuration change is required, you may write a new configuration at an alternative prefix. Here, although the \n/traefik_configurations/2/...\n keys have been set, the old configuration is still active because the \n/traefik/alias\n key still points to \n/traefik_configurations/1\n:\n\n\n\n\n\n\n\n\nKey\n\n\nValue\n\n\n\n\n\n\n\n\n\n\n/traefik/alias\n\n\n/traefik_configurations/1\n\n\n\n\n\n\n/traefik_configurations/1/backends/backend1/servers/server1/url\n\n\nhttp://172.17.0.2:80\n\n\n\n\n\n\n/traefik_configurations/1/backends/backend1/servers/server1/weight\n\n\n10\n\n\n\n\n\n\n/traefik_configurations/2/backends/backend1/servers/server1/url\n\n\nhttp://172.17.0.2:80\n\n\n\n\n\n\n/traefik_configurations/2/backends/backend1/servers/server1/weight\n\n\n5\n\n\n\n\n\n\n/traefik_configurations/2/backends/backend1/servers/server2/url\n\n\nhttp://172.17.0.3:80\n\n\n\n\n\n\n/traefik_configurations/2/backends/backend1/servers/server2/weight\n\n\n5\n\n\n\n\n\n\n\n\nOnce the \n/traefik/alias\n key is updated, the new \n/traefik_configurations/2\n configuration becomes active atomically. Here, we have a 50% balance between the \nhttp://172.17.0.3:80\n and the \nhttp://172.17.0.4:80\n hosts while no traffic is sent to the \n172.17.0.2:80\n host:\n\n\n\n\n\n\n\n\nKey\n\n\nValue\n\n\n\n\n\n\n\n\n\n\n/traefik/alias\n\n\n/traefik_configurations/2\n\n\n\n\n\n\n/traefik_configurations/1/backends/backend1/servers/server1/url\n\n\nhttp://172.17.0.2:80\n\n\n\n\n\n\n/traefik_configurations/1/backends/backend1/servers/server1/weight\n\n\n10\n\n\n\n\n\n\n/traefik_configurations/2/backends/backend1/servers/server1/url\n\n\nhttp://172.17.0.3:80\n\n\n\n\n\n\n/traefik_configurations/2/backends/backend1/servers/server1/weight\n\n\n5\n\n\n\n\n\n\n/traefik_configurations/2/backends/backend1/servers/server2/url\n\n\nhttp://172.17.0.4:80\n\n\n\n\n\n\n/traefik_configurations/2/backends/backend1/servers/server2/weight\n\n\n5\n\n\n\n\n\n\n\n\nNote that Tr\u00e6f\u026ak \nwill not watch for key changes in the \n/traefik_configurations\n prefix\n. It will only watch for changes in the \n/traefik/alias\n. \nFurther, if the \n/traefik/alias\n key is set, all other configuration with \n/traefik/backends\n or \n/traefik/frontends\n prefix are ignored.\n\n\nStore configuration in Key-value store\n\n\nDon't forget to \nsetup the connection between Tr\u00e6f\u026ak and Key-value store\n.\nThe static Tr\u00e6f\u026ak configuration in a key-value store can be automatically created and updated, using the \nstoreconfig\n subcommand\n.\n\n\n$ traefik\u00a0storeconfig\u00a0[flags] ...\n\n\n\n\nThis command is here only to automate the \nprocess which upload the configuration into the Key-value store\n.\nTr\u00e6f\u026ak will not start but the \nstatic configuration\n will be uploaded into the Key-value store.\nIf you configured ACME (Let's Encrypt), your registration account and your certificates will also be uploaded.\n\n\nTo upload your ACME certificates to the KV store, get your traefik TOML file and add the new \nstorage\n option in the \nacme\n section:\n\n\n[acme]\nemail = \ntest@traefik.io\n\nstorage = \ntraefik/acme/account\n # the key where to store your certificates in the KV store\nstorageFile = \nacme.json\n # your old certificates store\n\n\n\n\nCall \ntraefik\u00a0storeconfig\n to upload your config in the KV store.\nThen remove the line \nstorageFile = \"acme.json\"\n from your TOML config file.\nThat's it!", 
            "title": "Key-value store configuration"
        }, 
        {
            "location": "/user-guide/kv-config/#key-value-store-configuration", 
            "text": "Both  static global configuration  and  dynamic  configuration can be sorted in a Key-value store.  This section explains how to launch Tr\u00e6f\u026ak using a configuration loaded from a Key-value store.  Tr\u00e6f\u026ak supports several Key-value stores:   Consul  etcd  ZooKeeper    boltdb", 
            "title": "Key-value store configuration"
        }, 
        {
            "location": "/user-guide/kv-config/#static-configuration-in-key-value-store", 
            "text": "We will see the steps to set it up with an easy example. \nNote that we could do the same with any other Key-value Store.", 
            "title": "Static configuration in Key-value store"
        }, 
        {
            "location": "/user-guide/kv-config/#docker-compose-file-for-consul", 
            "text": "The Tr\u00e6f\u026ak global configuration will be getted from a  Consul  store.   First we have to launch Consul in a container. \nThe  docker-compose file  allows us to launch Consul and four instances of the trivial app  emilevauge/whoamI  :   consul:\n  image: progrium/consul\n  command: -server -bootstrap -log-level debug -ui-dir /ui\n  ports:\n    -  8400:8400 \n    -  8500:8500 \n    -  8600:53/udp \n  expose:\n    -  8300 \n    -  8301 \n    -  8301/udp \n    -  8302 \n    -  8302/udp   \n\nwhoami1:\n  image: emilevauge/whoami\n\nwhoami2:\n  image: emilevauge/whoami\n\nwhoami3:\n  image: emilevauge/whoami\n\nwhoami4:\n  image: emilevauge/whoami", 
            "title": "docker-compose file for Consul"
        }, 
        {
            "location": "/user-guide/kv-config/#upload-the-configuration-in-the-key-value-store", 
            "text": "We should now fill the store with the Tr\u00e6f\u026ak global configuration, as we do with a  TOML file configuration .\nTo do that, we can send the Key-value pairs via  curl commands  or via the  Web UI .  Hopefully, Tr\u00e6f\u026ak allows automation of this process using the  storeconfig  subcommand.\nPlease refer to the  store Tr\u00e6f\u026ak configuration  section to get documentation on it.  Here is the toml configuration we would like to store in the Key-value Store  :  logLevel =  DEBUG \n\ndefaultEntryPoints = [ http ,  https ]\n\n[entryPoints]\n  [entryPoints.http]\n  address =  :80 \n  [entryPoints.https]\n  address =  :443 \n    [entryPoints.https.tls]\n      [[entryPoints.https.tls.certificates]]\n      CertFile =  integration/fixtures/https/snitest.com.cert \n      KeyFile =  integration/fixtures/https/snitest.com.key \n      [[entryPoints.https.tls.certificates]]\n      CertFile =  -----BEGIN CERTIFICATE-----\n                       cert file content \n                      -----END CERTIFICATE----- \n      KeyFile =  -----BEGIN CERTIFICATE-----\n                       key file content \n                      -----END CERTIFICATE----- \n\n\n[consul]\n  endpoint =  127.0.0.1:8500 \n  watch = true\n  prefix =  traefik \n\n[web]\n  address =  :8081   And there, the same global configuration in the Key-value Store (using  prefix = \"traefik\" ):      Key  Value      /traefik/loglevel  DEBUG    /traefik/defaultentrypoints/0  http    /traefik/defaultentrypoints/1  https    /traefik/entrypoints/http/address  :80    /traefik/entrypoints/https/address  :443    /traefik/entrypoints/https/tls/certificates/0/certfile  integration/fixtures/https/snitest.com.cert    /traefik/entrypoints/https/tls/certificates/0/keyfile  integration/fixtures/https/snitest.com.key    /traefik/entrypoints/https/tls/certificates/1/certfile  --BEGIN CERTIFICATE-- cert file content --END CERTIFICATE--    /traefik/entrypoints/https/tls/certificates/1/keyfile  --BEGIN CERTIFICATE-- key file content --END CERTIFICATE--    /traefik/consul/endpoint  127.0.0.1:8500    /traefik/consul/watch  true    /traefik/consul/prefix  traefik    /traefik/web/address  :8081     In case you are setting key values manually,:\n - Remember to specify the indexes ( 0 , 1 ,  2 , ... ) under prefixes  /traefik/defaultentrypoints/  and  /traefik/entrypoints/https/tls/certificates/  in order to match the global configuration structure.\n - Be careful to give the correct IP address and port on the key  /traefik/consul/endpoint .  Note that we can either give path to certificate file or directly the file content itself.", 
            "title": "Upload the configuration in the Key-value store"
        }, 
        {
            "location": "/user-guide/kv-config/#launch-trfk", 
            "text": "We will now launch Tr\u00e6f\u026ak in a container.\nWe use CLI flags to setup the connection between Tr\u00e6f\u026ak and Consul.\nAll the rest of the global configuration is stored in Consul.  Here is the  docker-compose file  :  traefik:\n  image: traefik\n  command: --consul --consul.endpoint=127.0.0.1:8500\n  ports:\n    -  80:80 \n    -  8080:8080   NB : Be careful to give the correct IP address and port in the flag  --consul.endpoint .", 
            "title": "Launch Tr\u00e6f\u026ak"
        }, 
        {
            "location": "/user-guide/kv-config/#tls-support", 
            "text": "So far, only  Consul  and  etcd  support TLS connections. \nTo set it up, we should enable  consul security  (or  etcd security ).  Then, we have to provide CA, Cert and Key to Tr\u00e6f\u026ak using  consul  flags :   --consul.tls  --consul.tls.ca=path/to/the/file  --consul.tls.cert=path/to/the/file  --consul.tls.key=path/to/the/file     Or etcd flags :   --etcd.tls  --etcd.tls.ca=path/to/the/file  --etcd.tls.cert=path/to/the/file  --etcd.tls.key=path/to/the/file   Note that we can either give directly directly the file content itself (instead of the path to certificate) in a TOML file configuration.  Remember the command  traefik --help  to display the updated list of flags.", 
            "title": "TLS support"
        }, 
        {
            "location": "/user-guide/kv-config/#dynamic-configuration-in-key-value-store", 
            "text": "Following our example, we will provide backends/frontends rules to Tr\u00e6f\u026ak.  Note that this section is independent of the way Tr\u00e6f\u026ak got its static configuration. \nIt means that the static configuration can either come from the same Key-value store or from any other sources.", 
            "title": "Dynamic configuration in Key-value store"
        }, 
        {
            "location": "/user-guide/kv-config/#key-value-storage-structure", 
            "text": "Here is the toml configuration we would like to store in the store :  [file]\n\n# rules\n[backends]\n  [backends.backend1]\n    [backends.backend1.circuitbreaker]\n      expression =  NetworkErrorRatio()   0.5 \n    [backends.backend1.servers.server1]\n    url =  http://172.17.0.2:80 \n    weight = 10\n    [backends.backend1.servers.server2]\n    url =  http://172.17.0.3:80 \n    weight = 1\n  [backends.backend2]\n    [backends.backend1.maxconn]\n      amount = 10\n      extractorfunc =  request.host \n    [backends.backend2.LoadBalancer]\n      method =  drr \n    [backends.backend2.servers.server1]\n    url =  http://172.17.0.4:80 \n    weight = 1\n    [backends.backend2.servers.server2]\n    url =  http://172.17.0.5:80 \n    weight = 2\n\n[frontends]\n  [frontends.frontend1]\n  backend =  backend2 \n    [frontends.frontend1.routes.test_1]\n    rule =  Host:test.localhost \n  [frontends.frontend2]\n  backend =  backend1 \n  passHostHeader = true\n  priority = 10\n  entrypoints = [ https ] # overrides defaultEntryPoints\n    [frontends.frontend2.routes.test_1]\n    rule =  Host:{subdomain:[a-z]+}.localhost \n  [frontends.frontend3]\n  entrypoints = [ http ,  https ] # overrides defaultEntryPoints\n  backend =  backend2 \n    rule =  Path:/test   And there, the same dynamic configuration in a KV Store (using  prefix = \"traefik\" ):    backend 1      Key  Value      /traefik/backends/backend1/circuitbreaker/expression  NetworkErrorRatio()   0.5    /traefik/backends/backend1/servers/server1/url  http://172.17.0.2:80    /traefik/backends/backend1/servers/server1/weight  10    /traefik/backends/backend1/servers/server2/url  http://172.17.0.3:80    /traefik/backends/backend1/servers/server2/weight  1    /traefik/backends/backend1/servers/server2/tags  api,helloworld      backend 2      Key  Value      /traefik/backends/backend2/maxconn/amount  10    /traefik/backends/backend2/maxconn/extractorfunc  request.host    /traefik/backends/backend2/loadbalancer/method  drr    /traefik/backends/backend2/servers/server1/url  http://172.17.0.4:80    /traefik/backends/backend2/servers/server1/weight  1    /traefik/backends/backend2/servers/server2/url  http://172.17.0.5:80    /traefik/backends/backend2/servers/server2/weight  2    /traefik/backends/backend2/servers/server2/tags  web      frontend 1      Key  Value      /traefik/frontends/frontend1/backend  backend2    /traefik/frontends/frontend1/routes/test_1/rule  Host:test.localhost      frontend 2      Key  Value      /traefik/frontends/frontend2/backend  backend1    /traefik/frontends/frontend2/passHostHeader  true    /traefik/frontends/frontend2/priority  10    /traefik/frontends/frontend2/entrypoints  http,https    /traefik/frontends/frontend2/routes/test_2/rule  PathPrefix:/test", 
            "title": "Key-value storage structure"
        }, 
        {
            "location": "/user-guide/kv-config/#atomic-configuration-changes", 
            "text": "Tr\u00e6f\u026ak can watch the backends/frontends configuration changes and generate its configuration automatically.   Note that only backends/frontends rules are dynamic, the rest of the Tr\u00e6f\u026ak configuration stay static.   The  Etcd  and  Consul  backends do not support updating multiple keys atomically. As a result, it may be possible for Tr\u00e6f\u026ak to read an intermediate configuration state despite judicious use of the  --providersThrottleDuration  flag. To solve this problem, Tr\u00e6f\u026ak supports a special key called  /traefik/alias . If set, Tr\u00e6f\u026ak use the value as an alternative key prefix.  Given the key structure below, Tr\u00e6f\u026ak will use the  http://172.17.0.2:80  as its only backend (frontend keys have been omitted for brevity).     Key  Value      /traefik/alias  /traefik_configurations/1    /traefik_configurations/1/backends/backend1/servers/server1/url  http://172.17.0.2:80    /traefik_configurations/1/backends/backend1/servers/server1/weight  10     When an atomic configuration change is required, you may write a new configuration at an alternative prefix. Here, although the  /traefik_configurations/2/...  keys have been set, the old configuration is still active because the  /traefik/alias  key still points to  /traefik_configurations/1 :     Key  Value      /traefik/alias  /traefik_configurations/1    /traefik_configurations/1/backends/backend1/servers/server1/url  http://172.17.0.2:80    /traefik_configurations/1/backends/backend1/servers/server1/weight  10    /traefik_configurations/2/backends/backend1/servers/server1/url  http://172.17.0.2:80    /traefik_configurations/2/backends/backend1/servers/server1/weight  5    /traefik_configurations/2/backends/backend1/servers/server2/url  http://172.17.0.3:80    /traefik_configurations/2/backends/backend1/servers/server2/weight  5     Once the  /traefik/alias  key is updated, the new  /traefik_configurations/2  configuration becomes active atomically. Here, we have a 50% balance between the  http://172.17.0.3:80  and the  http://172.17.0.4:80  hosts while no traffic is sent to the  172.17.0.2:80  host:     Key  Value      /traefik/alias  /traefik_configurations/2    /traefik_configurations/1/backends/backend1/servers/server1/url  http://172.17.0.2:80    /traefik_configurations/1/backends/backend1/servers/server1/weight  10    /traefik_configurations/2/backends/backend1/servers/server1/url  http://172.17.0.3:80    /traefik_configurations/2/backends/backend1/servers/server1/weight  5    /traefik_configurations/2/backends/backend1/servers/server2/url  http://172.17.0.4:80    /traefik_configurations/2/backends/backend1/servers/server2/weight  5     Note that Tr\u00e6f\u026ak  will not watch for key changes in the  /traefik_configurations  prefix . It will only watch for changes in the  /traefik/alias . \nFurther, if the  /traefik/alias  key is set, all other configuration with  /traefik/backends  or  /traefik/frontends  prefix are ignored.", 
            "title": "Atomic configuration changes"
        }, 
        {
            "location": "/user-guide/kv-config/#store-configuration-in-key-value-store", 
            "text": "Don't forget to  setup the connection between Tr\u00e6f\u026ak and Key-value store .\nThe static Tr\u00e6f\u026ak configuration in a key-value store can be automatically created and updated, using the  storeconfig  subcommand .  $ traefik\u00a0storeconfig\u00a0[flags] ...  This command is here only to automate the  process which upload the configuration into the Key-value store .\nTr\u00e6f\u026ak will not start but the  static configuration  will be uploaded into the Key-value store.\nIf you configured ACME (Let's Encrypt), your registration account and your certificates will also be uploaded.  To upload your ACME certificates to the KV store, get your traefik TOML file and add the new  storage  option in the  acme  section:  [acme]\nemail =  test@traefik.io \nstorage =  traefik/acme/account  # the key where to store your certificates in the KV store\nstorageFile =  acme.json  # your old certificates store  Call  traefik\u00a0storeconfig  to upload your config in the KV store.\nThen remove the line  storageFile = \"acme.json\"  from your TOML config file.\nThat's it!", 
            "title": "Store configuration in Key-value store"
        }, 
        {
            "location": "/user-guide/cluster/", 
            "text": "Clustering / High Availability\n\n\nThis guide explains how tu use Tr\u00e6f\u026ak in high availability mode.\nIn order to deploy and configure multiple Tr\u00e6f\u026ak instances, without copying the same configuration file on each instance, we will use a distributed Key-Value store.\n\n\nPrerequisites\n\n\nYou will need a working KV store cluster.\n\n\nFile configuration to KV store migration\n\n\nWe created a special Tr\u00e6f\u026ak command to help configuring your Key Value store from a Tr\u00e6f\u026ak TOML configuration file.\nPlease refer to \nthis section\n to get more details.\n\n\nDeploy a Tr\u00e6f\u026ak cluster\n\n\nOnce your Tr\u00e6f\u026ak configuration is uploaded on your KV store, you can start each Tr\u00e6f\u026ak instance.\nA Tr\u00e6f\u026ak cluster is based on a master/slave model. When starting, Tr\u00e6f\u026ak will elect a master. If this instance fails, another master will be automatically elected.", 
            "title": "Clustering/HA"
        }, 
        {
            "location": "/user-guide/cluster/#clustering-high-availability", 
            "text": "This guide explains how tu use Tr\u00e6f\u026ak in high availability mode.\nIn order to deploy and configure multiple Tr\u00e6f\u026ak instances, without copying the same configuration file on each instance, we will use a distributed Key-Value store.", 
            "title": "Clustering / High Availability"
        }, 
        {
            "location": "/user-guide/cluster/#prerequisites", 
            "text": "You will need a working KV store cluster.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/user-guide/cluster/#file-configuration-to-kv-store-migration", 
            "text": "We created a special Tr\u00e6f\u026ak command to help configuring your Key Value store from a Tr\u00e6f\u026ak TOML configuration file.\nPlease refer to  this section  to get more details.", 
            "title": "File configuration to KV store migration"
        }, 
        {
            "location": "/user-guide/cluster/#deploy-a-trfk-cluster", 
            "text": "Once your Tr\u00e6f\u026ak configuration is uploaded on your KV store, you can start each Tr\u00e6f\u026ak instance.\nA Tr\u00e6f\u026ak cluster is based on a master/slave model. When starting, Tr\u00e6f\u026ak will elect a master. If this instance fails, another master will be automatically elected.", 
            "title": "Deploy a Tr\u00e6f\u026ak cluster"
        }, 
        {
            "location": "/benchmarks/", 
            "text": "Benchmarks\n\n\nConfiguration\n\n\nI would like to thanks \nvincentbernat\n from \nexoscale.ch\n who kindly provided the infrastructure needed for the benchmarks.\n\n\nI used 4 VMs for the tests with the following configuration:\n\n\n\n\n32 GB RAM\n\n\n8 CPU Cores\n\n\n10 GB SSD\n\n\nUbuntu 14.04 LTS 64-bit\n\n\n\n\nSetup\n\n\n\n\nOne VM used to launch the benchmarking tool \nwrk\n\n\nOne VM for traefik (v1.0.0-beta.416) / nginx (v1.4.6)\n\n\nTwo VMs for 2 backend servers in go \nwhoami\n\n\n\n\nEach VM has been tuned using the following limits:\n\n\nsysctl -w fs.file-max=\n9999999\n\nsysctl -w fs.nr_open=\n9999999\n\nsysctl -w net.core.netdev_max_backlog=\n4096\n\nsysctl -w net.core.rmem_max=\n16777216\n\nsysctl -w net.core.somaxconn=\n65535\n\nsysctl -w net.core.wmem_max=\n16777216\n\nsysctl -w net.ipv4.ip_local_port_range=\n1025       65535\n\nsysctl -w net.ipv4.tcp_fin_timeout=\n30\n\nsysctl -w net.ipv4.tcp_keepalive_time=\n30\n\nsysctl -w net.ipv4.tcp_max_syn_backlog=\n20480\n\nsysctl -w net.ipv4.tcp_max_tw_buckets=\n400000\n\nsysctl -w net.ipv4.tcp_no_metrics_save=\n1\n\nsysctl -w net.ipv4.tcp_syn_retries=\n2\n\nsysctl -w net.ipv4.tcp_synack_retries=\n2\n\nsysctl -w net.ipv4.tcp_tw_recycle=\n1\n\nsysctl -w net.ipv4.tcp_tw_reuse=\n1\n\nsysctl -w vm.min_free_kbytes=\n65536\n\nsysctl -w vm.overcommit_memory=\n1\n\nulimit -n 9999999\n\n\n\n\nNginx\n\n\nHere is the config Nginx file use \n/etc/nginx/nginx.conf\n:\n\n\nuser www-data;\nworker_processes auto;\nworker_rlimit_nofile 200000;\npid /var/run/nginx.pid;\n\nevents {\n    worker_connections 10000;\n    use epoll;\n    multi_accept on;\n}\n\nhttp {\n    sendfile on;\n    tcp_nopush on;\n    tcp_nodelay on;\n    keepalive_timeout 300;\n    keepalive_requests 10000;\n    types_hash_max_size 2048;\n\n    open_file_cache max=200000 inactive=300s; \n    open_file_cache_valid 300s; \n    open_file_cache_min_uses 2;\n    open_file_cache_errors on;\n\n    server_tokens off;\n    dav_methods off;\n\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n\n    access_log /var/log/nginx/access.log combined;\n    error_log /var/log/nginx/error.log warn;\n\n    gzip off;\n    gzip_vary off;\n\n    include /etc/nginx/conf.d/*.conf;\n    include /etc/nginx/sites-enabled/*.conf;\n}\n\n\n\n\nHere is the Nginx vhost file used:\n\n\nupstream whoami {\n    server IP-whoami1:80;\n    server IP-whoami2:80;\n    keepalive 300;\n}\n\nserver {\n    listen 8001;\n    server_name test.traefik;\n    access_log off;\n    error_log /dev/null crit;\n    if ($host != \ntest.traefik\n) {\n        return 404;\n    }\n    location / {\n        proxy_pass http://whoami;\n        proxy_http_version 1.1;\n        proxy_set_header Connection \n;\n    proxy_set_header  X-Forwarded-Host $host;\n    }\n}\n\n\n\n\nTraefik\n\n\nHere is the \ntraefik.toml\n file used:\n\n\nMaxIdleConnsPerHost = 100000\ndefaultEntryPoints = [\nhttp\n]\n\n[entryPoints]\n  [entryPoints.http]\n  address = \n:8000\n\n\n[file]\n[backends]\n  [backends.backend1]\n    [backends.backend1.servers.server1]\n    url = \nhttp://IP-whoami1:80\n\n    weight = 1\n    [backends.backend1.servers.server2]\n    url = \nhttp://IP-whoami2:80\n\n    weight = 1\n\n[frontends]\n  [frontends.frontend1]\n  backend = \nbackend1\n\n    [frontends.frontend1.routes.test_1]\n    rule = \nHost: test.traefik\n\n\n\n\n\nResults\n\n\nwhoami:\n\n\nwrk -t20 -c1000 -d60s -H \nHost: test.traefik\n --latency  http://IP-whoami:80/bench\nRunning 1m test @ http://IP-whoami:80/bench\n  20 threads and 1000 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    70.28ms  134.72ms   1.91s    89.94%\n    Req/Sec     2.92k   742.42     8.78k    68.80%\n  Latency Distribution\n     50%   10.63ms\n     75%   75.64ms\n     90%  205.65ms\n     99%  668.28ms\n  3476705 requests in 1.00m, 384.61MB read\n  Socket errors: connect 0, read 0, write 0, timeout 103\nRequests/sec:  57894.35\nTransfer/sec:      6.40MB\n\n\n\n\nnginx:\n\n\nwrk -t20 -c1000 -d60s -H \nHost: test.traefik\n --latency  http://IP-nginx:8001/bench\nRunning 1m test @ http://IP-nginx:8001/bench\n  20 threads and 1000 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency   101.25ms  180.09ms   1.99s    89.34%\n    Req/Sec     1.69k   567.69     9.39k    72.62%\n  Latency Distribution\n     50%   15.46ms\n     75%  129.11ms\n     90%  302.44ms\n     99%  846.59ms\n  2018427 requests in 1.00m, 298.36MB read\n  Socket errors: connect 0, read 0, write 0, timeout 90\nRequests/sec:  33591.67\nTransfer/sec:      4.97MB\n\n\n\n\ntraefik:\n\n\nwrk -t20 -c1000 -d60s -H \nHost: test.traefik\n --latency  http://IP-traefik:8000/bench\nRunning 1m test @ http://IP-traefik:8000/bench\n  20 threads and 1000 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    91.72ms  150.43ms   2.00s    90.50%\n    Req/Sec     1.43k   266.37     2.97k    69.77%\n  Latency Distribution\n     50%   19.74ms\n     75%  121.98ms\n     90%  237.39ms\n     99%  687.49ms\n  1705073 requests in 1.00m, 188.63MB read\n  Socket errors: connect 0, read 0, write 0, timeout 7\nRequests/sec:  28392.44\nTransfer/sec:      3.14MB\n\n\n\n\nConclusion\n\n\nTraefik is obviously slower than Nginx, but not so much: Traefik can serve 28392 requests/sec and Nginx 33591 requests/sec which gives a ratio of 85%.\nNot bad for young project :) !\n\n\nSome areas of possible improvements:\n\n\n\n\nUse \nGO_REUSEPORT\n listener\n\n\nRun a separate server instance per CPU core with \nGOMAXPROCS=1\n (it appears during benchmarks that there is a lot more context switches with traefik than with nginx)", 
            "title": "Benchmarks"
        }, 
        {
            "location": "/benchmarks/#benchmarks", 
            "text": "", 
            "title": "Benchmarks"
        }, 
        {
            "location": "/benchmarks/#configuration", 
            "text": "I would like to thanks  vincentbernat  from  exoscale.ch  who kindly provided the infrastructure needed for the benchmarks.  I used 4 VMs for the tests with the following configuration:   32 GB RAM  8 CPU Cores  10 GB SSD  Ubuntu 14.04 LTS 64-bit", 
            "title": "Configuration"
        }, 
        {
            "location": "/benchmarks/#setup", 
            "text": "One VM used to launch the benchmarking tool  wrk  One VM for traefik (v1.0.0-beta.416) / nginx (v1.4.6)  Two VMs for 2 backend servers in go  whoami   Each VM has been tuned using the following limits:  sysctl -w fs.file-max= 9999999 \nsysctl -w fs.nr_open= 9999999 \nsysctl -w net.core.netdev_max_backlog= 4096 \nsysctl -w net.core.rmem_max= 16777216 \nsysctl -w net.core.somaxconn= 65535 \nsysctl -w net.core.wmem_max= 16777216 \nsysctl -w net.ipv4.ip_local_port_range= 1025       65535 \nsysctl -w net.ipv4.tcp_fin_timeout= 30 \nsysctl -w net.ipv4.tcp_keepalive_time= 30 \nsysctl -w net.ipv4.tcp_max_syn_backlog= 20480 \nsysctl -w net.ipv4.tcp_max_tw_buckets= 400000 \nsysctl -w net.ipv4.tcp_no_metrics_save= 1 \nsysctl -w net.ipv4.tcp_syn_retries= 2 \nsysctl -w net.ipv4.tcp_synack_retries= 2 \nsysctl -w net.ipv4.tcp_tw_recycle= 1 \nsysctl -w net.ipv4.tcp_tw_reuse= 1 \nsysctl -w vm.min_free_kbytes= 65536 \nsysctl -w vm.overcommit_memory= 1 \nulimit -n 9999999", 
            "title": "Setup"
        }, 
        {
            "location": "/benchmarks/#nginx", 
            "text": "Here is the config Nginx file use  /etc/nginx/nginx.conf :  user www-data;\nworker_processes auto;\nworker_rlimit_nofile 200000;\npid /var/run/nginx.pid;\n\nevents {\n    worker_connections 10000;\n    use epoll;\n    multi_accept on;\n}\n\nhttp {\n    sendfile on;\n    tcp_nopush on;\n    tcp_nodelay on;\n    keepalive_timeout 300;\n    keepalive_requests 10000;\n    types_hash_max_size 2048;\n\n    open_file_cache max=200000 inactive=300s; \n    open_file_cache_valid 300s; \n    open_file_cache_min_uses 2;\n    open_file_cache_errors on;\n\n    server_tokens off;\n    dav_methods off;\n\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n\n    access_log /var/log/nginx/access.log combined;\n    error_log /var/log/nginx/error.log warn;\n\n    gzip off;\n    gzip_vary off;\n\n    include /etc/nginx/conf.d/*.conf;\n    include /etc/nginx/sites-enabled/*.conf;\n}  Here is the Nginx vhost file used:  upstream whoami {\n    server IP-whoami1:80;\n    server IP-whoami2:80;\n    keepalive 300;\n}\n\nserver {\n    listen 8001;\n    server_name test.traefik;\n    access_log off;\n    error_log /dev/null crit;\n    if ($host !=  test.traefik ) {\n        return 404;\n    }\n    location / {\n        proxy_pass http://whoami;\n        proxy_http_version 1.1;\n        proxy_set_header Connection  ;\n    proxy_set_header  X-Forwarded-Host $host;\n    }\n}", 
            "title": "Nginx"
        }, 
        {
            "location": "/benchmarks/#traefik", 
            "text": "Here is the  traefik.toml  file used:  MaxIdleConnsPerHost = 100000\ndefaultEntryPoints = [ http ]\n\n[entryPoints]\n  [entryPoints.http]\n  address =  :8000 \n\n[file]\n[backends]\n  [backends.backend1]\n    [backends.backend1.servers.server1]\n    url =  http://IP-whoami1:80 \n    weight = 1\n    [backends.backend1.servers.server2]\n    url =  http://IP-whoami2:80 \n    weight = 1\n\n[frontends]\n  [frontends.frontend1]\n  backend =  backend1 \n    [frontends.frontend1.routes.test_1]\n    rule =  Host: test.traefik", 
            "title": "Traefik"
        }, 
        {
            "location": "/benchmarks/#results", 
            "text": "", 
            "title": "Results"
        }, 
        {
            "location": "/benchmarks/#whoami", 
            "text": "wrk -t20 -c1000 -d60s -H  Host: test.traefik  --latency  http://IP-whoami:80/bench\nRunning 1m test @ http://IP-whoami:80/bench\n  20 threads and 1000 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    70.28ms  134.72ms   1.91s    89.94%\n    Req/Sec     2.92k   742.42     8.78k    68.80%\n  Latency Distribution\n     50%   10.63ms\n     75%   75.64ms\n     90%  205.65ms\n     99%  668.28ms\n  3476705 requests in 1.00m, 384.61MB read\n  Socket errors: connect 0, read 0, write 0, timeout 103\nRequests/sec:  57894.35\nTransfer/sec:      6.40MB", 
            "title": "whoami:"
        }, 
        {
            "location": "/benchmarks/#nginx_1", 
            "text": "wrk -t20 -c1000 -d60s -H  Host: test.traefik  --latency  http://IP-nginx:8001/bench\nRunning 1m test @ http://IP-nginx:8001/bench\n  20 threads and 1000 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency   101.25ms  180.09ms   1.99s    89.34%\n    Req/Sec     1.69k   567.69     9.39k    72.62%\n  Latency Distribution\n     50%   15.46ms\n     75%  129.11ms\n     90%  302.44ms\n     99%  846.59ms\n  2018427 requests in 1.00m, 298.36MB read\n  Socket errors: connect 0, read 0, write 0, timeout 90\nRequests/sec:  33591.67\nTransfer/sec:      4.97MB", 
            "title": "nginx:"
        }, 
        {
            "location": "/benchmarks/#traefik_1", 
            "text": "wrk -t20 -c1000 -d60s -H  Host: test.traefik  --latency  http://IP-traefik:8000/bench\nRunning 1m test @ http://IP-traefik:8000/bench\n  20 threads and 1000 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    91.72ms  150.43ms   2.00s    90.50%\n    Req/Sec     1.43k   266.37     2.97k    69.77%\n  Latency Distribution\n     50%   19.74ms\n     75%  121.98ms\n     90%  237.39ms\n     99%  687.49ms\n  1705073 requests in 1.00m, 188.63MB read\n  Socket errors: connect 0, read 0, write 0, timeout 7\nRequests/sec:  28392.44\nTransfer/sec:      3.14MB", 
            "title": "traefik:"
        }, 
        {
            "location": "/benchmarks/#conclusion", 
            "text": "Traefik is obviously slower than Nginx, but not so much: Traefik can serve 28392 requests/sec and Nginx 33591 requests/sec which gives a ratio of 85%.\nNot bad for young project :) !  Some areas of possible improvements:   Use  GO_REUSEPORT  listener  Run a separate server instance per CPU core with  GOMAXPROCS=1  (it appears during benchmarks that there is a lot more context switches with traefik than with nginx)", 
            "title": "Conclusion"
        }
    ]
}